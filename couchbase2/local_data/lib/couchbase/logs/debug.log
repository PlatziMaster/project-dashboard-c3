[ns_server:info,2020-06-16T21:32:30.333Z,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2020-06-16T21:32:30.419Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2020-06-16T21:32:30.419Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.420Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.420Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.420Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.420Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.420Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.420Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.420Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.420Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.421Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.421Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.421Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.421Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.421Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.421Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.421Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.422Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.422Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.422Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.422Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.423Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.423Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.424Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.424Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.424Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2020-06-16T21:32:30.424Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2020-06-16T21:32:30.468Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2020-06-16T21:32:30.471Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-06-16T21:32:30.482Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:init:196]ip config not found. Looks like we're brand new node
[ns_server:info,2020-06-16T21:32:30.496Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2020-06-16T21:32:30.538Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.539Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.539Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:30.549Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.555Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:30.556Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:30.557Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:30.558Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2020-06-16T21:32:30.558Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:30.566Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.569Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.572Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:30.576Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-06-16T21:32:30.580Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-06-16T21:32:30.582Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-06-16T21:32:30.587Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2020-06-16T21:32:30.587Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.588Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2020-06-16T21:32:30.592Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2020-06-16T21:32:30.654Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-06-16T21:32:30.655Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-06-16T21:32:30.655Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:30.655Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:32:30.655Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.202044>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:30.656Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.202044>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.3111058533.1128267778.202049>}
[ns_server:debug,2020-06-16T21:32:30.674Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-06-16T21:32:30.678Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-06-16T21:32:30.678Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-06-16T21:32:30.696Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2020-06-16T21:32:30.696Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.761Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:30.768Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,19,76}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-e321575] [64-bit] [smp:2:2] [ds:2:2:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2020,6,16},{21,32,30}}},
               {memory,
                   [{total,36153976},
                    {processes,9154320},
                    {processes_used,9150848},
                    {system,26999656},
                    {atom,388625},
                    {atom_used,364446},
                    {binary,125776},
                    {code,8251333},
                    {ets,1504888}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,file,
                    code_server,ets,filename,error_handler,error_logger,
                    application,application_controller,application_master,
                    file_server,heart,kernel,file_io_server,gen,erl_eval,
                    erl_parse,code,gen_event,gen_server,proc_lib,lists,
                    erl_lint,supervisor,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{sasl,"SASL  CXC 138 11","3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.4.4"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ns_server,"Couchbase server","6.5.1-6299-enterprise"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,httpd_sup,auth,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ale_dynamic_sup,ssl_pem_cache_dist,kernel_safe_sup,rex,
                    global_group,net_sup,ssl_connection_sup,kernel_sup,
                    ssl_admin_sup,tftp_sup,global_name_server,ssl_sup,
                    root_sup,os_mon_sup,erts_code_purger,file_server_2,
                    error_logger,cpu_sup,memsup,erl_epmd,init,disksup,ale,
                    erl_signal_server,net_kernel,dist_manager,ssl_pem_cache,
                    ssl_manager,ssl_dist_admin_sup,ssl_dist_connection_sup,
                    ssl_dist_sup,ssl_tls_dist_proxy,ssl_manager_dist,user,
                    sasl_safe_sup,ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,4}]
[ns_server:info,2020-06-16T21:32:30.778Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"5949a1cb71c5fc16e405c5516c10ea572d6092ca\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"4b1a36ba5580231fdff3cccdc13d881d414441be\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"9e60a140980fb321259f7a1aa68ad269c8d08be4\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"5716bf0df2d36db8ff45c6508a328a92b9457cbf\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"6299\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.1\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"6938f549fde15cb7fb825420509ebf528cbb9925\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"d975bb8d3e189a81f13ac0fee77327184b23050b\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"34e5def72a16608678f2e37bf6521970345b341d\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"16221facdc82e4416cdd7832589e00e0450a97b3\" />",
 "  <project name=\"couchdb\" revision=\"f47945baa733653fb0ab268d8f11c62104f6689f\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"6bf93c48217400824f0a68e6e2631af1a4f0d352\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"1671a4be4dc978014102d51d705becd2fe3dc8db\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"ef4f6c1ae8133a14ff865278e1c0ec8205b698d5\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"104dc876579adc97ebc688074155f4d69b6b8ee7\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"0642262623b651cffbd074f8fb7827dff29d60e4\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"5836cfc682e9d90036b8fc780d6a28b033e6b34c\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"2e83c38f0d59c8d11c724a43d221dbf929ecd39d\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"fe736e8f5f5162c32ec6cabbdbf87e83808833e7\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"73401bdcb718bbacab843b40ae443a531dc133d8\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"7fc841317177cc5483481dcde2f700074cbb1d3f\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"715d4c32c56346118f81c30b74b7f2dd65efe1e2\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"a77874d1379bd1896b8a507b9d70420fa918a5f8\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"354d5201a478ff4fe1edeee420efa7f5083fdff9\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"54254d07f46b45cf8d19879a5440dff3f04d7ed2\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"f7ed5f6aff5c9077e223b685883ab5318a6c8893\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"03c56234a265a7e4670f8e2db19b0f1769f11637\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"74c19969e8ef1b5309077a03885d00e273378f6c\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"f9c0a66eb346ee0a1302e0829110d675a404f685\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"713ad35f30fa026628943782f0670cb582f36885\" />",
 "  <project name=\"query-ui\" revision=\"c527ff2adb16f930a21b025abe93b891772a1287\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"4208ad825dda03a6a3d2197df8ec57948aebcc12\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"b0771570e74fc79ab9516515f5d90cbc5ef9f0b2\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"16a65eed670e9de30bb7a857e0cdcc5bbda95101\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2020-06-16T21:32:30.785Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.788Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.794Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:30.798Z,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2020-06-16T21:32:30.798Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:30.798Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:30.900Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2020-06-16T21:32:30.934Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2020-06-16T21:32:30.947Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1125]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2020-06-16T21:32:31.054Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2020-06-16T21:32:31.084Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   false]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9999]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   8092]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
   {port,0}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1198},
 {fts_memory_quota,256},
 {memory_quota,1698},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,['ns_1@cb.local']},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   {6,5,1}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
   <<"a771316973056c9759aee152c7c79f8b">>]}]
[error_logger:info,2020-06-16T21:32:31.114Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:31.119Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:31.122Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:31.123Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:31.132Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-06-16T21:32:31.132Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|false]
[error_logger:info,2020-06-16T21:32:31.132Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:31.133Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|inet]
[ns_server:debug,2020-06-16T21:32:31.134Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}]
[error_logger:info,2020-06-16T21:32:31.145Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:31.202Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:31.210Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2020-06-16T21:32:31.211Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:31.211Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:31.233Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:31.240Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:31.714Z,ns_1@cb.local:<0.218.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:debug,2020-06-16T21:32:31.714Z,ns_1@cb.local:<0.218.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:info,2020-06-16T21:32:31.715Z,ns_1@cb.local:<0.218.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:debug,2020-06-16T21:32:31.769Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_server_cert:generate_cert_and_pkey:83]Generated certificate and private key in 512150 us
[ns_server:debug,2020-06-16T21:32:31.770Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFhkjqO3exlcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBhNzlmZjAzNjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYTc5ZmYw\nMzYwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCuMW3fkLVrFD70n31X\nHfZxRUv1bwFofCvMQXfMYOM2ehu/iA1c18ONS+0MlYT4IE34TyjPGySjvWTd2c3I\nmLbbRC+l82ORsB77WAm/VHqBDKthOUF"...>>,
  <<"*****">>}]
[ns_server:debug,2020-06-16T21:32:31.770Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562351}}]}]
[ns_server:info,2020-06-16T21:32:31.780Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:maybe_generate_local_cert:622]Failed to read node certificate. Perhaps it wasn't created yet. Error: {error,
                                                                        {badmatch,
                                                                         {error,
                                                                          enoent}}}
[ns_server:debug,2020-06-16T21:32:32.184Z,ns_1@cb.local:<0.222.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:debug,2020-06-16T21:32:32.184Z,ns_1@cb.local:<0.222.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:info,2020-06-16T21:32:32.184Z,ns_1@cb.local:<0.222.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:info,2020-06-16T21:32:32.229Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:do_generate_local_cert:610]Saved local cert for node 'ns_1@cb.local'
[ns_server:debug,2020-06-16T21:32:32.295Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-06-16T21:32:32.296Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-06-16T21:32:32.296Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-06-16T21:32:32.341Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:464]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,25,35,168,237,222,
             198,87,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,
             32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,
             114,118,101,114,32,97,55,57,102,102,48,51,54,48,30,23,13,49,51,
             48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,
             53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
             104,98,97,115,101,32,83,101,114,118,101,114,32,97,55,57,102,102,
             48,51,54,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,
             130,1,15,0,48,130,1,10,2,130,1,1,0,174,49,109,223,144,181,107,
             20,62,244,159,125,87,29,246,113,69,75,245,111,1,104,124,43,204,
             65,119,204,96,227,54,122,27,191,136,13,92,215,195,141,75,237,12,
             149,132,248,32,77,248,79,40,207,27,36,163,189,100,221,217,205,
             200,152,182,219,68,47,165,243,99,145,176,30,251,88,9,191,84,122,
             129,12,171,97,57,65,117,125,201,187,161,29,55,194,30,179,108,82,
             182,70,39,238,138,116,199,165,87,79,107,145,116,250,124,126,48,
             140,190,101,28,25,74,212,33,10,220,201,83,77,196,78,26,171,13,
             190,112,217,232,242,223,218,25,215,78,185,111,98,28,131,240,85,
             124,175,54,208,58,18,75,254,127,17,160,74,103,211,94,73,212,86,
             137,34,50,83,13,254,231,46,78,93,215,205,139,19,221,23,123,237,
             17,128,66,215,13,15,101,154,18,227,32,124,218,146,24,199,197,
             156,196,98,54,249,162,187,115,68,170,8,100,160,207,69,98,155,
             127,228,31,192,139,179,81,185,239,116,200,26,127,204,129,56,156,
             113,231,187,199,137,166,176,132,181,123,246,124,38,169,227,195,
             19,133,100,121,173,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,
             1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,
             5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,
             42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,89,124,248,161,22,
             73,145,149,85,172,148,139,7,93,32,7,175,91,14,172,231,118,40,78,
             27,178,89,75,131,208,230,143,18,102,90,239,184,191,23,110,16,
             188,53,1,140,250,222,182,110,227,147,146,54,213,57,173,240,98,
             40,234,121,98,38,48,248,150,245,251,234,226,199,93,237,126,208,
             23,34,64,242,190,249,78,248,187,146,149,159,106,2,225,94,161,
             102,109,22,144,33,95,3,166,3,130,10,120,163,22,1,220,93,251,89,
             47,21,2,34,23,199,184,53,76,186,10,45,185,16,104,172,97,84,9,
             190,82,230,168,86,130,242,216,104,76,95,149,170,215,57,168,115,
             153,222,43,173,32,118,189,189,71,64,37,87,38,97,20,91,141,219,
             144,142,176,73,66,145,96,212,63,39,6,222,25,163,243,199,142,209,
             62,110,156,172,26,182,105,118,182,221,216,241,46,255,41,42,166,
             137,80,220,176,6,199,46,39,84,21,82,186,29,131,184,174,30,89,
             198,145,196,172,166,178,197,125,249,206,150,58,124,85,249,51,63,
             254,149,179,199,207,211,231,103,179,81,14,189,155,8,39,97,84,27,
             150,157,173>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2020-06-16T21:32:32.344Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:32.402Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-06-16T21:32:32.403Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-06-16T21:32:32.404Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-06-16T21:32:32.404Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-06-16T21:32:32.462Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.224.0>,menelaus_web}
             started: [{pid,<0.225.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 25,35,168,237,222,198,87,48,13,6,9,42,134,
                                 72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
                                 6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,97,55,
                                 57,102,102,48,51,54,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,97,
                                 55,57,102,102,48,51,54,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,174,49,109,223,
                                 144,181,107,20,62,244,159,125,87,29,246,113,
                                 69,75,245,111,1,104,124,43,204,65,119,204,
                                 96,227,54,122,27,191,136,13,92,215,195,141,
                                 75,237,12,149,132,248,32,77,248,79,40,207,
                                 27,36,163,189,100,221,217,205,200,152,182,
                                 219,68,47,165,243,99,145,176,30,251,88,9,
                                 191,84,122,129,12,171,97,57,65,117,125,201,
                                 187,161,29,55,194,30,179,108,82,182,70,39,
                                 238,138,116,199,165,87,79,107,145,116,250,
                                 124,126,48,140,190,101,28,25,74,212,33,10,
                                 220,201,83,77,196,78,26,171,13,190,112,217,
                                 232,242,223,218,25,215,78,185,111,98,28,131,
                                 240,85,124,175,54,208,58,18,75,254,127,17,
                                 160,74,103,211,94,73,212,86,137,34,50,83,13,
                                 254,231,46,78,93,215,205,139,19,221,23,123,
                                 237,17,128,66,215,13,15,101,154,18,227,32,
                                 124,218,146,24,199,197,156,196,98,54,249,
                                 162,187,115,68,170,8,100,160,207,69,98,155,
                                 127,228,31,192,139,179,81,185,239,116,200,
                                 26,127,204,129,56,156,113,231,187,199,137,
                                 166,176,132,181,123,246,124,38,169,227,195,
                                 19,133,100,121,173,2,3,1,0,1,163,56,48,54,
                                 48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,
                                 19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,
                                 3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,
                                 255,48,13,6,9,42,134,72,134,247,13,1,1,11,5,
                                 0,3,130,1,1,0,89,124,248,161,22,73,145,149,
                                 85,172,148,139,7,93,32,7,175,91,14,172,231,
                                 118,40,78,27,178,89,75,131,208,230,143,18,
                                 102,90,239,184,191,23,110,16,188,53,1,140,
                                 250,222,182,110,227,147,146,54,213,57,173,
                                 240,98,40,234,121,98,38,48,248,150,245,251,
                                 234,226,199,93,237,126,208,23,34,64,242,190,
                                 249,78,248,187,146,149,159,106,2,225,94,161,
                                 102,109,22,144,33,95,3,166,3,130,10,120,163,
                                 22,1,220,93,251,89,47,21,2,34,23,199,184,53,
                                 76,186,10,45,185,16,104,172,97,84,9,190,82,
                                 230,168,86,130,242,216,104,76,95,149,170,
                                 215,57,168,115,153,222,43,173,32,118,189,
                                 189,71,64,37,87,38,97,20,91,141,219,144,142,
                                 176,73,66,145,96,212,63,39,6,222,25,163,243,
                                 199,142,209,62,110,156,172,26,182,105,118,
                                 182,221,216,241,46,255,41,42,166,137,80,220,
                                 176,6,199,46,39,84,21,82,186,29,131,184,174,
                                 30,89,198,145,196,172,166,178,197,125,249,
                                 206,150,58,124,85,249,51,63,254,149,179,199,
                                 207,211,231,103,179,81,14,189,155,8,39,97,
                                 84,27,150,157,173>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:32.465Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-06-16T21:32:32.466Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-06-16T21:32:32.466Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-06-16T21:32:32.467Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-06-16T21:32:32.468Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.224.0>,menelaus_web}
             started: [{pid,<0.243.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 25,35,168,237,222,198,87,48,13,6,9,42,134,
                                 72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
                                 6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,97,55,
                                 57,102,102,48,51,54,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,97,
                                 55,57,102,102,48,51,54,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,174,49,109,223,
                                 144,181,107,20,62,244,159,125,87,29,246,113,
                                 69,75,245,111,1,104,124,43,204,65,119,204,
                                 96,227,54,122,27,191,136,13,92,215,195,141,
                                 75,237,12,149,132,248,32,77,248,79,40,207,
                                 27,36,163,189,100,221,217,205,200,152,182,
                                 219,68,47,165,243,99,145,176,30,251,88,9,
                                 191,84,122,129,12,171,97,57,65,117,125,201,
                                 187,161,29,55,194,30,179,108,82,182,70,39,
                                 238,138,116,199,165,87,79,107,145,116,250,
                                 124,126,48,140,190,101,28,25,74,212,33,10,
                                 220,201,83,77,196,78,26,171,13,190,112,217,
                                 232,242,223,218,25,215,78,185,111,98,28,131,
                                 240,85,124,175,54,208,58,18,75,254,127,17,
                                 160,74,103,211,94,73,212,86,137,34,50,83,13,
                                 254,231,46,78,93,215,205,139,19,221,23,123,
                                 237,17,128,66,215,13,15,101,154,18,227,32,
                                 124,218,146,24,199,197,156,196,98,54,249,
                                 162,187,115,68,170,8,100,160,207,69,98,155,
                                 127,228,31,192,139,179,81,185,239,116,200,
                                 26,127,204,129,56,156,113,231,187,199,137,
                                 166,176,132,181,123,246,124,38,169,227,195,
                                 19,133,100,121,173,2,3,1,0,1,163,56,48,54,
                                 48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,
                                 19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,
                                 3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,
                                 255,48,13,6,9,42,134,72,134,247,13,1,1,11,5,
                                 0,3,130,1,1,0,89,124,248,161,22,73,145,149,
                                 85,172,148,139,7,93,32,7,175,91,14,172,231,
                                 118,40,78,27,178,89,75,131,208,230,143,18,
                                 102,90,239,184,191,23,110,16,188,53,1,140,
                                 250,222,182,110,227,147,146,54,213,57,173,
                                 240,98,40,234,121,98,38,48,248,150,245,251,
                                 234,226,199,93,237,126,208,23,34,64,242,190,
                                 249,78,248,187,146,149,159,106,2,225,94,161,
                                 102,109,22,144,33,95,3,166,3,130,10,120,163,
                                 22,1,220,93,251,89,47,21,2,34,23,199,184,53,
                                 76,186,10,45,185,16,104,172,97,84,9,190,82,
                                 230,168,86,130,242,216,104,76,95,149,170,
                                 215,57,168,115,153,222,43,173,32,118,189,
                                 189,71,64,37,87,38,97,20,91,141,219,144,142,
                                 176,73,66,145,96,212,63,39,6,222,25,163,243,
                                 199,142,209,62,110,156,172,26,182,105,118,
                                 182,221,216,241,46,255,41,42,166,137,80,220,
                                 176,6,199,46,39,84,21,82,186,29,131,184,174,
                                 30,89,198,145,196,172,166,178,197,125,249,
                                 206,150,58,124,85,249,51,63,254,149,179,199,
                                 207,211,231,103,179,81,14,189,155,8,39,97,
                                 84,27,150,157,173>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:32.469Z,ns_1@cb.local:<0.223.0>:restartable:start_child:98]Started child process <0.224.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2020-06-16T21:32:32.469Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.223.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:32.469Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:32.490Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.261.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:32.495Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:32.514Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.266.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:32.518Z,ns_1@cb.local:users_replicator<0.266.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2020-06-16T21:32:32.523Z,ns_1@cb.local:users_storage<0.267.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.266.0>
[ns_server:debug,2020-06-16T21:32:32.523Z,ns_1@cb.local:users_replicator<0.266.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.267.0>
[ns_server:debug,2020-06-16T21:32:32.530Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2020-06-16T21:32:32.530Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.267.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:32.531Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:32.557Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2020-06-16T21:32:32.558Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.269.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:32.566Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.272.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:32.566Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.263.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:32.567Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.275.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:32.569Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.276.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:32.611Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2020-06-16T21:32:32.612Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 81ms
[ns_server:debug,2020-06-16T21:32:32.613Z,ns_1@cb.local:wait_link_to_couchdb_node<0.280.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2020-06-16T21:32:32.613Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.279.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:32.614Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-06-16T21:32:32.614Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:32.614Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.202307>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:32.614Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.202307>,
                                  inet_tcp_dist,<0.283.0>,
                                  #Ref<0.3111058533.1128267778.202311>}
[ns_server:debug,2020-06-16T21:32:32.615Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.202307>,
                               inet_tcp_dist,<0.283.0>,
                               #Ref<0.3111058533.1128267778.202311>}
[ns_server:debug,2020-06-16T21:32:32.615Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-06-16T21:32:32.615Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.283.0>,shutdown}}
[error_logger:info,2020-06-16T21:32:32.615Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:32.626Z,ns_1@cb.local:users_replicator<0.266.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2020-06-16T21:32:32.815Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:32.816Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:32:32.816Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.202322>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:32.816Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.202322>,
                                  inet_tcp_dist,<0.286.0>,
                                  #Ref<0.3111058533.1128267777.203253>}
[error_logger:info,2020-06-16T21:32:32.817Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.286.0>,shutdown}}
[ns_server:debug,2020-06-16T21:32:32.817Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.202322>,
                               inet_tcp_dist,<0.286.0>,
                               #Ref<0.3111058533.1128267777.203253>}
[error_logger:info,2020-06-16T21:32:32.817Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:32.817Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-06-16T21:32:33.019Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:32:33.019Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.202336>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:33.020Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.202336>,
                                  inet_tcp_dist,<0.289.0>,
                                  #Ref<0.3111058533.1128267778.202340>}
[error_logger:info,2020-06-16T21:32:33.020Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-06-16T21:32:33.033Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.289.0>,shutdown}}
[error_logger:info,2020-06-16T21:32:33.033Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:33.033Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.202336>,
                               inet_tcp_dist,<0.289.0>,
                               #Ref<0.3111058533.1128267778.202340>}
[ns_server:debug,2020-06-16T21:32:33.034Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-06-16T21:32:33.236Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-06-16T21:32:33.235Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:33.236Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267777.203259>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:33.236Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267777.203259>,
                                  inet_tcp_dist,<0.292.0>,
                                  #Ref<0.3111058533.1128267777.203263>}
[ns_server:debug,2020-06-16T21:32:33.237Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267777.203259>,
                               inet_tcp_dist,<0.292.0>,
                               #Ref<0.3111058533.1128267777.203263>}
[error_logger:info,2020-06-16T21:32:33.237Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.292.0>,shutdown}}
[ns_server:debug,2020-06-16T21:32:33.237Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-06-16T21:32:33.237Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:33.440Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-06-16T21:32:33.440Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:33.440Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267777.203273>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:33.440Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267777.203273>,
                                  inet_tcp_dist,<0.295.0>,
                                  #Ref<0.3111058533.1128267777.203277>}
[ns_server:debug,2020-06-16T21:32:33.444Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-06-16T21:32:33.444Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267777.203273>,
                               inet_tcp_dist,<0.295.0>,
                               #Ref<0.3111058533.1128267777.203277>}
[error_logger:info,2020-06-16T21:32:33.444Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.295.0>,shutdown}}
[error_logger:info,2020-06-16T21:32:33.445Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-06-16T21:32:33.645Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:33.646Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:32:33.646Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.202353>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:33.646Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.202353>,
                                  inet_tcp_dist,<0.298.0>,
                                  #Ref<0.3111058533.1128267777.203283>}
[error_logger:info,2020-06-16T21:32:33.647Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.298.0>,shutdown}}
[ns_server:debug,2020-06-16T21:32:33.647Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.202353>,
                               inet_tcp_dist,<0.298.0>,
                               #Ref<0.3111058533.1128267777.203283>}
[error_logger:info,2020-06-16T21:32:33.647Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:33.647Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-06-16T21:32:33.848Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:33.848Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:32:33.848Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267777.203286>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:33.848Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267777.203286>,
                                  inet_tcp_dist,<0.301.0>,
                                  #Ref<0.3111058533.1128267778.202367>}
[error_logger:info,2020-06-16T21:32:33.849Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.301.0>,shutdown}}
[ns_server:debug,2020-06-16T21:32:33.849Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267777.203286>,
                               inet_tcp_dist,<0.301.0>,
                               #Ref<0.3111058533.1128267778.202367>}
[ns_server:debug,2020-06-16T21:32:33.849Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2020-06-16T21:32:33.849Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-06-16T21:32:34.050Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:34.051Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:32:34.051Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.202377>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:34.051Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.202377>,
                                  inet_tcp_dist,<0.304.0>,
                                  #Ref<0.3111058533.1128267777.203291>}
[ns_server:debug,2020-06-16T21:32:34.052Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2020-06-16T21:32:34.052Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.202377>,
                               inet_tcp_dist,<0.304.0>,
                               #Ref<0.3111058533.1128267777.203291>}
[error_logger:info,2020-06-16T21:32:34.052Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.304.0>,shutdown}}
[error_logger:info,2020-06-16T21:32:34.052Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2020-06-16T21:32:34.252Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:32:34.253Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:32:34.253Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.202391>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:32:34.254Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.202391>,
                                  inet_tcp_dist,<0.307.0>,
                                  #Ref<0.3111058533.1128267777.203292>}
[ns_server:debug,2020-06-16T21:32:34.365Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-06-16T21:32:34.567Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-06-16T21:32:34.769Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-06-16T21:32:34.971Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-06-16T21:32:35.174Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-06-16T21:32:35.375Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-06-16T21:32:35.581Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2020-06-16T21:32:35.808Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2020-06-16T21:32:36.446Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.317.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:36.647Z,ns_1@cb.local:ns_couchdb_port<0.279.0>:ns_port_server:log:224]ns_couchdb<0.279.0>: Apache CouchDB  (LogLevel=info) is starting.

[ns_server:info,2020-06-16T21:32:36.899Z,ns_1@cb.local:ns_couchdb_port<0.279.0>:ns_port_server:log:224]ns_couchdb<0.279.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2020-06-16T21:32:36.974Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.280.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:36.988Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[ns_server:debug,2020-06-16T21:32:36.990Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{3,63759562356}}]}]
[ns_server:debug,2020-06-16T21:32:36.990Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562356}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-06-16T21:32:36.990Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{4,63759562356}}]}]
[ns_server:debug,2020-06-16T21:32:36.993Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562356}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[error_logger:info,2020-06-16T21:32:37.002Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.321.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.009Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.322.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:37.016Z,ns_1@cb.local:ns_server_sup<0.320.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2020-06-16T21:32:37.017Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.323.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.028Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.324.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-06-16T21:32:37.036Z,ns_1@cb.local:ns_log<0.326.0>:ns_log:read_logs:91]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first startup): {error,
                                                                                                 enoent}
[error_logger:info,2020-06-16T21:32:37.037Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.326.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.039Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.327.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.057Z,ns_1@cb.local:memcached_passwords<0.328.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-06-16T21:32:37.063Z,ns_1@cb.local:memcached_passwords<0.328.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-06-16T21:32:37.138Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.328.0>
[ns_server:debug,2020-06-16T21:32:37.138Z,ns_1@cb.local:memcached_passwords<0.328.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:32:37.139Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.328.0>
[ns_server:debug,2020-06-16T21:32:37.146Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2020-06-16T21:32:37.146Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.328.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.155Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.332.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2020-06-16T21:32:37.155Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.331.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.162Z,ns_1@cb.local:memcached_permissions<0.333.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:warn,2020-06-16T21:32:37.171Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1105]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-06-16T21:32:37.171Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2020-06-16T21:32:37.174Z,ns_1@cb.local:memcached_permissions<0.333.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:info,2020-06-16T21:32:37.176Z,ns_1@cb.local:ns_couchdb_port<0.279.0>:ns_port_server:log:224]ns_couchdb<0.279.0>: 209: Booted. Waiting for shutdown request
ns_couchdb<0.279.0>: working as port

[ns_server:debug,2020-06-16T21:32:37.182Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.333.0>
[ns_server:debug,2020-06-16T21:32:37.182Z,ns_1@cb.local:memcached_permissions<0.333.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:32:37.182Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.333.0>
[ns_server:debug,2020-06-16T21:32:37.187Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2020-06-16T21:32:37.188Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.333.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-06-16T21:32:37.189Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1105]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2020-06-16T21:32:37.189Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2020-06-16T21:32:37.192Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.336.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.195Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.338.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.195Z,ns_1@cb.local:ns_node_disco<0.339.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[ns_server:debug,2020-06-16T21:32:37.196Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[user:info,2020-06-16T21:32:37.196Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_init:84]Initial otp cookie generated: {sanitized,
                                  <<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}
[ns_server:debug,2020-06-16T21:32:37.197Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{5,63759562357}}]}]
[ns_server:debug,2020-06-16T21:32:37.197Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]},
 {cookie,{sanitized,<<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}}]
[ns_server:debug,2020-06-16T21:32:37.198Z,ns_1@cb.local:<0.340.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}
[ns_server:debug,2020-06-16T21:32:37.207Z,ns_1@cb.local:<0.340.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}
[error_logger:info,2020-06-16T21:32:37.208Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.339.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.215Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.342.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.220Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.343.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.237Z,ns_1@cb.local:ns_config_rep<0.345.0>:ns_config_rep:init:71]init pulling
[ns_server:debug,2020-06-16T21:32:37.238Z,ns_1@cb.local:ns_config_rep<0.345.0>:ns_config_rep:init:73]init pushing
[error_logger:info,2020-06-16T21:32:37.238Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.344.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.240Z,ns_1@cb.local:ns_config_rep<0.345.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2020-06-16T21:32:37.255Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2020-06-16T21:32:37.255Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-06-16T21:32:37.255Z,ns_1@cb.local:<0.351.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}
[ns_server:debug,2020-06-16T21:32:37.256Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-06-16T21:32:37.257Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]},
 {cookie,{sanitized,<<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}}]
[ns_server:debug,2020-06-16T21:32:37.257Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562356}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-06-16T21:32:37.257Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              2078178643},
                                                                             {0,
                                                                              2078178643},
                                                                             false,
                                                                             []}
[ns_server:debug,2020-06-16T21:32:37.258Z,ns_1@cb.local:<0.351.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}
[ns_server:debug,2020-06-16T21:32:37.258Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-06-16T21:32:37.258Z,ns_1@cb.local:<0.352.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}
[ns_server:debug,2020-06-16T21:32:37.258Z,ns_1@cb.local:<0.352.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}
[ns_server:debug,2020-06-16T21:32:37.259Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562356}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-06-16T21:32:37.260Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIFhkjqO3exlcwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBhNzlmZjAzNjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYTc5ZmYw\nMzYwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCuMW3fkLVrFD70n31X\nHfZxRUv1bwFofCvMQXfMYOM2ehu/iA1c18ONS+0MlYT4IE34TyjPGySjvWTd2c3I\nmLbbRC+l82ORsB77WAm/VHqBDKthOUF"...>>,
  <<"*****">>}]
[ns_server:debug,2020-06-16T21:32:37.260Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-06-16T21:32:37.261Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|false]
[ns_server:debug,2020-06-16T21:32:37.262Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|inet]
[ns_server:debug,2020-06-16T21:32:37.262Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2020-06-16T21:32:37.263Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit ->
[{auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-06-16T21:32:37.263Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]
[ns_server:debug,2020-06-16T21:32:37.263Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2020-06-16T21:32:37.264Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:32:37.264Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[],{configs,[]}]
[ns_server:debug,2020-06-16T21:32:37.264Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
1198
[ns_server:debug,2020-06-16T21:32:37.264Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2020-06-16T21:32:37.265Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2020-06-16T21:32:37.265Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
256
[ns_server:debug,2020-06-16T21:32:37.265Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2020-06-16T21:32:37.265Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2020-06-16T21:32:37.265Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
30
[ns_server:debug,2020-06-16T21:32:37.265Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2020-06-16T21:32:37.265Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memory_quota ->
1698
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
['ns_1@cb.local']
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
['ns_1@cb.local']
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest_creds ->
null
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2020-06-16T21:32:37.266Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"in"...>>
[ns_server:debug,2020-06-16T21:32:37.267Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2020-06-16T21:32:37.267Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2020-06-16T21:32:37.267Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',audit} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}]
[ns_server:debug,2020-06-16T21:32:37.267Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',capi_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|8092]
[ns_server:debug,2020-06-16T21:32:37.267Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_admin_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9110]
[ns_server:debug,2020-06-16T21:32:37.268Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_client_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9113]
[ns_server:debug,2020-06-16T21:32:37.268Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9112]
[ns_server:debug,2020-06-16T21:32:37.268Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9111]
[ns_server:debug,2020-06-16T21:32:37.269Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cluster_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9115]
[ns_server:debug,2020-06-16T21:32:37.270Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_console_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9114]
[ns_server:debug,2020-06-16T21:32:37.270Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_data_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9116]
[ns_server:debug,2020-06-16T21:32:37.270Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_debug_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|-1]
[ns_server:debug,2020-06-16T21:32:37.270Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|8095]
[ns_server:debug,2020-06-16T21:32:37.271Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_messaging_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9118]
[ns_server:debug,2020-06-16T21:32:37.271Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9119]
[ns_server:debug,2020-06-16T21:32:37.271Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9121]
[ns_server:debug,2020-06-16T21:32:37.272Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_parent_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9122]
[ns_server:debug,2020-06-16T21:32:37.272Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_replication_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9120]
[ns_server:debug,2020-06-16T21:32:37.273Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_result_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9117]
[ns_server:debug,2020-06-16T21:32:37.273Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|18095]
[ns_server:debug,2020-06-16T21:32:37.274Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',compaction_daemon} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-06-16T21:32:37.274Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',config_version} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
 {6,5,1}]
[error_logger:info,2020-06-16T21:32:37.274Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.345.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.274Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_debug_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9140]
[error_logger:info,2020-06-16T21:32:37.274Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.337.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:37.274Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|8096]
[ns_server:debug,2020-06-16T21:32:37.274Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_https_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|18096]
[ns_server:debug,2020-06-16T21:32:37.275Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9130]
[ns_server:debug,2020-06-16T21:32:37.274Z,ns_1@cb.local:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,auto_failover_cfg,
                               auto_reprovision_cfg,autocompaction,buckets,
                               cbas_memory_quota,cert_and_pkey,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@cb.local',address_family},
                               {node,'ns_1@cb.local',audit},
                               {node,'ns_1@cb.local',capi_port},
                               {node,'ns_1@cb.local',cbas_admin_port},
                               {node,'ns_1@cb.local',cbas_cc_client_port},
                               {node,'ns_1@cb.local',cbas_cc_cluster_port},
                               {node,'ns_1@cb.local',cbas_cc_http_port},
                               {node,'ns_1@cb.local',cbas_cluster_port},
                               {node,'ns_1@cb.local',cbas_console_port},
                               {node,'ns_1@cb.local',cbas_data_port},
                               {node,'ns_1@cb.local',cbas_debug_port},
                               {node,'ns_1@cb.local',cbas_dirs},
                               {node,'ns_1@cb.local',cbas_http_port},
                               {node,'ns_1@cb.local',cbas_messaging_port},
                               {node,'ns_1@cb.local',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@cb.local',cbas_metadata_port},
                               {node,'ns_1@cb.local',cbas_parent_port},
                               {node,'ns_1@cb.local',cbas_replication_port},
                               {node,'ns_1@cb.local',cbas_result_port},
                               {node,'ns_1@cb.local',cbas_ssl_port},
                               {node,'ns_1@cb.local',compaction_daemon},
                               {node,'ns_1@cb.local',config_version},
                               {node,'ns_1@cb.local',erl_external_listeners},
                               {node,'ns_1@cb.local',eventing_debug_port},
                               {node,'ns_1@cb.local',eventing_dir},
                               {node,'ns_1@cb.local',eventing_http_port},
                               {node,'ns_1@cb.local',eventing_https_port},
                               {node,'ns_1@cb.local',fts_grpc_port},
                               {node,'ns_1@cb.local',fts_grpc_ssl_port},
                               {node,'ns_1@cb.local',fts_http_port},
                               {node,'ns_1@cb.local',fts_ssl_port}]..)
[ns_server:debug,2020-06-16T21:32:37.275Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|19130]
[ns_server:debug,2020-06-16T21:32:37.275Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|8094]
[ns_server:debug,2020-06-16T21:32:37.275Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|18094]
[ns_server:debug,2020-06-16T21:32:37.275Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_admin_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9100]
[ns_server:debug,2020-06-16T21:32:37.276Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9102]
[ns_server:debug,2020-06-16T21:32:37.276Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_https_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|19102]
[ns_server:debug,2020-06-16T21:32:37.276Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_scan_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9101]
[ns_server:debug,2020-06-16T21:32:37.276Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stcatchup_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9104]
[ns_server:debug,2020-06-16T21:32:37.276Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stinit_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9103]
[ns_server:debug,2020-06-16T21:32:37.276Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stmaint_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9105]
[ns_server:debug,2020-06-16T21:32:37.276Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',is_enterprise} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|true]
[ns_server:debug,2020-06-16T21:32:37.279Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',isasl} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-06-16T21:32:37.279Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',membership} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
 active]
[ns_server:debug,2020-06-16T21:32:37.282Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2020-06-16T21:32:37.283Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_config} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-06-16T21:32:37.283Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|11206]
[ns_server:debug,2020-06-16T21:32:37.284Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_defaults} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-06-16T21:32:37.284Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',moxi} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {port,0}]
[error_logger:info,2020-06-16T21:32:37.284Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.357.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.284Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ns_log} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-06-16T21:32:37.285Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',port_servers} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}]
[ns_server:debug,2020-06-16T21:32:37.285Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9999]
[ns_server:debug,2020-06-16T21:32:37.286Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9999]
[ns_server:debug,2020-06-16T21:32:37.286Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',query_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|8093]
[ns_server:debug,2020-06-16T21:32:37.286Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',rest} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-06-16T21:32:37.286Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',saslauthd_enabled} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|true]
[ns_server:debug,2020-06-16T21:32:37.287Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_capi_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|18092]
[ns_server:debug,2020-06-16T21:32:37.287Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_query_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|18093]
[ns_server:debug,2020-06-16T21:32:37.287Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_rest_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|18091]
[ns_server:debug,2020-06-16T21:32:37.287Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',uuid} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|
 <<"a771316973056c9759aee152c7c79f8b">>]
[ns_server:debug,2020-06-16T21:32:37.288Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',xdcr_rest_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|9998]
[ns_server:debug,2020-06-16T21:32:37.288Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|false]
[ns_server:debug,2020-06-16T21:32:37.288Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{5,63759562357}}]}]
[error_logger:info,2020-06-16T21:32:37.295Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.359.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.295Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.362.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.296Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.363.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.296Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.364.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.297Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:trigger_ssl_reload:596]Notify services [capi_ssl_service] about secure_headers_changed change
[ns_server:debug,2020-06-16T21:32:37.297Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:742]Going to notify following services: [capi_ssl_service]
[error_logger:info,2020-06-16T21:32:37.305Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.369.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:37.326Z,ns_1@cb.local:<0.368.0>:ns_ssl_services_setup:notify_service:774]Successfully notified service capi_ssl_service
[error_logger:info,2020-06-16T21:32:37.326Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.371.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:37.326Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:758]Succesfully notified services [capi_ssl_service]
[error_logger:info,2020-06-16T21:32:37.327Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.373.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.327Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.370.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.333Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.378.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.336Z,ns_1@cb.local:ns_heart<0.371.0>:ns_heart:grab_latest_stats:264]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,260}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,251}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,188}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,119}]}]}}

[ns_server:debug,2020-06-16T21:32:37.336Z,ns_1@cb.local:ns_heart<0.371.0>:ns_heart:grab_latest_stats:264]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,260}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,251}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,188}]}]}}

[error_logger:info,2020-06-16T21:32:37.342Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.380.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.342Z,ns_1@cb.local:<0.376.0>:restartable:start_child:98]Started child process <0.377.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2020-06-16T21:32:37.343Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.376.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.343Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.383.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.358Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.384.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.358Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.385.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.359Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.386.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.359Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.387.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.364Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.389.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.370Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.392.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.371Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.394.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.388Z,ns_1@cb.local:ns_heart<0.371.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2020-06-16T21:32:37.392Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.396.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.392Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.398.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.395Z,ns_1@cb.local:ns_heart<0.371.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-06-16T21:32:37.407Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.399.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.414Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.400.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.428Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.401.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.429Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.402.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:37.433Z,ns_1@cb.local:<0.404.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-06-16T21:32:37.434Z,ns_1@cb.local:<0.404.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-06-16T21:32:37.434Z,ns_1@cb.local:<0.404.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-06-16T21:32:37.435Z,ns_1@cb.local:<0.404.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-06-16T21:32:37.436Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.404.0>,menelaus_web}
             started: [{pid,<0.410.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:37.448Z,ns_1@cb.local:<0.404.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-06-16T21:32:37.449Z,ns_1@cb.local:<0.404.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-06-16T21:32:37.449Z,ns_1@cb.local:<0.404.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-06-16T21:32:37.449Z,ns_1@cb.local:<0.404.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-06-16T21:32:37.450Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.404.0>,menelaus_web}
             started: [{pid,<0.429.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"::"},{name,menelaus_web_ipv6}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.450Z,ns_1@cb.local:<0.403.0>:restartable:start_child:98]Started child process <0.404.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2020-06-16T21:32:37.450Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.403.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.455Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.447.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.457Z,ns_1@cb.local:ns_heart_slow_status_updater<0.373.0>:ns_heart:grab_latest_stats:264]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,260}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,251}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,245}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2020-06-16T21:32:37.457Z,ns_1@cb.local:ns_heart_slow_status_updater<0.373.0>:ns_heart:grab_latest_stats:264]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,260}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,283}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,251}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,245}]}]}}

[ns_server:debug,2020-06-16T21:32:37.459Z,ns_1@cb.local:ns_heart_slow_status_updater<0.373.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-06-16T21:32:37.459Z,ns_1@cb.local:ns_heart_slow_status_updater<0.373.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2020-06-16T21:32:37.460Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.457.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.471Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.458.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.479Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.459.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-06-16T21:32:37.479Z,ns_1@cb.local:ns_server_sup<0.320.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "6.5.1-6299-enterprise".
[error_logger:info,2020-06-16T21:32:37.480Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.395.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.481Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.465.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.488Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.469.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.489Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.470.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.489Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.468.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:37.497Z,ns_1@cb.local:ns_ports_setup<0.465.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-06-16T21:32:37.515Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.481.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.578Z,ns_1@cb.local:memcached_auth_server<0.483.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2020-06-16T21:32:37.578Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.483.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.579Z,ns_1@cb.local:ns_audit_cfg<0.485.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      1},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {disabled,
                                                                                      []},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-06-16T21:32:37.601Z,ns_1@cb.local:ns_audit_cfg<0.485.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2020-06-16T21:32:37.602Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.485.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2020-06-16T21:32:37.604Z,ns_1@cb.local:<0.488.0>:ns_memcached:connect:1108]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-06-16T21:32:37.616Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.489.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.617Z,ns_1@cb.local:memcached_config_mgr<0.490.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2020-06-16T21:32:37.617Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.490.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:37.624Z,ns_1@cb.local:<0.491.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2020-06-16T21:32:37.624Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.491.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.632Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.492.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.634Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.493.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.648Z,ns_1@cb.local:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":5,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}]}">>
[ns_server:warn,2020-06-16T21:32:37.651Z,ns_1@cb.local:<0.497.0>:ns_memcached:connect:1108]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2020-06-16T21:32:37.652Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.498.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.655Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.499.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.656Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.495.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.662Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.501.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.667Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.505.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.679Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.507.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.685Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.508.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.685Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.510.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.688Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.511.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.689Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.513.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.699Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.514.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.703Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.516.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.704Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.518.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.707Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.519.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.708Z,ns_1@cb.local:ns_ports_setup<0.465.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12940.109.0>
[ns_server:debug,2020-06-16T21:32:37.708Z,ns_1@cb.local:memcached_config_mgr<0.490.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[error_logger:info,2020-06-16T21:32:37.712Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.522.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.714Z,ns_1@cb.local:goxdcr_status_keeper<0.522.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-06-16T21:32:37.715Z,ns_1@cb.local:goxdcr_status_keeper<0.522.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-06-16T21:32:37.717Z,ns_1@cb.local:memcached_config_mgr<0.490.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12940.116.0>
[error_logger:info,2020-06-16T21:32:37.722Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.527.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.732Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.530.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.757Z,ns_1@cb.local:memcached_config_mgr<0.490.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[error_logger:info,2020-06-16T21:32:37.757Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.532.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.759Z,ns_1@cb.local:memcached_config_mgr<0.490.0>:memcached_config_mgr:init:86]activated memcached port server
[error_logger:info,2020-06-16T21:32:37.766Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.535.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.774Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.538.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.774Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.529.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.775Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.541.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.775Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.526.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:37.793Z,ns_1@cb.local:<0.545.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.545.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2020-06-16T21:32:37.799Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[error_logger:info,2020-06-16T21:32:37.799Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.543.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.799Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:32:37.800Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-06-16T21:32:37.800Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:32:37.800Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2020-06-16T21:32:37.801Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2020-06-16T21:32:37.803Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.547.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.804Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.546.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.804Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.548.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.824Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.552.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.833Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.553.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.834Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.551.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:37.844Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.555.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.849Z,ns_1@cb.local:leader_registry_sup<0.554.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2020-06-16T21:32:37.849Z,ns_1@cb.local:leader_registry_sup<0.554.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-06-16T21:32:37.850Z,ns_1@cb.local:leader_registry_sup<0.554.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2020-06-16T21:32:37.863Z,ns_1@cb.local:mb_master<0.558.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2020-06-16T21:32:37.864Z,ns_1@cb.local:leader_registry<0.555.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2020-06-16T21:32:37.881Z,ns_1@cb.local:mb_master<0.558.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[ns_server:debug,2020-06-16T21:32:37.887Z,ns_1@cb.local:leader_lease_acquirer<0.561.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-06-16T21:32:37.887Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.561.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.893Z,ns_1@cb.local:leader_quorum_nodes_manager<0.563.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2020-06-16T21:32:37.893Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.563.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:37.915Z,ns_1@cb.local:mb_master_sup<0.560.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.565.0> on 'ns_1@cb.local'

[error_logger:info,2020-06-16T21:32:37.915Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.565.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:37.920Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.567.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:37.930Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,0]}]

[ns_server:info,2020-06-16T21:32:37.930Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,1]
[ns_server:debug,2020-06-16T21:32:37.931Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,1]},
 {set,client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2020-06-16T21:32:37.935Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,5]
[ns_server:debug,2020-06-16T21:32:37.936Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,5]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>},
 {set,{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {set,buckets,[{configs,[]}]},
 {delete,{rbac_upgrade,[5,5]}},
 {set,audit,
      [{enabled,[]},
       {disabled_users,[]},
       {auditd_enabled,false},
       {rotate_interval,86400},
       {rotate_size,20971520},
       {disabled,[]},
       {sync,[]},
       {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {set,quorum_nodes,['ns_1@cb.local']},
 {set,scramsha_fallback_salt,<<23,98,128,12,178,147,38,16,61,167,19,38>>}]

[ns_server:info,2020-06-16T21:32:37.936Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,0]
[ns_server:debug,2020-06-16T21:32:37.936Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,0]}]

[ns_server:info,2020-06-16T21:32:37.947Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,5]
[ns_server:debug,2020-06-16T21:32:37.958Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,5]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Eventing function definition was created or updated">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Eventing function definition was deleted">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Eventing function definition was read">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Eventing deployed functions list was read">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Eventing function draft definitions were read">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Eventing function draft definitions were deleted">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Save a draft definition to the store">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Save settings for a given app">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Get config for eventing">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Save config for eventing">>},
         {enabled,true},
         {module,eventing}]},
       {32782,
        [{name,<<"Cleanup Eventing">>},
         {description,<<"Clears up app definitions and settings from metakv">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Get settings for a given app">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Import a list of functions">>},
         {enabled,false},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Export the list of functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Eventing running function list was read">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]}]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true}]},
 {set,max_bucket_count,30},
 {set,retry_rebalance,
      [{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>}]

[ns_server:debug,2020-06-16T21:32:37.963Z,ns_1@cb.local:leader_quorum_nodes_manager<0.563.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-06-16T21:32:37.964Z,ns_1@cb.local:leader_quorum_nodes_manager<0.563.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2020-06-16T21:32:37.964Z,ns_1@cb.local:leader_lease_acquirer<0.561.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2020-06-16T21:32:37.964Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,2078178643},
                                                                {0,2078178643},
                                                                false,[]} to {[6,
                                                                               5],
                                                                              {0,
                                                                               2078178643},
                                                                              {0,
                                                                               2078178643},
                                                                              false,
                                                                              []}
[ns_server:debug,2020-06-16T21:32:37.966Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2020-06-16T21:32:37.967Z,ns_1@cb.local:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([audit,audit_decriptors,auto_failover_cfg,
                               buckets,client_cert_auth,
                               cluster_compat_version,max_bucket_count,
                               quorum_nodes,retry_rebalance,
                               scramsha_fallback_salt,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2020-06-16T21:32:37.971Z,ns_1@cb.local:memcached_permissions<0.333.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-06-16T21:32:37.976Z,ns_1@cb.local:leader_quorum_nodes_manager<0.563.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2020-06-16T21:32:37.991Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:trigger_ssl_reload:596]Notify services [ssl_service,capi_ssl_service] about client_cert_auth change
[ns_server:debug,2020-06-16T21:32:37.991Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:742]Going to notify following services: [capi_ssl_service,ssl_service]
[ns_server:debug,2020-06-16T21:32:37.993Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {32768,
  [{name,<<"Create Function">>},
   {description,<<"Eventing function definition was created or updated">>},
   {enabled,true},
   {module,eventing}]},
 {32769,
  [{name,<<"Delete Function">>},
   {description,<<"Eventing function definition was deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32770,
  [{name,<<"Fetch Functions">>},
   {description,<<"Eventing function definition was read">>},
   {enabled,false},
   {module,eventing}]},
 {32771,
  [{name,<<"List Deployed">>},
   {description,<<"Eventing deployed functions list was read">>},
   {enabled,false},
   {module,eventing}]},
 {32772,
  [{name,<<"Fetch Drafts">>},
   {description,<<"Eventing function draft definitions were read">>},
   {enabled,false},
   {module,eventing}]},
 {32773,
  [{name,<<"Delete Drafts">>},
   {description,<<"Eventing function draft definitions were deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32774,
  [{name,<<"Save Draft">>},
   {description,<<"Save a draft definition to the store">>},
   {enabled,true},
   {module,eventing}]},
 {32775,
  [{name,<<"Start Debug">>},
   {description,<<"Start eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32776,
  [{name,<<"Stop Debug">>},
   {description,<<"Stop eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32777,
  [{name,<<"Start Tracing">>},
   {description,<<"Start tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32778,
  [{name,<<"Stop Tracing">>},
   {description,<<"Stop tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32779,
  [{name,<<"Set Settings">>},
   {description,<<"Save settings for a given app">>},
   {enabled,true},
   {module,eventing}]},
 {32780,
  [{name,<<"Fetch Config">>},
   {description,<<"Get config for eventing">>},
   {enabled,false},
   {module,eventing}]},
 {32781,
  [{name,<<"Save Config">>},
   {description,<<"Save config for eventing">>},
   {enabled,true},
   {module,eventing}]},
 {32782,
  [{name,<<"Cleanup Eventing">>},
   {description,<<"Clears up app definitions and settings from metakv">>},
   {enabled,true},
   {module,eventing}]},
 {32783,
  [{name,<<"Get Settings">>},
   {description,<<"Get settings for a given app">>},
   {enabled,false},
   {module,eventing}]},
 {32784,
  [{name,<<"Import Functions">>},
   {description,<<"Import a list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32785,
  [{name,<<"Export Functions">>},
   {description,<<"Export the list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32786,
  [{name,<<"List Running">>},
   {description,<<"Eventing running function list was read">>},
   {enabled,false},
   {module,eventing}]},
 {36865,
  [{name,<<"Service configuration change">>},
   {description,<<"A successful service configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {36866,
  [{name,<<"Node configuration change">>},
   {description,<<"A successful node configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2020-06-16T21:32:37.994Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]}|
 <<23,98,128,12,178,147,38,16,61,167,19,38>>]
[ns_server:debug,2020-06-16T21:32:37.994Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2020-06-16T21:32:37.995Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562357}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2020-06-16T21:32:37.995Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2020-06-16T21:32:37.995Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{5,63759562357}}]},6,5]
[ns_server:debug,2020-06-16T21:32:37.995Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2020-06-16T21:32:37.995Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562357}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2020-06-16T21:32:37.996Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562357}}],{configs,[]}]
[ns_server:debug,2020-06-16T21:32:37.996Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]}|30]
[ns_server:debug,2020-06-16T21:32:37.996Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562357}}]},
 'ns_1@cb.local']
[ns_server:debug,2020-06-16T21:32:37.996Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{6,63759562357}}]}]
[ns_server:debug,2020-06-16T21:32:37.997Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.333.0>
[ns_server:debug,2020-06-16T21:32:37.997Z,ns_1@cb.local:memcached_permissions<0.333.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:32:37.997Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.333.0>
[ns_server:debug,2020-06-16T21:32:37.999Z,ns_1@cb.local:ns_config_rep<0.345.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2020-06-16T21:32:37.999Z,ns_1@cb.local:ns_config_rep<0.345.0>:ns_config_rep:handle_call:128]Fully synchronized config in 158 us
[user:warn,2020-06-16T21:32:37.999Z,ns_1@cb.local:compat_mode_manager<0.568.0>:compat_mode_manager:handle_consider_switching_compat_mode:49]Changed cluster compat mode from undefined to [6,5]
[error_logger:info,2020-06-16T21:32:38.001Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.568.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:38.002Z,ns_1@cb.local:<0.223.0>:restartable:loop:71]Restarting child <0.224.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.590.0>,#Ref<0.3111058533.1128267777.203975>}
[ns_server:debug,2020-06-16T21:32:38.003Z,ns_1@cb.local:<0.223.0>:restartable:shutdown_child:120]Successfully terminated process <0.224.0>
[ns_server:debug,2020-06-16T21:32:38.007Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:info,2020-06-16T21:32:38.056Z,ns_1@cb.local:<0.589.0>:ns_ssl_services_setup:notify_service:774]Successfully notified service capi_ssl_service
[ns_server:debug,2020-06-16T21:32:38.057Z,ns_1@cb.local:leader_lease_agent<0.553.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"b6e4175e17c89828057127316f6f28f4">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:info,2020-06-16T21:32:38.060Z,ns_1@cb.local:<0.593.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-06-16T21:32:38.060Z,ns_1@cb.local:<0.593.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-06-16T21:32:38.061Z,ns_1@cb.local:<0.593.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-06-16T21:32:38.061Z,ns_1@cb.local:<0.593.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2020-06-16T21:32:38.065Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.593.0>,menelaus_web}
             started: [{pid,<0.598.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 25,35,168,237,222,198,87,48,13,6,9,42,134,
                                 72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
                                 6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,97,55,
                                 57,102,102,48,51,54,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,97,
                                 55,57,102,102,48,51,54,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,174,49,109,223,
                                 144,181,107,20,62,244,159,125,87,29,246,113,
                                 69,75,245,111,1,104,124,43,204,65,119,204,
                                 96,227,54,122,27,191,136,13,92,215,195,141,
                                 75,237,12,149,132,248,32,77,248,79,40,207,
                                 27,36,163,189,100,221,217,205,200,152,182,
                                 219,68,47,165,243,99,145,176,30,251,88,9,
                                 191,84,122,129,12,171,97,57,65,117,125,201,
                                 187,161,29,55,194,30,179,108,82,182,70,39,
                                 238,138,116,199,165,87,79,107,145,116,250,
                                 124,126,48,140,190,101,28,25,74,212,33,10,
                                 220,201,83,77,196,78,26,171,13,190,112,217,
                                 232,242,223,218,25,215,78,185,111,98,28,131,
                                 240,85,124,175,54,208,58,18,75,254,127,17,
                                 160,74,103,211,94,73,212,86,137,34,50,83,13,
                                 254,231,46,78,93,215,205,139,19,221,23,123,
                                 237,17,128,66,215,13,15,101,154,18,227,32,
                                 124,218,146,24,199,197,156,196,98,54,249,
                                 162,187,115,68,170,8,100,160,207,69,98,155,
                                 127,228,31,192,139,179,81,185,239,116,200,
                                 26,127,204,129,56,156,113,231,187,199,137,
                                 166,176,132,181,123,246,124,38,169,227,195,
                                 19,133,100,121,173,2,3,1,0,1,163,56,48,54,
                                 48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,
                                 19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,
                                 3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,
                                 255,48,13,6,9,42,134,72,134,247,13,1,1,11,5,
                                 0,3,130,1,1,0,89,124,248,161,22,73,145,149,
                                 85,172,148,139,7,93,32,7,175,91,14,172,231,
                                 118,40,78,27,178,89,75,131,208,230,143,18,
                                 102,90,239,184,191,23,110,16,188,53,1,140,
                                 250,222,182,110,227,147,146,54,213,57,173,
                                 240,98,40,234,121,98,38,48,248,150,245,251,
                                 234,226,199,93,237,126,208,23,34,64,242,190,
                                 249,78,248,187,146,149,159,106,2,225,94,161,
                                 102,109,22,144,33,95,3,166,3,130,10,120,163,
                                 22,1,220,93,251,89,47,21,2,34,23,199,184,53,
                                 76,186,10,45,185,16,104,172,97,84,9,190,82,
                                 230,168,86,130,242,216,104,76,95,149,170,
                                 215,57,168,115,153,222,43,173,32,118,189,
                                 189,71,64,37,87,38,97,20,91,141,219,144,142,
                                 176,73,66,145,96,212,63,39,6,222,25,163,243,
                                 199,142,209,62,110,156,172,26,182,105,118,
                                 182,221,216,241,46,255,41,42,166,137,80,220,
                                 176,6,199,46,39,84,21,82,186,29,131,184,174,
                                 30,89,198,145,196,172,166,178,197,125,249,
                                 206,150,58,124,85,249,51,63,254,149,179,199,
                                 207,211,231,103,179,81,14,189,155,8,39,97,
                                 84,27,150,157,173>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:38.074Z,ns_1@cb.local:<0.594.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"b6e4175e17c89828057127316f6f28f4">>)
[error_logger:info,2020-06-16T21:32:38.080Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.616.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:32:38.081Z,ns_1@cb.local:<0.593.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-06-16T21:32:38.081Z,ns_1@cb.local:<0.593.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-06-16T21:32:38.084Z,ns_1@cb.local:<0.593.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-06-16T21:32:38.085Z,ns_1@cb.local:<0.593.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-06-16T21:32:38.089Z,ns_1@cb.local:<0.223.0>:restartable:start_child:98]Started child process <0.593.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2020-06-16T21:32:38.090Z,ns_1@cb.local:<0.590.0>:ns_ssl_services_setup:notify_service:774]Successfully notified service ssl_service
[ns_server:info,2020-06-16T21:32:38.090Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:758]Succesfully notified services [ssl_service,capi_ssl_service]
[ns_server:info,2020-06-16T21:32:38.091Z,ns_1@cb.local:ns_orchestrator_child_sup<0.595.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.635.0> on 'ns_1@cb.local'

[error_logger:info,2020-06-16T21:32:38.089Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.593.0>,menelaus_web}
             started: [{pid,<0.617.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 25,35,168,237,222,198,87,48,13,6,9,42,134,
                                 72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
                                 6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,97,55,
                                 57,102,102,48,51,54,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,97,
                                 55,57,102,102,48,51,54,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,174,49,109,223,
                                 144,181,107,20,62,244,159,125,87,29,246,113,
                                 69,75,245,111,1,104,124,43,204,65,119,204,
                                 96,227,54,122,27,191,136,13,92,215,195,141,
                                 75,237,12,149,132,248,32,77,248,79,40,207,
                                 27,36,163,189,100,221,217,205,200,152,182,
                                 219,68,47,165,243,99,145,176,30,251,88,9,
                                 191,84,122,129,12,171,97,57,65,117,125,201,
                                 187,161,29,55,194,30,179,108,82,182,70,39,
                                 238,138,116,199,165,87,79,107,145,116,250,
                                 124,126,48,140,190,101,28,25,74,212,33,10,
                                 220,201,83,77,196,78,26,171,13,190,112,217,
                                 232,242,223,218,25,215,78,185,111,98,28,131,
                                 240,85,124,175,54,208,58,18,75,254,127,17,
                                 160,74,103,211,94,73,212,86,137,34,50,83,13,
                                 254,231,46,78,93,215,205,139,19,221,23,123,
                                 237,17,128,66,215,13,15,101,154,18,227,32,
                                 124,218,146,24,199,197,156,196,98,54,249,
                                 162,187,115,68,170,8,100,160,207,69,98,155,
                                 127,228,31,192,139,179,81,185,239,116,200,
                                 26,127,204,129,56,156,113,231,187,199,137,
                                 166,176,132,181,123,246,124,38,169,227,195,
                                 19,133,100,121,173,2,3,1,0,1,163,56,48,54,
                                 48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,
                                 19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,
                                 3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,
                                 255,48,13,6,9,42,134,72,134,247,13,1,1,11,5,
                                 0,3,130,1,1,0,89,124,248,161,22,73,145,149,
                                 85,172,148,139,7,93,32,7,175,91,14,172,231,
                                 118,40,78,27,178,89,75,131,208,230,143,18,
                                 102,90,239,184,191,23,110,16,188,53,1,140,
                                 250,222,182,110,227,147,146,54,213,57,173,
                                 240,98,40,234,121,98,38,48,248,150,245,251,
                                 234,226,199,93,237,126,208,23,34,64,242,190,
                                 249,78,248,187,146,149,159,106,2,225,94,161,
                                 102,109,22,144,33,95,3,166,3,130,10,120,163,
                                 22,1,220,93,251,89,47,21,2,34,23,199,184,53,
                                 76,186,10,45,185,16,104,172,97,84,9,190,82,
                                 230,168,86,130,242,216,104,76,95,149,170,
                                 215,57,168,115,153,222,43,173,32,118,189,
                                 189,71,64,37,87,38,97,20,91,141,219,144,142,
                                 176,73,66,145,96,212,63,39,6,222,25,163,243,
                                 199,142,209,62,110,156,172,26,182,105,118,
                                 182,221,216,241,46,255,41,42,166,137,80,220,
                                 176,6,199,46,39,84,21,82,186,29,131,184,174,
                                 30,89,198,145,196,172,166,178,197,125,249,
                                 206,150,58,124,85,249,51,63,254,149,179,199,
                                 207,211,231,103,179,81,14,189,155,8,39,97,
                                 84,27,150,157,173>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.091Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.635.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:38.105Z,ns_1@cb.local:memcached_config_mgr<0.490.0>:memcached_config_mgr:apply_changed_memcached_config:179]New memcached config is hot-reloadable.
[ns_server:info,2020-06-16T21:32:38.106Z,ns_1@cb.local:ns_orchestrator_child_sup<0.595.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.636.0> on 'ns_1@cb.local'

[ns_server:info,2020-06-16T21:32:38.106Z,ns_1@cb.local:ns_orchestrator_child_sup<0.595.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.637.0> on 'ns_1@cb.local'

[error_logger:info,2020-06-16T21:32:38.106Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.636.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.107Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.637.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.111Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.595.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:38.111Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2020-06-16T21:32:38.113Z,ns_1@cb.local:memcached_config_mgr<0.490.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2020-06-16T21:32:38.122Z,ns_1@cb.local:<0.639.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-06-16T21:32:38.122Z,ns_1@cb.local:<0.639.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2020-06-16T21:32:38.140Z,ns_1@cb.local:ns_orchestrator_sup<0.566.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.639.0> on 'ns_1@cb.local'

[ns_server:debug,2020-06-16T21:32:38.141Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{7,63759562358}}]}]
[error_logger:info,2020-06-16T21:32:38.140Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.639.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:38.141Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562357}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:info,2020-06-16T21:32:38.141Z,ns_1@cb.local:mb_master_sup<0.560.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.642.0> on 'ns_1@cb.local'

[error_logger:info,2020-06-16T21:32:38.141Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.566.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:38.144Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.642.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:38.144Z,ns_1@cb.local:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:32:38.179Z,ns_1@cb.local:<0.647.0>:license_reporting:init:66]Starting license_reporting server
[ns_server:info,2020-06-16T21:32:38.179Z,ns_1@cb.local:mb_master_sup<0.560.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.647.0> on 'ns_1@cb.local'

[error_logger:info,2020-06-16T21:32:38.180Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.647.0>},
                       {id,license_reporting},
                       {mfargs,{license_reporting,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.180Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.558.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:38.181Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.554.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:38.181Z,ns_1@cb.local:<0.549.0>:restartable:start_child:98]Started child process <0.550.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2020-06-16T21:32:38.181Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.549.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:38.193Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.651.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.194Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.653.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.194Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.654.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.207Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.655.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.219Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.656.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.255Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.661.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.255Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.663.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:38.255Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.664.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.300Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.670.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2020-06-16T21:32:38.303Z,ns_1@cb.local:memcached_config_mgr<0.490.0>:memcached_config_mgr:hot_reload_config:248]Hot-reloaded memcached.json for config change of the following keys: [<<"client_cert_auth">>,
                                                                      <<"datatype_snappy">>,
                                                                      <<"scramsha_fallback_salt">>]
[error_logger:info,2020-06-16T21:32:38.310Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.676.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.311Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.658.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:38.316Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.678.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:32:38.341Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[error_logger:info,2020-06-16T21:32:38.342Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.679.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.342Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.320.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:32:38.341Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.207.0>
[ns_server:debug,2020-06-16T21:32:38.342Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2020-06-16T21:32:38.344Z,ns_1@cb.local:<0.206.0>:restartable:start_child:98]Started child process <0.207.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2020-06-16T21:32:38.344Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.206.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:38.349Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.681.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:32:38.351Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.185.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:32:38.352Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@cb.local'

[ns_server:debug,2020-06-16T21:32:38.352Z,ns_1@cb.local:<0.5.0>:child_erlang:child_loop:130]142: Entered child_loop
[ns_server:debug,2020-06-16T21:32:38.366Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2020-06-16T21:32:38.636Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.686.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.686.0>
[ns_server:debug,2020-06-16T21:32:38.637Z,ns_1@cb.local:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.686.0>} started
[ns_server:debug,2020-06-16T21:32:38.640Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.687.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.687.0>
[ns_server:debug,2020-06-16T21:32:38.656Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-06-16T21:32:38.669Z,ns_1@cb.local:ns_audit_cfg<0.485.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2020-06-16T21:32:38.698Z,ns_1@cb.local:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":7,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-06-16T21:32:38.718Z,ns_1@cb.local:ns_audit_cfg<0.485.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[ns_server:debug,2020-06-16T21:32:39.160Z,ns_1@cb.local:<0.639.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@cb.local',<<"a771316973056c9759aee152c7c79f8b">>} state new -> up
[ns_server:debug,2020-06-16T21:32:41.229Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {[],wrong_token}
[ns_server:debug,2020-06-16T21:32:42.331Z,ns_1@cb.local:ns_heart_slow_status_updater<0.373.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-06-16T21:32:42.719Z,ns_1@cb.local:goxdcr_status_keeper<0.522.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-06-16T21:32:42.721Z,ns_1@cb.local:goxdcr_status_keeper<0.522.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2020-06-16T21:33:07.803Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-06-16T21:33:07.803Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:33:07.803Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-06-16T21:33:07.803Z,ns_1@cb.local:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2020-06-16T21:33:33.357Z,ns_1@cb.local:netconfig_updater<0.202.0>:netconfig_updater:apply_config_unprotected:158]Node is going to apply the following settings: [{externalListeners,
                                                 [{inet,false},
                                                  {inet6,false}]}]
[ns_server:debug,2020-06-16T21:33:33.381Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated cb_dist config "/opt/couchbase/var/lib/couchbase/config/dist_cfg": [{external_listeners,
                                                                                      [inet_tcp_dist,
                                                                                       inet6_tcp_dist]},
                                                                                     {preferred_external_proto,
                                                                                      inet_tcp_dist},
                                                                                     {preferred_local_proto,
                                                                                      inet_tcp_dist}]
[ns_server:debug,2020-06-16T21:33:33.388Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Reloading configuration: [{external_listeners,
                                       [inet_tcp_dist,inet6_tcp_dist]},
                                   {preferred_external_proto,inet_tcp_dist},
                                   {preferred_local_proto,inet_tcp_dist}]
[ns_server:debug,2020-06-16T21:33:33.388Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{8,63759562413}}]}]
[ns_server:debug,2020-06-16T21:33:33.389Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|inet]
[ns_server:debug,2020-06-16T21:33:33.389Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{9,63759562413}}]}]
[ns_server:info,2020-06-16T21:33:33.390Z,ns_1@cb.local:netconfig_updater<0.202.0>:netconfig_updater:apply_config_unprotected:187]Node network settings ([{externalListeners,[{inet,false},{inet6,false}]}]) successfully applied
[ns_server:debug,2020-06-16T21:33:33.390Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]}|false]
[ns_server:debug,2020-06-16T21:33:33.391Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{10,63759562413}}]}]
[ns_server:debug,2020-06-16T21:33:33.392Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562351}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-06-16T21:33:33.392Z,ns_1@cb.local:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {node,'ns_1@cb.local',address_family},
                               {node,'ns_1@cb.local',erl_external_listeners},
                               {node,'ns_1@cb.local',node_encryption}]..)
[cluster:info,2020-06-16T21:33:33.444Z,ns_1@cb.local:ns_cluster<0.189.0>:ns_cluster:handle_call:355]Changing address to "127.0.0.1" due to client request
[cluster:info,2020-06-16T21:33:33.444Z,ns_1@cb.local:ns_cluster<0.189.0>:ns_cluster:do_change_address:596]Change of address to "127.0.0.1" is requested.
[ns_server:debug,2020-06-16T21:33:33.445Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Closing listener inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:33.445Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Closing listener inet6_tcp_dist
[ns_server:debug,2020-06-16T21:33:33.445Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.202391>,
                               inet_tcp_dist,<0.307.0>,
                               #Ref<0.3111058533.1128267777.203292>}
[error_logger:info,2020-06-16T21:33:33.445Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,626,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:33.445Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.202044>,
                               inet_tcp_dist,<0.183.0>,
                               #Ref<0.3111058533.1128267778.202049>}
[ns_server:debug,2020-06-16T21:33:33.463Z,nonode@nohost:<0.327.0>:misc:delaying_crash:1608]Delaying crash exit:{{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                                 [{ns_crash_log,'babysitter_of_ns_1@cb.local'},
                                  consume,infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,214}]},
             {ns_log,crash_consumption_loop,0,
                     [{file,"src/ns_log.erl"},{line,62}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1605}]},
             {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,232}]}]
[user:warn,2020-06-16T21:33:33.464Z,nonode@nohost:ns_node_disco<0.339.0>:ns_node_disco:handle_info:188]Node nonode@nohost saw that node 'ns_1@cb.local' went down. Details: [{nodedown_reason,
                                                                       net_kernel_terminated}]
[error_logger:error,2020-06-16T21:33:33.465Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]cb_dist: terminating with reason: shutdown
[ns_server:debug,2020-06-16T21:33:33.465Z,nonode@nohost:<0.1846.0>:dist_manager:teardown:306]Got nodedown msg {nodedown,'ns_1@cb.local',
                           [{nodedown_reason,net_kernel_terminated}]} after terminating net kernel
[ns_server:info,2020-06-16T21:33:33.466Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:do_adjust_address:322]Adjusted IP to "127.0.0.1"
[ns_server:info,2020-06-16T21:33:33.466Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2020-06-16T21:33:33.481Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.1850.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:33.482Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.1851.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:33.482Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.1849.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:33:33.483Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.1852.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:33.483Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.1854.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:33:33.484Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.1855.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:33:33.484Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.1853.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:33:33.484Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.1848.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:33:33.487Z,nonode@nohost:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config [{external_listeners,
                                        [inet_tcp_dist,inet6_tcp_dist]},
                                       {preferred_external_proto,
                                        inet_tcp_dist},
                                       {preferred_local_proto,inet_tcp_dist}]
[error_logger:info,2020-06-16T21:33:33.487Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.1856.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[ns_server:error,2020-06-16T21:33:33.495Z,nonode@nohost:ns_config<0.193.0>:ns_config:handle_info:900]Saving ns_config failed. Trying to ignore: {distribution_not_started,
                                            [{auth,set_cookie,2,
                                              [{file,"auth.erl"},{line,120}]},
                                             {ns_server,get_babysitter_node,
                                              0,
                                              [{file,"src/ns_server.erl"},
                                               {line,280}]},
                                             {encryption_service,
                                              maybe_clear_backup_key,1,
                                              [{file,
                                                "src/encryption_service.erl"},
                                               {line,76}]},
                                             {proc_lib,init_p_do_apply,3,
                                              [{file,"proc_lib.erl"},
                                               {line,247}]}]}
[ns_server:debug,2020-06-16T21:33:33.496Z,nonode@nohost:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2020-06-16T21:33:33.496Z,nonode@nohost:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2020-06-16T21:33:33.497Z,nonode@nohost:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2020-06-16T21:33:33.500Z,ns_1@127.0.0.1:<0.268.0>:doc_replicator:nodeup_monitoring_loop:124]got nodeup event. Considering ddocs replication
[user:info,2020-06-16T21:33:33.500Z,ns_1@127.0.0.1:ns_node_disco<0.339.0>:ns_node_disco:handle_info:182]Node 'ns_1@127.0.0.1' saw that node 'ns_1@127.0.0.1' came up. Tags: []
[ns_server:debug,2020-06-16T21:33:33.500Z,ns_1@127.0.0.1:users_replicator<0.266.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-06-16T21:33:33.500Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2020-06-16T21:33:33.500Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[error_logger:error,2020-06-16T21:33:33.500Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_config_default:encrypt_and_save/1
    pid: <0.1842.0>
    registered_name: []
    exception error: distribution_not_started
      in function  auth:set_cookie/2 (auth.erl, line 120)
      in call from ns_server:get_babysitter_node/0 (src/ns_server.erl, line 280)
      in call from encryption_service:maybe_clear_backup_key/1 (src/encryption_service.erl, line 76)
    ancestors: [ns_config,ns_config_sup,ns_server_cluster_sup,root_sup,
                  <0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.193.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 135124
  neighbours:

[error_logger:info,2020-06-16T21:33:33.501Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.1857.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:33.501Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.1858.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:33.501Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.1859.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:33.502Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.1847.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:33:33.510Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2020-06-16T21:33:33.510Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2020-06-16T21:33:33.511Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:33.511Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:33.511Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267777.209981>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:33:33.511Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267777.209981>,
                                  inet_tcp_dist,<0.1868.0>,
                                  #Ref<0.3111058533.1128267778.208075>}
[ns_server:debug,2020-06-16T21:33:33.516Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2020-06-16T21:33:33.517Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:do_adjust_address:326]Re-setting cookie {{sanitized,<<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>},
                   'ns_1@127.0.0.1'}
[ns_server:info,2020-06-16T21:33:33.525Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2020-06-16T21:33:33.526Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2020-06-16T21:33:33.534Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[ns_server:debug,2020-06-16T21:33:33.534Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:complete_rename:370]Renaming node from 'ns_1@cb.local' to 'ns_1@127.0.0.1' in config
[error_logger:info,2020-06-16T21:33:33.534Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:33.534Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',erl_external_listeners} -> {node,
                                                                     'ns_1@127.0.0.1',
                                                                     erl_external_listeners}:
  [{inet,false},{inet6,false}] ->
  [{inet,false},{inet6,false}]
[ns_server:debug,2020-06-16T21:33:33.534Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',node_encryption} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              node_encryption}:
  false ->
  false
[ns_server:debug,2020-06-16T21:33:33.534Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:33.534Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',address_family} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             address_family}:
  inet ->
  inet
[ns_server:debug,2020-06-16T21:33:33.534Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267777.209993>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:33:33.534Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267777.209993>,
                                  inet_tcp_dist,<0.1870.0>,
                                  #Ref<0.3111058533.1128267777.209997>}
[ns_server:debug,2020-06-16T21:33:33.536Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Accepted new connection from <0.1865.0>: {con,
                                                   #Ref<0.3111058533.1128267777.210000>,
                                                   inet_tcp_dist,undefined,
                                                   undefined}
[ns_server:debug,2020-06-16T21:33:33.536Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Accepting connection from acceptor <0.1865.0> using module inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:33.536Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267777.210000>,
                                  inet_tcp_dist,<0.1872.0>,
                                  #Ref<0.3111058533.1128267777.210002>}
[ns_server:debug,2020-06-16T21:33:33.536Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267777.209993>,
                               inet_tcp_dist,<0.1870.0>,
                               #Ref<0.3111058533.1128267777.209997>}
[error_logger:info,2020-06-16T21:33:33.536Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1870.0>,
                    {recv_challenge_failed,no_node,"ns_1@127.0.0.1"}}}
[error_logger:info,2020-06-16T21:33:33.537Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:33.537Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',eventing_dir} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           eventing_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2020-06-16T21:33:33.537Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267777.210000>,
                               inet_tcp_dist,<0.1872.0>,
                               #Ref<0.3111058533.1128267777.210002>}
[error_logger:info,2020-06-16T21:33:33.537Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1872.0>,{recv_challenge_reply_failed,{error,closed}}}}
[error_logger:info,2020-06-16T21:33:33.537Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@127.0.0.1'}}
[ns_server:debug,2020-06-16T21:33:33.537Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_dirs} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        cbas_dirs}:
  ["/opt/couchbase/var/lib/couchbase/data"] ->
  ["/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-06-16T21:33:33.538Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf nodes_wanted -> nodes_wanted:
  ['ns_1@cb.local'] ->
  ['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:33.538Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf quorum_nodes -> quorum_nodes:
  ['ns_1@cb.local'] ->
  ['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:33.538Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf server_groups -> server_groups:
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]] ->
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-06-16T21:33:33.538Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',audit} -> {node,'ns_1@127.0.0.1',
                                                    audit}:
  [] ->
  []
[ns_server:debug,2020-06-16T21:33:33.538Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',capi_port} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        capi_port}:
  8092 ->
  8092
[ns_server:debug,2020-06-16T21:33:33.538Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_admin_port} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              cbas_admin_port}:
  9110 ->
  9110
[ns_server:debug,2020-06-16T21:33:33.539Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_cc_client_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  cbas_cc_client_port}:
  9113 ->
  9113
[ns_server:debug,2020-06-16T21:33:33.539Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_cc_cluster_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   cbas_cc_cluster_port}:
  9112 ->
  9112
[ns_server:debug,2020-06-16T21:33:33.539Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_cc_http_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_cc_http_port}:
  9111 ->
  9111
[ns_server:debug,2020-06-16T21:33:33.539Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_cluster_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_cluster_port}:
  9115 ->
  9115
[ns_server:debug,2020-06-16T21:33:33.539Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_console_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_console_port}:
  9114 ->
  9114
[ns_server:debug,2020-06-16T21:33:33.539Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_data_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             cbas_data_port}:
  9116 ->
  9116
[ns_server:debug,2020-06-16T21:33:33.540Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_debug_port} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              cbas_debug_port}:
  -1 ->
  -1
[ns_server:debug,2020-06-16T21:33:33.540Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_http_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             cbas_http_port}:
  8095 ->
  8095
[ns_server:debug,2020-06-16T21:33:33.540Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_messaging_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  cbas_messaging_port}:
  9118 ->
  9118
[ns_server:debug,2020-06-16T21:33:33.540Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_metadata_callback_port} -> {node,
                                                                          'ns_1@127.0.0.1',
                                                                          cbas_metadata_callback_port}:
  9119 ->
  9119
[ns_server:debug,2020-06-16T21:33:33.540Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_metadata_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 cbas_metadata_port}:
  9121 ->
  9121
[ns_server:debug,2020-06-16T21:33:33.540Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_parent_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               cbas_parent_port}:
  9122 ->
  9122
[ns_server:debug,2020-06-16T21:33:33.540Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_replication_port} -> {node,
                                                                    'ns_1@127.0.0.1',
                                                                    cbas_replication_port}:
  9120 ->
  9120
[ns_server:debug,2020-06-16T21:33:33.541Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_result_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               cbas_result_port}:
  9117 ->
  9117
[ns_server:debug,2020-06-16T21:33:33.541Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',cbas_ssl_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            cbas_ssl_port}:
  18095 ->
  18095
[ns_server:debug,2020-06-16T21:33:33.541Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',compaction_daemon} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                compaction_daemon}:
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}] ->
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]
[ns_server:debug,2020-06-16T21:33:33.541Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',config_version} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             config_version}:
  {6,5,1} ->
  {6,5,1}
[ns_server:debug,2020-06-16T21:33:33.541Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',eventing_debug_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  eventing_debug_port}:
  9140 ->
  9140
[ns_server:debug,2020-06-16T21:33:33.541Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',eventing_http_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 eventing_http_port}:
  8096 ->
  8096
[ns_server:debug,2020-06-16T21:33:33.542Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',eventing_https_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  eventing_https_port}:
  18096 ->
  18096
[ns_server:debug,2020-06-16T21:33:33.542Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',fts_grpc_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            fts_grpc_port}:
  9130 ->
  9130
[ns_server:debug,2020-06-16T21:33:33.542Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',fts_grpc_ssl_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                fts_grpc_ssl_port}:
  19130 ->
  19130
[ns_server:debug,2020-06-16T21:33:33.542Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',fts_http_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            fts_http_port}:
  8094 ->
  8094
[ns_server:debug,2020-06-16T21:33:33.542Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',fts_ssl_port} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           fts_ssl_port}:
  18094 ->
  18094
[ns_server:debug,2020-06-16T21:33:33.542Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_admin_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 indexer_admin_port}:
  9100 ->
  9100
[ns_server:debug,2020-06-16T21:33:33.542Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_http_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                indexer_http_port}:
  9102 ->
  9102
[ns_server:debug,2020-06-16T21:33:33.543Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_https_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 indexer_https_port}:
  19102 ->
  19102
[ns_server:debug,2020-06-16T21:33:33.543Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_scan_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                indexer_scan_port}:
  9101 ->
  9101
[ns_server:debug,2020-06-16T21:33:33.543Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_stcatchup_port} -> {node,
                                                                     'ns_1@127.0.0.1',
                                                                     indexer_stcatchup_port}:
  9104 ->
  9104
[ns_server:debug,2020-06-16T21:33:33.543Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_stinit_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  indexer_stinit_port}:
  9103 ->
  9103
[ns_server:debug,2020-06-16T21:33:33.543Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',indexer_stmaint_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   indexer_stmaint_port}:
  9105 ->
  9105
[ns_server:debug,2020-06-16T21:33:33.543Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',is_enterprise} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            is_enterprise}:
  true ->
  true
[ns_server:debug,2020-06-16T21:33:33.544Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',isasl} -> {node,'ns_1@127.0.0.1',
                                                    isasl}:
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}] ->
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-06-16T21:33:33.544Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',membership} -> {node,
                                                         'ns_1@127.0.0.1',
                                                         membership}:
  active ->
  active
[ns_server:debug,2020-06-16T21:33:33.547Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',memcached} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        memcached}:
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}] ->
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]
[ns_server:debug,2020-06-16T21:33:33.548Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',memcached_config} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               memcached_config}:
  {[{interfaces,
        {memcached_config_mgr,omit_missing_mcd_ports,
            [{[{host,<<"*">>},
               {port,port},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,dedicated_port},
               {system,true},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,ssl_port},
               {ssl,
                   {[{key,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
                     {cert,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,dedicated_ssl_port},
               {system,true},
               {ssl,
                   {[{key,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
                     {cert,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
    {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
    {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {opentracing,
        {[{enabled,opentracing_enabled},
          {module,{"~s",[opentracing_module]}},
          {config,{"~s",[opentracing_config]}}]}},
    {admin,{"~s",[admin_user]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize},
          {sleeptime,log_sleeptime}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}}]} ->
  {[{interfaces,
        {memcached_config_mgr,omit_missing_mcd_ports,
            [{[{host,<<"*">>},
               {port,port},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,dedicated_port},
               {system,true},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,ssl_port},
               {ssl,
                   {[{key,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
                     {cert,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
             {[{host,<<"*">>},
               {port,dedicated_ssl_port},
               {system,true},
               {ssl,
                   {[{key,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
                     {cert,
                         <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
               {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
               {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
    {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
    {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {opentracing,
        {[{enabled,opentracing_enabled},
          {module,{"~s",[opentracing_module]}},
          {config,{"~s",[opentracing_config]}}]}},
    {admin,{"~s",[admin_user]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize},
          {sleeptime,log_sleeptime}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}}]}
[ns_server:debug,2020-06-16T21:33:33.549Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',memcached_dedicated_ssl_port} -> {node,
                                                                           'ns_1@127.0.0.1',
                                                                           memcached_dedicated_ssl_port}:
  11206 ->
  11206
[ns_server:debug,2020-06-16T21:33:33.550Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',memcached_defaults} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 memcached_defaults}:
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}] ->
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-06-16T21:33:33.550Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',moxi} -> {node,'ns_1@127.0.0.1',moxi}:
  [{port,0}] ->
  [{port,0}]
[ns_server:debug,2020-06-16T21:33:33.551Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',ns_log} -> {node,'ns_1@127.0.0.1',
                                                     ns_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-06-16T21:33:33.551Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',port_servers} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           port_servers}:
  [] ->
  []
[ns_server:debug,2020-06-16T21:33:33.551Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',projector_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             projector_port}:
  9999 ->
  9999
[ns_server:debug,2020-06-16T21:33:33.551Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',projector_ssl_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 projector_ssl_port}:
  9999 ->
  9999
[ns_server:debug,2020-06-16T21:33:33.551Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',query_port} -> {node,
                                                         'ns_1@127.0.0.1',
                                                         query_port}:
  8093 ->
  8093
[ns_server:debug,2020-06-16T21:33:33.551Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',rest} -> {node,'ns_1@127.0.0.1',rest}:
  [{port,8091},{port_meta,global}] ->
  [{port,8091},{port_meta,global}]
[ns_server:debug,2020-06-16T21:33:33.551Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',saslauthd_enabled} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                saslauthd_enabled}:
  true ->
  true
[ns_server:debug,2020-06-16T21:33:33.552Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',ssl_capi_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            ssl_capi_port}:
  18092 ->
  18092
[ns_server:debug,2020-06-16T21:33:33.552Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',ssl_query_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             ssl_query_port}:
  18093 ->
  18093
[ns_server:debug,2020-06-16T21:33:33.552Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',ssl_rest_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            ssl_rest_port}:
  18091 ->
  18091
[ns_server:debug,2020-06-16T21:33:33.552Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',uuid} -> {node,'ns_1@127.0.0.1',uuid}:
  <<"a771316973056c9759aee152c7c79f8b">> ->
  <<"a771316973056c9759aee152c7c79f8b">>
[ns_server:debug,2020-06-16T21:33:33.552Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',xdcr_rest_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             xdcr_rest_port}:
  9998 ->
  9998
[ns_server:debug,2020-06-16T21:33:33.553Z,ns_1@127.0.0.1:ns_config<0.193.0>:dist_manager:rename_node_in_config:392]renaming node conf {node,'ns_1@cb.local',{project_intact,is_vulnerable}} -> {node,
                                                                             'ns_1@127.0.0.1',
                                                                             {project_intact,
                                                                              is_vulnerable}}:
  false ->
  false
[ns_server:debug,2020-06-16T21:33:33.553Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([nodes_wanted,quorum_nodes,server_groups,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_dir},
                               {node,'ns_1@127.0.0.1',eventing_http_port},
                               {node,'ns_1@127.0.0.1',eventing_https_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
                               {node,'ns_1@127.0.0.1',fts_http_port},
                               {node,'ns_1@127.0.0.1',fts_ssl_port},
                               {node,'ns_1@127.0.0.1',indexer_admin_port},
                               {node,'ns_1@127.0.0.1',indexer_http_port},
                               {node,'ns_1@127.0.0.1',indexer_https_port},
                               {node,'ns_1@127.0.0.1',indexer_scan_port},
                               {node,'ns_1@127.0.0.1',indexer_stcatchup_port},
                               {node,'ns_1@127.0.0.1',indexer_stinit_port},
                               {node,'ns_1@127.0.0.1',indexer_stmaint_port},
                               {node,'ns_1@127.0.0.1',is_enterprise},
                               {node,'ns_1@127.0.0.1',isasl},
                               {node,'ns_1@127.0.0.1',membership},
                               {node,'ns_1@127.0.0.1',memcached},
                               {node,'ns_1@127.0.0.1',memcached_config},
                               {node,'ns_1@127.0.0.1',
                                   memcached_dedicated_ssl_port},
                               {node,'ns_1@127.0.0.1',memcached_defaults},
                               {node,'ns_1@127.0.0.1',moxi},
                               {node,'ns_1@127.0.0.1',node_encryption},
                               {node,'ns_1@127.0.0.1',ns_log},
                               {node,'ns_1@127.0.0.1',port_servers},
                               {node,'ns_1@127.0.0.1',projector_port},
                               {node,'ns_1@127.0.0.1',projector_ssl_port},
                               {node,'ns_1@127.0.0.1',query_port},
                               {node,'ns_1@127.0.0.1',rest},
                               {node,'ns_1@127.0.0.1',saslauthd_enabled},
                               {node,'ns_1@127.0.0.1',ssl_capi_port},
                               {node,'ns_1@127.0.0.1',ssl_query_port},
                               {node,'ns_1@127.0.0.1',ssl_rest_port},
                               {node,'ns_1@127.0.0.1',uuid},
                               {node,'ns_1@127.0.0.1',xdcr_rest_port},
                               {node,'ns_1@127.0.0.1',
                                   {project_intact,is_vulnerable}}]..)
[ns_server:debug,2020-06-16T21:33:33.555Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:info,2020-06-16T21:33:33.560Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:handle_info:544]Got certificate and pkey change
[ns_server:warn,2020-06-16T21:33:33.560Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.563.0>:leader_quorum_nodes_manager:handle_quorum_nodes_updated:170]Somebody else updated the quorum nodes when we are the master node.
Our quorum nodes: ['ns_1@cb.local']
Their quorum nodes: ['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:33.561Z,ns_1@127.0.0.1:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2020-06-16T21:33:33.564Z,ns_1@127.0.0.1:mb_master<0.558.0>:mb_master:update_peers:577]List of peers has changed from ['ns_1@cb.local'] to ['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:33.564Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{11,63759562413}}]}]
[ns_server:debug,2020-06-16T21:33:33.564Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-06-16T21:33:33.564Z,ns_1@127.0.0.1:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2020-06-16T21:33:33.564Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Accepted new connection from <0.1865.0>: {con,
                                                   #Ref<0.3111058533.1128267778.208108>,
                                                   inet_tcp_dist,undefined,
                                                   undefined}
[ns_server:debug,2020-06-16T21:33:33.564Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|false]
[error_logger:error,2020-06-16T21:33:33.561Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server leader_quorum_nodes_manager terminating 
** Last message in was quorum_nodes_updated
** When Server state == {state,{set,1,16,16,8,80,48,
                                    {[],[],[],[],[],[],[],[],[],[],[],[],[],
                                     [],[],[]},
                                    {{[],[],[],[],[],[],[],[],[],[],[],[],
                                      ['ns_1@cb.local'],
                                      [],[],[]}}}}
** Reason for termination == 
** {{quorum_nodes_update_conflict,['ns_1@cb.local'],['ns_1@127.0.0.1']},
    [{leader_quorum_nodes_manager,handle_quorum_nodes_updated,1,
         [{file,"src/leader_quorum_nodes_manager.erl"},{line,175}]},
     {leader_quorum_nodes_manager,handle_info,2,
         [{file,"src/leader_quorum_nodes_manager.erl"},{line,95}]},
     {gen_server2,handle_info,2,[{file,"src/gen_server2.erl"},{line,228}]},
     {gen_server,try_dispatch,4,[{file,"gen_server.erl"},{line,616}]},
     {gen_server,handle_msg,6,[{file,"gen_server.erl"},{line,686}]},
     {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,247}]}]}

[ns_server:debug,2020-06-16T21:33:33.564Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9998]
[ns_server:debug,2020-06-16T21:33:33.565Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|
 <<"a771316973056c9759aee152c7c79f8b">>]
[ns_server:debug,2020-06-16T21:33:33.565Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Accepting connection from acceptor <0.1865.0> using module inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:33.565Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|18091]
[ns_server:debug,2020-06-16T21:33:33.565Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|18093]
[ns_server:debug,2020-06-16T21:33:33.565Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.208108>,
                                  inet_tcp_dist,<0.1881.0>,
                                  #Ref<0.3111058533.1128267778.208111>}
[ns_server:debug,2020-06-16T21:33:33.566Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|18092]
[ns_server:debug,2020-06-16T21:33:33.566Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|true]
[ns_server:debug,2020-06-16T21:33:33.566Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2020-06-16T21:33:33.566Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|8093]
[ns_server:debug,2020-06-16T21:33:33.567Z,ns_1@127.0.0.1:<0.1878.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}
[ns_server:debug,2020-06-16T21:33:33.567Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9999]
[ns_server:debug,2020-06-16T21:33:33.567Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9999]
[ns_server:debug,2020-06-16T21:33:33.567Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}]
[ns_server:debug,2020-06-16T21:33:33.568Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2020-06-16T21:33:33.568Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.208108>,
                               inet_tcp_dist,<0.1881.0>,
                               #Ref<0.3111058533.1128267778.208111>}
[error_logger:info,2020-06-16T21:33:33.568Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1881.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2020-06-16T21:33:33.568Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 {port,0}]
[ns_server:debug,2020-06-16T21:33:33.568Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'couchdb_ns_1@cb.local' to be established
[ns_server:debug,2020-06-16T21:33:33.569Z,ns_1@127.0.0.1:<0.1878.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"vXoswWZt5HCWXBl8atVyoGYARjI7x8y/PtwJau4o5cE=">>}
[error_logger:info,2020-06-16T21:33:33.569Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:33.569Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2020-06-16T21:33:33.569Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:33.570Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.208128>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:33:33.569Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|11206]
[ns_server:debug,2020-06-16T21:33:33.570Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-06-16T21:33:33.570Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.208128>,
                                  inet_tcp_dist,<0.1892.0>,
                                  #Ref<0.3111058533.1128267778.208129>}
[error_logger:info,2020-06-16T21:33:33.570Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:33.571Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:33.572Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.208133>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:33:33.572Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2020-06-16T21:33:33.573Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.208133>,
                                  inet_tcp_dist,<0.1896.0>,
                                  #Ref<0.3111058533.1128267778.208134>}
[ns_server:debug,2020-06-16T21:33:33.578Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'couchdb_ns_1@cb.local' to come up
[ns_server:debug,2020-06-16T21:33:33.578Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2020-06-16T21:33:33.584Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|
 active]
[ns_server:debug,2020-06-16T21:33:33.584Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Accepted new connection from <0.1865.0>: {con,
                                                   #Ref<0.3111058533.1128267778.208140>,
                                                   inet_tcp_dist,undefined,
                                                   undefined}
[ns_server:debug,2020-06-16T21:33:33.585Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2020-06-16T21:33:33.585Z,ns_1@127.0.0.1:leader_activities<0.552.0>:leader_activities:handle_internal_process_down:511]Process {quorum_nodes_manager,<0.563.0>} terminated with reason {quorum_nodes_update_conflict,
                                                                 ['ns_1@cb.local'],
                                                                 ['ns_1@127.0.0.1']}
[ns_server:debug,2020-06-16T21:33:33.585Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Accepting connection from acceptor <0.1865.0> using module inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:33.586Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.1898.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2020-06-16T21:33:33.587Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|true]
[ns_server:debug,2020-06-16T21:33:33.587Z,ns_1@127.0.0.1:<0.580.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.563.0>} exited with reason {quorum_nodes_update_conflict,
                                                                                ['ns_1@cb.local'],
                                                                                ['ns_1@127.0.0.1']}
[ns_server:debug,2020-06-16T21:33:33.587Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.208140>,
                                  inet_tcp_dist,<0.1899.0>,
                                  #Ref<0.3111058533.1128267778.208142>}
[ns_server:debug,2020-06-16T21:33:33.587Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9105]
[error_logger:error,2020-06-16T21:33:33.587Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: leader_quorum_nodes_manager:init/1
    pid: <0.563.0>
    registered_name: leader_quorum_nodes_manager
    exception exit: {quorum_nodes_update_conflict,
                        ['ns_1@cb.local'],
                        ['ns_1@127.0.0.1']}
      in function  leader_quorum_nodes_manager:handle_quorum_nodes_updated/1 (src/leader_quorum_nodes_manager.erl, line 175)
      in call from leader_quorum_nodes_manager:handle_info/2 (src/leader_quorum_nodes_manager.erl, line 95)
      in call from gen_server2:handle_info/2 (src/gen_server2.erl, line 228)
      in call from gen_server:try_dispatch/4 (gen_server.erl, line 616)
      in call from gen_server:handle_msg/6 (gen_server.erl, line 686)
    ancestors: [mb_master_sup,mb_master,leader_registry_sup,
                  leader_services_sup,<0.549.0>,ns_server_sup,
                  ns_server_nodes_sup,<0.206.0>,ns_server_cluster_sup,
                  root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.560.0>,<0.580.0>]
    dictionary: [{{'$gen_server2',{have_callback,init}},true},
                  {{'$gen_server2',{have_callback,code_change}},false},
                  {{'$gen_server2',{have_callback,handle_cast}},false},
                  {{'$gen_server2',{have_callback,handle_call}},true},
                  {{'$gen_server2',{have_callback,terminate}},false},
                  {{'$gen_server2',module},leader_quorum_nodes_manager},
                  {{'$gen_server2',{have_callback,handle_info}},true},
                  {{'$gen_server2',{have_callback,handle_job_death}},false}]
    trap_exit: true
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 7862
  neighbours:

[ns_server:debug,2020-06-16T21:33:33.588Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9103]
[ns_server:debug,2020-06-16T21:33:33.588Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9104]
[error_logger:error,2020-06-16T21:33:33.588Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,mb_master_sup}
     Context:    child_terminated
     Reason:     {quorum_nodes_update_conflict,
                     ['ns_1@cb.local'],
                     ['ns_1@127.0.0.1']}
     Offender:   [{pid,<0.563.0>},
                  {id,leader_quorum_nodes_manager},
                  {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.208133>,
                               inet_tcp_dist,<0.1896.0>,
                               #Ref<0.3111058533.1128267778.208134>}
[ns_server:debug,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.208140>,
                               inet_tcp_dist,<0.1899.0>,
                               #Ref<0.3111058533.1128267778.208142>}
[error_logger:info,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.1898.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:33.588Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9101]
[ns_server:debug,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|19102]
[error_logger:info,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1896.0>,
                    {recv_challenge_failed,no_node,"ns_1@127.0.0.1"}}}
[error_logger:info,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9102]
[error_logger:info,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1899.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9100]
[error_logger:info,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@127.0.0.1'}}
[ns_server:debug,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.1898.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|18094]
[ns_server:debug,2020-06-16T21:33:33.589Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|8094]
[ns_server:debug,2020-06-16T21:33:33.590Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|19130]
[ns_server:debug,2020-06-16T21:33:33.590Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9130]
[ns_server:debug,2020-06-16T21:33:33.590Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|18096]
[ns_server:debug,2020-06-16T21:33:33.590Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|8096]
[ns_server:debug,2020-06-16T21:33:33.591Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9140]
[ns_server:debug,2020-06-16T21:33:33.591Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|
 {6,5,1}]
[ns_server:debug,2020-06-16T21:33:33.592Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2020-06-16T21:33:33.593Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|18095]
[ns_server:debug,2020-06-16T21:33:33.593Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9117]
[ns_server:debug,2020-06-16T21:33:33.593Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9120]
[ns_server:debug,2020-06-16T21:33:33.594Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9122]
[ns_server:debug,2020-06-16T21:33:33.594Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9121]
[ns_server:debug,2020-06-16T21:33:33.594Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9119]
[ns_server:debug,2020-06-16T21:33:33.595Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9118]
[ns_server:debug,2020-06-16T21:33:33.595Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|8095]
[ns_server:debug,2020-06-16T21:33:33.596Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|-1]
[ns_server:debug,2020-06-16T21:33:33.596Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9116]
[ns_server:debug,2020-06-16T21:33:33.597Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9114]
[ns_server:debug,2020-06-16T21:33:33.597Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9115]
[ns_server:debug,2020-06-16T21:33:33.597Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9111]
[ns_server:debug,2020-06-16T21:33:33.598Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9112]
[ns_server:debug,2020-06-16T21:33:33.598Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9113]
[ns_server:debug,2020-06-16T21:33:33.598Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|9110]
[ns_server:debug,2020-06-16T21:33:33.599Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|8092]
[ns_server:debug,2020-06-16T21:33:33.599Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}]
[ns_server:debug,2020-06-16T21:33:33.599Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
server_groups ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562413}}]},
 [{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2020-06-16T21:33:33.599Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:33.600Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562413}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:33.600Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2020-06-16T21:33:33.600Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2020-06-16T21:33:33.601Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|inet]
[ns_server:debug,2020-06-16T21:33:33.601Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]}|false]
[ns_server:debug,2020-06-16T21:33:33.602Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562413}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2020-06-16T21:33:33.603Z,ns_1@127.0.0.1:dist_manager<0.166.0>:dist_manager:complete_rename:373]Node 'ns_1@cb.local' has been renamed to 'ns_1@127.0.0.1'.
[ns_server:info,2020-06-16T21:33:33.619Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:maybe_generate_local_cert:619]Detected existing node certificate that did not match cluster certificate. Will re-generate
[ns_server:warn,2020-06-16T21:33:33.938Z,ns_1@127.0.0.1:ns_tick_agent<0.651.0>:ns_tick_agent:handle_tick:71]Ignoring tick from a non-master node 'ns_1@cb.local'. Master: 'ns_1@127.0.0.1'
[error_logger:info,2020-06-16T21:33:34.131Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:34.132Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:34.132Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267778.208175>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:33:34.132Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.208175>,
                                  inet_tcp_dist,<0.1913.0>,
                                  #Ref<0.3111058533.1128267778.208179>}
[ns_server:debug,2020-06-16T21:33:34.133Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Accepted new connection from <0.1865.0>: {con,
                                                   #Ref<0.3111058533.1128267777.210175>,
                                                   inet_tcp_dist,undefined,
                                                   undefined}
[ns_server:debug,2020-06-16T21:33:34.133Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Accepting connection from acceptor <0.1865.0> using module inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:34.133Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267777.210175>,
                                  inet_tcp_dist,<0.1915.0>,
                                  #Ref<0.3111058533.1128267777.210177>}
[error_logger:info,2020-06-16T21:33:34.134Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1913.0>,
                    {recv_challenge_failed,no_node,"ns_1@127.0.0.1"}}}
[error_logger:info,2020-06-16T21:33:34.134Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@cb.local'}}
[ns_server:warn,2020-06-16T21:33:34.134Z,ns_1@127.0.0.1:<0.594.0>:leader_lease_acquire_worker:handle_exception:250]Failed to acquire lease from 'ns_1@cb.local': {exit,
                                               {{nodedown,'ns_1@cb.local'},
                                                {gen_server,call,
                                                 [{leader_lease_agent,
                                                   'ns_1@cb.local'},
                                                  {acquire_lease,
                                                   'ns_1@127.0.0.1',
                                                   <<"b6e4175e17c89828057127316f6f28f4">>,
                                                   [{timeout,7999},
                                                    {period,15000}]},
                                                  infinity]}}}
[ns_server:debug,2020-06-16T21:33:34.134Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.208175>,
                               inet_tcp_dist,<0.1913.0>,
                               #Ref<0.3111058533.1128267778.208179>}
[ns_server:debug,2020-06-16T21:33:34.134Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267777.210175>,
                               inet_tcp_dist,<0.1915.0>,
                               #Ref<0.3111058533.1128267777.210177>}
[error_logger:info,2020-06-16T21:33:34.135Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1915.0>,{recv_challenge_reply_failed,{error,closed}}}}
[error_logger:info,2020-06-16T21:33:34.135Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@127.0.0.1'}}
[ns_server:debug,2020-06-16T21:33:34.146Z,ns_1@127.0.0.1:<0.639.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@127.0.0.1',<<"a771316973056c9759aee152c7c79f8b">>} state new -> up
[error_logger:info,2020-06-16T21:33:34.146Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:34.146Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:34.146Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267777.210193>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:33:34.146Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267777.210193>,
                                  inet_tcp_dist,<0.1917.0>,
                                  #Ref<0.3111058533.1128267777.210197>}
[ns_server:debug,2020-06-16T21:33:34.147Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Accepted new connection from <0.1865.0>: {con,
                                                   #Ref<0.3111058533.1128267778.208188>,
                                                   inet_tcp_dist,undefined,
                                                   undefined}
[ns_server:debug,2020-06-16T21:33:34.148Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Accepting connection from acceptor <0.1865.0> using module inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:34.148Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.208188>,
                                  inet_tcp_dist,<0.1919.0>,
                                  #Ref<0.3111058533.1128267778.208190>}
[error_logger:info,2020-06-16T21:33:34.148Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1917.0>,
                    {recv_challenge_failed,no_node,"ns_1@127.0.0.1"}}}
[error_logger:info,2020-06-16T21:33:34.148Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:34.148Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267777.210193>,
                               inet_tcp_dist,<0.1917.0>,
                               #Ref<0.3111058533.1128267777.210197>}
[ns_server:debug,2020-06-16T21:33:34.149Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.208188>,
                               inet_tcp_dist,<0.1919.0>,
                               #Ref<0.3111058533.1128267778.208190>}
[error_logger:info,2020-06-16T21:33:34.149Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1919.0>,{recv_challenge_reply_failed,{error,closed}}}}
[error_logger:info,2020-06-16T21:33:34.149Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@127.0.0.1'}}
[error_logger:error,2020-06-16T21:33:34.465Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_log:'-start_link_crash_consumer/0-fun-0-'/0
    pid: <0.327.0>
    registered_name: []
    exception exit: {{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                                 [{ns_crash_log,'babysitter_of_ns_1@cb.local'},
                                  consume,infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 214)
      in call from ns_log:crash_consumption_loop/0 (src/ns_log.erl, line 62)
      in call from misc:delaying_crash/2 (src/misc.erl, line 1605)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 0
    messages: []
    links: [<0.320.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 1598
    stack_size: 27
    reductions: 5003
  neighbours:

[error_logger:error,2020-06-16T21:33:34.465Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{nodedown,'babysitter_of_ns_1@cb.local'},
                  {gen_server,call,
                              [{ns_crash_log,'babysitter_of_ns_1@cb.local'},
                               consume,infinity]}}
     Offender:   [{pid,<0.327.0>},
                  {name,ns_crash_log_consumer},
                  {mfargs,{ns_log,start_link_crash_consumer,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2020-06-16T21:33:34.465Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.1921.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.478Z,ns_1@127.0.0.1:<0.1908.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:debug,2020-06-16T21:33:34.478Z,ns_1@127.0.0.1:<0.1908.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:info,2020-06-16T21:33:34.479Z,ns_1@127.0.0.1:<0.1908.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:info,2020-06-16T21:33:34.523Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:do_generate_local_cert:610]Saved local cert for node 'ns_1@127.0.0.1'
[ns_server:debug,2020-06-16T21:33:34.576Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2020-06-16T21:33:34.576Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'ns_1@cb.local'}}
[ns_server:debug,2020-06-16T21:33:34.576Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.3111058533.1128267777.210286>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2020-06-16T21:33:34.576Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267777.210286>,
                                  inet_tcp_dist,<0.1934.0>,
                                  #Ref<0.3111058533.1128267777.210289>}
[ns_server:debug,2020-06-16T21:33:34.580Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Accepted new connection from <0.1865.0>: {con,
                                                   #Ref<0.3111058533.1128267778.208342>,
                                                   inet_tcp_dist,undefined,
                                                   undefined}
[ns_server:debug,2020-06-16T21:33:34.583Z,ns_1@127.0.0.1:net_kernel<0.1859.0>:cb_dist:info_msg:754]cb_dist: Accepting connection from acceptor <0.1865.0> using module inet_tcp_dist
[ns_server:debug,2020-06-16T21:33:34.583Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.3111058533.1128267778.208342>,
                                  inet_tcp_dist,<0.1936.0>,
                                  #Ref<0.3111058533.1128267777.210296>}
[ns_server:debug,2020-06-16T21:33:34.586Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267777.210286>,
                               inet_tcp_dist,<0.1934.0>,
                               #Ref<0.3111058533.1128267777.210289>}
[error_logger:info,2020-06-16T21:33:34.586Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1934.0>,
                    {recv_challenge_failed,no_node,"ns_1@127.0.0.1"}}}
[ns_server:debug,2020-06-16T21:33:34.586Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.3111058533.1128267778.208342>,
                               inet_tcp_dist,<0.1936.0>,
                               #Ref<0.3111058533.1128267777.210296>}
[error_logger:info,2020-06-16T21:33:34.586Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@cb.local'}}
[error_logger:info,2020-06-16T21:33:34.587Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.1936.0>,{recv_challenge_reply_failed,{error,closed}}}}
[error_logger:info,2020-06-16T21:33:34.587Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'ns_1@127.0.0.1'}}
[ns_server:debug,2020-06-16T21:33:34.604Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2020-06-16T21:33:34.604Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2020-06-16T21:33:34.604Z,ns_1@127.0.0.1:cb_dist<0.1856.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2020-06-16T21:33:34.605Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:handle_info:547]Wrote new pem file
[ns_server:debug,2020-06-16T21:33:34.605Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:742]Going to notify following services: [ssl_service,capi_ssl_service,xdcr_proxy,
                                     memcached,event]
[ns_server:info,2020-06-16T21:33:34.605Z,ns_1@127.0.0.1:<0.1948.0>:ns_ssl_services_setup:notify_service:774]Successfully notified service memcached
[ns_server:info,2020-06-16T21:33:34.605Z,ns_1@127.0.0.1:<0.1949.0>:ns_ssl_services_setup:notify_service:774]Successfully notified service event
[ns_server:debug,2020-06-16T21:33:34.605Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of ssl_certs requested
[ns_server:debug,2020-06-16T21:33:34.606Z,ns_1@127.0.0.1:<0.223.0>:restartable:loop:71]Restarting child <0.593.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.1945.0>,#Ref<0.3111058533.1128267778.208384>}
[ns_server:debug,2020-06-16T21:33:34.607Z,ns_1@127.0.0.1:<0.223.0>:restartable:shutdown_child:120]Successfully terminated process <0.593.0>
[ns_server:debug,2020-06-16T21:33:34.607Z,ns_1@127.0.0.1:<0.1947.0>:ns_ports_manager:restart_port_by_name:43]Requesting restart of port xdcr_proxy
[ns_server:info,2020-06-16T21:33:34.610Z,ns_1@127.0.0.1:<0.1947.0>:ns_ssl_services_setup:notify_service:774]Successfully notified service xdcr_proxy
[ns_server:debug,2020-06-16T21:33:34.613Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [ssl_certs] succeeded
[ns_server:info,2020-06-16T21:33:34.632Z,ns_1@127.0.0.1:<0.1946.0>:ns_ssl_services_setup:notify_service:774]Successfully notified service capi_ssl_service
[ns_server:info,2020-06-16T21:33:34.633Z,ns_1@127.0.0.1:<0.1950.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-06-16T21:33:34.633Z,ns_1@127.0.0.1:<0.1950.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2020-06-16T21:33:34.634Z,ns_1@127.0.0.1:<0.1950.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-06-16T21:33:34.634Z,ns_1@127.0.0.1:<0.1950.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2020-06-16T21:33:34.636Z,ns_1@127.0.0.1:<0.1950.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2020-06-16T21:33:34.636Z,ns_1@127.0.0.1:<0.1950.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[error_logger:info,2020-06-16T21:33:34.635Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.1950.0>,menelaus_web}
             started: [{pid,<0.1951.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 25,35,168,237,222,198,87,48,13,6,9,42,134,
                                 72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
                                 6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,97,55,
                                 57,102,102,48,51,54,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,97,
                                 55,57,102,102,48,51,54,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,174,49,109,223,
                                 144,181,107,20,62,244,159,125,87,29,246,113,
                                 69,75,245,111,1,104,124,43,204,65,119,204,
                                 96,227,54,122,27,191,136,13,92,215,195,141,
                                 75,237,12,149,132,248,32,77,248,79,40,207,
                                 27,36,163,189,100,221,217,205,200,152,182,
                                 219,68,47,165,243,99,145,176,30,251,88,9,
                                 191,84,122,129,12,171,97,57,65,117,125,201,
                                 187,161,29,55,194,30,179,108,82,182,70,39,
                                 238,138,116,199,165,87,79,107,145,116,250,
                                 124,126,48,140,190,101,28,25,74,212,33,10,
                                 220,201,83,77,196,78,26,171,13,190,112,217,
                                 232,242,223,218,25,215,78,185,111,98,28,131,
                                 240,85,124,175,54,208,58,18,75,254,127,17,
                                 160,74,103,211,94,73,212,86,137,34,50,83,13,
                                 254,231,46,78,93,215,205,139,19,221,23,123,
                                 237,17,128,66,215,13,15,101,154,18,227,32,
                                 124,218,146,24,199,197,156,196,98,54,249,
                                 162,187,115,68,170,8,100,160,207,69,98,155,
                                 127,228,31,192,139,179,81,185,239,116,200,
                                 26,127,204,129,56,156,113,231,187,199,137,
                                 166,176,132,181,123,246,124,38,169,227,195,
                                 19,133,100,121,173,2,3,1,0,1,163,56,48,54,
                                 48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,
                                 19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,
                                 3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,
                                 255,48,13,6,9,42,134,72,134,247,13,1,1,11,5,
                                 0,3,130,1,1,0,89,124,248,161,22,73,145,149,
                                 85,172,148,139,7,93,32,7,175,91,14,172,231,
                                 118,40,78,27,178,89,75,131,208,230,143,18,
                                 102,90,239,184,191,23,110,16,188,53,1,140,
                                 250,222,182,110,227,147,146,54,213,57,173,
                                 240,98,40,234,121,98,38,48,248,150,245,251,
                                 234,226,199,93,237,126,208,23,34,64,242,190,
                                 249,78,248,187,146,149,159,106,2,225,94,161,
                                 102,109,22,144,33,95,3,166,3,130,10,120,163,
                                 22,1,220,93,251,89,47,21,2,34,23,199,184,53,
                                 76,186,10,45,185,16,104,172,97,84,9,190,82,
                                 230,168,86,130,242,216,104,76,95,149,170,
                                 215,57,168,115,153,222,43,173,32,118,189,
                                 189,71,64,37,87,38,97,20,91,141,219,144,142,
                                 176,73,66,145,96,212,63,39,6,222,25,163,243,
                                 199,142,209,62,110,156,172,26,182,105,118,
                                 182,221,216,241,46,255,41,42,166,137,80,220,
                                 176,6,199,46,39,84,21,82,186,29,131,184,174,
                                 30,89,198,145,196,172,166,178,197,125,249,
                                 206,150,58,124,85,249,51,63,254,149,179,199,
                                 207,211,231,103,179,81,14,189,155,8,39,97,
                                 84,27,150,157,173>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:33:34.637Z,ns_1@127.0.0.1:<0.1950.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2020-06-16T21:33:34.637Z,ns_1@127.0.0.1:<0.1950.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2020-06-16T21:33:34.638Z,ns_1@127.0.0.1:<0.223.0>:restartable:start_child:98]Started child process <0.1950.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2020-06-16T21:33:34.638Z,ns_1@127.0.0.1:<0.1945.0>:ns_ssl_services_setup:notify_service:774]Successfully notified service ssl_service
[ns_server:info,2020-06-16T21:33:34.639Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:758]Succesfully notified services [event,memcached,xdcr_proxy,capi_ssl_service,
                               ssl_service]
[error_logger:info,2020-06-16T21:33:34.638Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.1950.0>,menelaus_web}
             started: [{pid,<0.1969.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,22,
                                 25,35,168,237,222,198,87,48,13,6,9,42,134,
                                 72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
                                 6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,97,55,
                                 57,102,102,48,51,54,48,30,23,13,49,51,48,49,
                                 48,49,48,48,48,48,48,48,90,23,13,52,57,49,
                                 50,51,49,50,51,53,57,53,57,90,48,36,49,34,
                                 48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,
                                 97,115,101,32,83,101,114,118,101,114,32,97,
                                 55,57,102,102,48,51,54,48,130,1,34,48,13,6,
                                 9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,15,
                                 0,48,130,1,10,2,130,1,1,0,174,49,109,223,
                                 144,181,107,20,62,244,159,125,87,29,246,113,
                                 69,75,245,111,1,104,124,43,204,65,119,204,
                                 96,227,54,122,27,191,136,13,92,215,195,141,
                                 75,237,12,149,132,248,32,77,248,79,40,207,
                                 27,36,163,189,100,221,217,205,200,152,182,
                                 219,68,47,165,243,99,145,176,30,251,88,9,
                                 191,84,122,129,12,171,97,57,65,117,125,201,
                                 187,161,29,55,194,30,179,108,82,182,70,39,
                                 238,138,116,199,165,87,79,107,145,116,250,
                                 124,126,48,140,190,101,28,25,74,212,33,10,
                                 220,201,83,77,196,78,26,171,13,190,112,217,
                                 232,242,223,218,25,215,78,185,111,98,28,131,
                                 240,85,124,175,54,208,58,18,75,254,127,17,
                                 160,74,103,211,94,73,212,86,137,34,50,83,13,
                                 254,231,46,78,93,215,205,139,19,221,23,123,
                                 237,17,128,66,215,13,15,101,154,18,227,32,
                                 124,218,146,24,199,197,156,196,98,54,249,
                                 162,187,115,68,170,8,100,160,207,69,98,155,
                                 127,228,31,192,139,179,81,185,239,116,200,
                                 26,127,204,129,56,156,113,231,187,199,137,
                                 166,176,132,181,123,246,124,38,169,227,195,
                                 19,133,100,121,173,2,3,1,0,1,163,56,48,54,
                                 48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,
                                 19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,
                                 3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,1,1,
                                 255,48,13,6,9,42,134,72,134,247,13,1,1,11,5,
                                 0,3,130,1,1,0,89,124,248,161,22,73,145,149,
                                 85,172,148,139,7,93,32,7,175,91,14,172,231,
                                 118,40,78,27,178,89,75,131,208,230,143,18,
                                 102,90,239,184,191,23,110,16,188,53,1,140,
                                 250,222,182,110,227,147,146,54,213,57,173,
                                 240,98,40,234,121,98,38,48,248,150,245,251,
                                 234,226,199,93,237,126,208,23,34,64,242,190,
                                 249,78,248,187,146,149,159,106,2,225,94,161,
                                 102,109,22,144,33,95,3,166,3,130,10,120,163,
                                 22,1,220,93,251,89,47,21,2,34,23,199,184,53,
                                 76,186,10,45,185,16,104,172,97,84,9,190,82,
                                 230,168,86,130,242,216,104,76,95,149,170,
                                 215,57,168,115,153,222,43,173,32,118,189,
                                 189,71,64,37,87,38,97,20,91,141,219,144,142,
                                 176,73,66,145,96,212,63,39,6,222,25,163,243,
                                 199,142,209,62,110,156,172,26,182,105,118,
                                 182,221,216,241,46,255,41,42,166,137,80,220,
                                 176,6,199,46,39,84,21,82,186,29,131,184,174,
                                 30,89,198,145,196,172,166,178,197,125,249,
                                 206,150,58,124,85,249,51,63,254,149,179,199,
                                 207,211,231,103,179,81,14,189,155,8,39,97,
                                 84,27,150,157,173>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[cluster:debug,2020-06-16T21:33:34.644Z,ns_1@127.0.0.1:<0.1844.0>:ns_cluster:maybe_rename:635]Renamed node from 'ns_1@cb.local' to 'ns_1@127.0.0.1'.
[ns_server:debug,2020-06-16T21:33:34.644Z,ns_1@127.0.0.1:ns_ports_setup<0.465.0>:ns_ports_setup:children_loop_continue:118]Remote monitor <12940.109.0> was unpaused after node name change. Restart loop.
[ns_server:debug,2020-06-16T21:33:34.644Z,ns_1@127.0.0.1:memcached_config_mgr<0.490.0>:memcached_config_mgr:handle_info:163]Got DOWN with reason: unpaused from memcached port server: <12940.116.0>. Shutting down
[ns_server:debug,2020-06-16T21:33:34.645Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.280.0>:ns_server_nodes_sup:wait_link_to_couchdb_node_loop:196]Link to couchdb node was unpaused.
[ns_server:debug,2020-06-16T21:33:34.645Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.280.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[ns_server:debug,2020-06-16T21:33:34.645Z,ns_1@127.0.0.1:<0.528.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.490.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <12940.116.0>,
                                                                                 unpaused}}
[ns_server:debug,2020-06-16T21:33:34.645Z,ns_1@127.0.0.1:ns_node_disco_events<0.338.0>:ns_node_disco_rep_events:handle_event:42]Detected a new nodes (['ns_1@127.0.0.1']).  Moving config around.
[ns_server:info,2020-06-16T21:33:34.645Z,ns_1@127.0.0.1:ns_node_disco_events<0.338.0>:ns_node_disco_log:handle_event:46]ns_node_disco_log: nodes changed: ['ns_1@127.0.0.1']
[error_logger:error,2020-06-16T21:33:34.645Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {shutdown,
                     {memcached_port_server_down,<12940.116.0>,unpaused}}
     Offender:   [{pid,<0.490.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2020-06-16T21:33:34.646Z,ns_1@127.0.0.1:memcached_config_mgr<0.1988.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2020-06-16T21:33:34.646Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.1988.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.646Z,ns_1@127.0.0.1:<0.376.0>:restartable:loop:71]Restarting child <0.377.0>
  MFA: {ns_doctor_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.189.0>,#Ref<0.3111058533.1128267778.208441>}
[ns_server:debug,2020-06-16T21:33:34.647Z,ns_1@127.0.0.1:<0.381.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.380.0>} exited with reason shutdown
[ns_server:debug,2020-06-16T21:33:34.649Z,ns_1@127.0.0.1:<0.376.0>:restartable:shutdown_child:120]Successfully terminated process <0.377.0>
[ns_server:warn,2020-06-16T21:33:34.650Z,ns_1@127.0.0.1:<0.1990.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@cb.local',
                                                                            <<"b6e4175e17c89828057127316f6f28f4">>} (valid for 12480ms)
[error_logger:info,2020-06-16T21:33:34.650Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.1994.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.650Z,ns_1@127.0.0.1:ns_ports_setup<0.465.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2020-06-16T21:33:34.650Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.1995.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.650Z,ns_1@127.0.0.1:<0.376.0>:restartable:start_child:98]Started child process <0.1992.0>
  MFA: {ns_doctor_sup,start_link,[]}
[ns_server:debug,2020-06-16T21:33:34.651Z,ns_1@127.0.0.1:<0.549.0>:restartable:loop:71]Restarting child <0.550.0>
  MFA: {leader_services_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.189.0>,#Ref<0.3111058533.1128267778.208456>}
[ns_server:info,2020-06-16T21:33:34.651Z,ns_1@127.0.0.1:mb_master<0.558.0>:mb_master:terminate:327]Synchronously shutting down child mb_master_sup
[ns_server:info,2020-06-16T21:33:34.651Z,ns_1@127.0.0.1:leader_registry<0.555.0>:leader_registry_server:handle_down:253]Process <0.647.0> registered as 'license_reporting' terminated.
[ns_server:debug,2020-06-16T21:33:34.652Z,ns_1@127.0.0.1:<0.648.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.647.0>} exited with reason shutdown
[ns_server:info,2020-06-16T21:33:34.652Z,ns_1@127.0.0.1:leader_registry<0.555.0>:leader_registry_server:handle_down:253]Process <0.642.0> registered as 'collections' terminated.
[ns_server:info,2020-06-16T21:33:34.652Z,ns_1@127.0.0.1:leader_registry<0.555.0>:leader_registry_server:handle_down:253]Process <0.639.0> registered as 'auto_failover' terminated.
[ns_server:debug,2020-06-16T21:33:34.652Z,ns_1@127.0.0.1:<0.640.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {compat_mode_events,<0.639.0>} exited with reason shutdown
[ns_server:info,2020-06-16T21:33:34.652Z,ns_1@127.0.0.1:leader_registry<0.555.0>:leader_registry_server:handle_down:253]Process <0.637.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2020-06-16T21:33:34.652Z,ns_1@127.0.0.1:leader_registry<0.555.0>:leader_registry_server:handle_down:253]Process <0.636.0> registered as 'auto_rebalance' terminated.
[ns_server:debug,2020-06-16T21:33:34.652Z,ns_1@127.0.0.1:ns_ports_setup<0.465.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12940.109.0>
[ns_server:info,2020-06-16T21:33:34.652Z,ns_1@127.0.0.1:leader_registry<0.555.0>:leader_registry_server:handle_down:253]Process <0.635.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2020-06-16T21:33:34.653Z,ns_1@127.0.0.1:memcached_config_mgr<0.1988.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:info,2020-06-16T21:33:34.653Z,ns_1@127.0.0.1:leader_registry<0.555.0>:leader_registry_server:handle_down:253]Process <0.565.0> registered as 'ns_tick' terminated.
[ns_server:debug,2020-06-16T21:33:34.653Z,ns_1@127.0.0.1:leader_activities<0.552.0>:leader_activities:handle_internal_process_down:511]Process {quorum_nodes_manager,<0.1898.0>} terminated with reason shutdown
[ns_server:debug,2020-06-16T21:33:34.653Z,ns_1@127.0.0.1:leader_lease_agent<0.553.0>:leader_lease_agent:handle_abolish_lease:255]Received abolish lease request from {lease_holder,
                                     <<"b6e4175e17c89828057127316f6f28f4">>,
                                     'ns_1@127.0.0.1'} when lease is {lease,
                                                                      {lease_holder,
                                                                       <<"b6e4175e17c89828057127316f6f28f4">>,
                                                                       'ns_1@cb.local'},
                                                                      -576460686307831404,
                                                                      -576460671307831404,
                                                                      {timer,
                                                                       #Ref<0.3111058533.1128267778.207842>,
                                                                       {lease_expired,
                                                                        {lease_holder,
                                                                         <<"b6e4175e17c89828057127316f6f28f4">>,
                                                                         'ns_1@cb.local'}}},
                                                                      active}
[ns_server:debug,2020-06-16T21:33:34.653Z,ns_1@127.0.0.1:leader_activities<0.552.0>:leader_activities:handle_internal_process_down:511]Process {acquirer,<0.561.0>} terminated with reason shutdown
[ns_server:debug,2020-06-16T21:33:34.653Z,ns_1@127.0.0.1:leader_lease_agent<0.553.0>:leader_lease_agent:handle_abolish_lease:260]Expiring abolished lease
[ns_server:debug,2020-06-16T21:33:34.654Z,ns_1@127.0.0.1:leader_registry<0.555.0>:leader_registry_server:handle_new_leader:241]New leader is undefined. Invalidating name cache.
[ns_server:debug,2020-06-16T21:33:34.654Z,ns_1@127.0.0.1:<0.1903.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.1898.0>} exited with reason shutdown
[ns_server:debug,2020-06-16T21:33:34.655Z,ns_1@127.0.0.1:<0.559.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.558.0>} exited with reason shutdown
[ns_server:debug,2020-06-16T21:33:34.655Z,ns_1@127.0.0.1:<0.556.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {leader_events,<0.555.0>} exited with reason shutdown
[ns_server:debug,2020-06-16T21:33:34.654Z,ns_1@127.0.0.1:<0.571.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_node_disco_events,<0.561.0>} exited with reason shutdown
[ns_server:debug,2020-06-16T21:33:34.656Z,ns_1@127.0.0.1:memcached_config_mgr<0.1988.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12940.116.0>
[ns_server:debug,2020-06-16T21:33:34.658Z,ns_1@127.0.0.1:leader_activities<0.552.0>:leader_activities:handle_internal_process_down:511]Process {agent,<0.553.0>} terminated with reason shutdown
[ns_server:debug,2020-06-16T21:33:34.658Z,ns_1@127.0.0.1:<0.549.0>:restartable:shutdown_child:120]Successfully terminated process <0.550.0>
[error_logger:info,2020-06-16T21:33:34.659Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.2009.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:34.664Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.2010.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:34.665Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.2008.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:33:34.666Z,ns_1@127.0.0.1:leader_registry_sup<0.2011.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[error_logger:info,2020-06-16T21:33:34.666Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.2012.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.666Z,ns_1@127.0.0.1:leader_registry_sup<0.2011.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2020-06-16T21:33:34.667Z,ns_1@127.0.0.1:leader_registry_sup<0.2011.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2020-06-16T21:33:34.667Z,ns_1@127.0.0.1:mb_master<0.2015.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:info,2020-06-16T21:33:34.667Z,ns_1@127.0.0.1:ns_log<0.326.0>:ns_log:handle_cast:151]suppressing duplicate log mb_master:undefined([<<"I'm the only node, so I'm the master.">>]) because it's been seen 1 times in the past 56.803686 secs (last seen 56.803686 secs ago
[ns_server:debug,2020-06-16T21:33:34.668Z,ns_1@127.0.0.1:leader_registry<0.2012.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[error_logger:info,2020-06-16T21:33:34.669Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2018.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.669Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.2020.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[error_logger:info,2020-06-16T21:33:34.670Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2020.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:33:34.671Z,ns_1@127.0.0.1:mb_master_sup<0.2017.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.2025.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-06-16T21:33:34.671Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.2020.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[error_logger:info,2020-06-16T21:33:34.671Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2025.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:34.672Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.2029.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.672Z,ns_1@127.0.0.1:leader_lease_agent<0.2010.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"a3fff1850e6849ca319f548d13deea17">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:debug,2020-06-16T21:33:34.673Z,ns_1@127.0.0.1:memcached_config_mgr<0.1988.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[error_logger:info,2020-06-16T21:33:34.674Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.2030.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:33:34.675Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.2031.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.2033.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-06-16T21:33:34.675Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.2032.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:34.675Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.2033.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:33:34.675Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.2031.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.2034.0> on 'ns_1@127.0.0.1'

[ns_server:info,2020-06-16T21:33:34.676Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.2031.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.2035.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-06-16T21:33:34.676Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.2034.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:34.676Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.2035.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:34.676Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.2031.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:33:34.677Z,ns_1@127.0.0.1:<0.2037.0>:auto_failover:init:185]init auto_failover.
[user:info,2020-06-16T21:33:34.678Z,ns_1@127.0.0.1:<0.2037.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2020-06-16T21:33:34.678Z,ns_1@127.0.0.1:ns_log<0.326.0>:ns_log:handle_cast:151]suppressing duplicate log auto_failover:undefined([<<"Enabled auto-failover with timeout 120 and max count 1">>]) because it's been seen 1 times in the past 56.555441 secs (last seen 56.555441 secs ago
[ns_server:debug,2020-06-16T21:33:34.680Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{12,63759562414}}]}]
[ns_server:info,2020-06-16T21:33:34.680Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.2027.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.2037.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-06-16T21:33:34.681Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562357}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2020-06-16T21:33:34.681Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[error_logger:info,2020-06-16T21:33:34.681Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.2037.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:34.682Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2027.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:info,2020-06-16T21:33:34.681Z,ns_1@127.0.0.1:mb_master_sup<0.2017.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.2040.0> on 'ns_1@127.0.0.1'

[error_logger:info,2020-06-16T21:33:34.684Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2040.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.684Z,ns_1@127.0.0.1:<0.2044.0>:license_reporting:init:66]Starting license_reporting server
[ns_server:info,2020-06-16T21:33:34.686Z,ns_1@127.0.0.1:mb_master_sup<0.2017.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.2044.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2020-06-16T21:33:34.686Z,ns_1@127.0.0.1:memcached_config_mgr<0.1988.0>:memcached_config_mgr:init:89]found memcached port to be already active
[error_logger:info,2020-06-16T21:33:34.687Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.2044.0>},
                       {id,license_reporting},
                       {mfargs,{license_reporting,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:34.688Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.2015.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:33:34.688Z,ns_1@127.0.0.1:<0.549.0>:restartable:start_child:98]Started child process <0.2007.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2020-06-16T21:33:34.688Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.2011.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[cluster:info,2020-06-16T21:33:34.688Z,ns_1@127.0.0.1:ns_cluster<0.189.0>:ns_cluster:do_change_address:602]Renamed node. New name is 'ns_1@127.0.0.1'.
[ns_server:debug,2020-06-16T21:33:34.689Z,ns_1@127.0.0.1:ns_audit<0.489.0>:ns_audit:handle_call:125]Audit rename_node: [{hostname,<<"127.0.0.1">>},
                    {node,'ns_1@cb.local'},
                    {real_userid,{[{domain,wrong_token},
                                   {user,<<"<ud></ud>">>}]}},
                    {sessionid,<<"5878f065e05322588a105f8f8d2bd112">>},
                    {remote,{[{ip,<<"172.19.0.1">>},{port,48826}]}},
                    {timestamp,<<"2020-06-16T21:33:34.688Z">>}]
[ns_server:info,2020-06-16T21:33:34.721Z,ns_1@127.0.0.1:<0.2021.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"a3fff1850e6849ca319f548d13deea17">>)
[ns_server:debug,2020-06-16T21:33:34.723Z,ns_1@127.0.0.1:ns_audit<0.489.0>:ns_audit:handle_call:125]Audit modify_index_storage_mode: [{storageMode,<<"memory_optimized">>},
                                  {real_userid,
                                      {[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                                  {sessionid,
                                      <<"5878f065e05322588a105f8f8d2bd112">>},
                                  {remote,
                                      {[{ip,<<"172.19.0.1">>},{port,48826}]}},
                                  {timestamp,<<"2020-06-16T21:33:34.723Z">>}]
[ns_server:debug,2020-06-16T21:33:34.723Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562414}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-06-16T21:33:34.723Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{13,63759562414}}]}]
[ns_server:debug,2020-06-16T21:33:34.724Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-06-16T21:33:34.744Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562414}}]}|1024]
[ns_server:debug,2020-06-16T21:33:34.745Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562414}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-06-16T21:33:34.745Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memory_quota ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562414}}]}|1098]
[ns_server:debug,2020-06-16T21:33:34.745Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{14,63759562414}}]}]
[ns_server:debug,2020-06-16T21:33:34.746Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([cbas_memory_quota,memory_quota,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-06-16T21:33:34.747Z,ns_1@127.0.0.1:ns_audit<0.489.0>:ns_audit:handle_call:125]Audit cluster_settings: [{cluster_name,<<"app">>},
                         {quotas,{[{kv,1098},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {sessionid,<<"5878f065e05322588a105f8f8d2bd112">>},
                         {remote,{[{ip,<<"172.19.0.1">>},{port,48826}]}},
                         {timestamp,<<"2020-06-16T21:33:34.747Z">>}]
[ns_server:debug,2020-06-16T21:33:34.749Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{15,63759562414}}]}]
[ns_server:debug,2020-06-16T21:33:34.749Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cluster_name ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562414}}]},
 97,112,112]
[ns_server:debug,2020-06-16T21:33:34.750Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:34.782Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{16,63759562414}}]}]
[ns_server:debug,2020-06-16T21:33:34.781Z,ns_1@127.0.0.1:ns_audit<0.489.0>:ns_audit:handle_call:125]Audit setup_node_services: [{services,[cbas,eventing,fts,index,kv,n1ql]},
                            {node,'ns_1@127.0.0.1'},
                            {real_userid,
                                {[{domain,wrong_token},
                                  {user,<<"<ud></ud>">>}]}},
                            {sessionid,<<"5878f065e05322588a105f8f8d2bd112">>},
                            {remote,{[{ip,<<"172.19.0.1">>},{port,48826}]}},
                            {timestamp,<<"2020-06-16T21:33:34.781Z">>}]
[ns_server:debug,2020-06-16T21:33:34.782Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',services} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562414}}]},
 cbas,eventing,fts,index,kv,n1ql]
[ns_server:debug,2020-06-16T21:33:34.785Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {node,'ns_1@127.0.0.1',services}]..)
[ns_server:debug,2020-06-16T21:33:34.787Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":16,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-06-16T21:33:34.799Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{17,63759562414}}]}]
[ns_server:debug,2020-06-16T21:33:34.799Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
settings ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562414}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2020-06-16T21:33:34.800Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:34.840Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([rest,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:34.852Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{18,63759562414}}]}]
[ns_server:debug,2020-06-16T21:33:34.852Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2020-06-16T21:33:34.853Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":18,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-06-16T21:33:34.905Z,ns_1@127.0.0.1:menelaus_ui_auth<0.396.0>:token_server:handle_cast:211]Purge tokens []
[ns_server:debug,2020-06-16T21:33:34.906Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {[6,5],
                                                                {0,2078178643},
                                                                {0,2078178643},
                                                                false,[]} to {[6,
                                                                               5],
                                                                              {0,
                                                                               2078178643},
                                                                              {0,
                                                                               2078178643},
                                                                              true,
                                                                              []}
[ns_server:debug,2020-06-16T21:33:34.907Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{19,63759562414}}]}]
[ns_server:debug,2020-06-16T21:33:34.909Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest_creds ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562414}}]}|
 {"<ud>admin</ud>",
  {auth,
   [{<<"plain">>,"*****"},
    {<<"sha512">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,
        <<"z6sVeGgpgW3iWNKmXbkj4D3Hp4eK/ZhAUHVHlY2Jpl0+a/jppfA73VUB5WpC8DuZGnJkHddZuQPik8M0STL7Ng==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"Z6upcrGnKH8OkAxWYvVF/xTKz1GRRsAhZwv41dnHV3Q=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"7Cpx1PQPvtzybqaNvtG2em5L8VQ=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2020-06-16T21:33:34.911Z,ns_1@127.0.0.1:ns_audit<0.489.0>:ns_audit:handle_call:125]Audit password_change: [{identity,{[{domain,builtin},
                                    {user,<<"<ud>admin</ud>">>}]}},
                        {real_userid,{[{domain,wrong_token},
                                       {user,<<"<ud></ud>">>}]}},
                        {sessionid,<<"5878f065e05322588a105f8f8d2bd112">>},
                        {remote,{[{ip,<<"172.19.0.1">>},{port,48826}]}},
                        {timestamp,<<"2020-06-16T21:33:34.909Z">>}]
[ns_server:debug,2020-06-16T21:33:34.912Z,ns_1@127.0.0.1:memcached_permissions<0.333.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-06-16T21:33:34.912Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{20,63759562414}}]}]
[ns_server:debug,2020-06-16T21:33:34.912Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
uuid ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562414}}]}|
 <<"5a089e7da3564977f29225c27f56ce48">>]
[ns_server:debug,2020-06-16T21:33:34.919Z,ns_1@127.0.0.1:memcached_passwords<0.328.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-06-16T21:33:34.924Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.333.0>
[ns_server:debug,2020-06-16T21:33:34.932Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([rest_creds,uuid,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:34.932Z,ns_1@127.0.0.1:memcached_permissions<0.333.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:34.934Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.333.0>
[ns_server:debug,2020-06-16T21:33:34.950Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2020-06-16T21:33:34.951Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2020-06-16T21:33:34.962Z,ns_1@127.0.0.1:ns_audit<0.489.0>:ns_audit:handle_call:125]Audit login_success: [{roles,[<<"admin">>]},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                      {sessionid,<<"f9f3a340cf7429ec95a89d7cd2175807">>},
                      {remote,{[{ip,<<"172.19.0.1">>},{port,48822}]}},
                      {timestamp,<<"2020-06-16T21:33:34.958Z">>}]
[error_logger:info,2020-06-16T21:33:34.963Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.2107.0>},
                       {id,{service_agent,cbas}},
                       {mfargs,{service_agent,start_link,[cbas]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.974Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.686.0>} needs_update
[error_logger:info,2020-06-16T21:33:34.989Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.2115.0>},
                       {id,{service_agent,eventing}},
                       {mfargs,{service_agent,start_link,[eventing]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:34.991Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.686.0>} needs_update
[ns_server:debug,2020-06-16T21:33:35.007Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[error_logger:info,2020-06-16T21:33:35.031Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.2122.0>},
                       {id,{kv,dcp_traffic_monitor}},
                       {mfargs,{dcp_traffic_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:35.031Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.2119.0>},
                       {id,{service_agent,fts}},
                       {mfargs,{service_agent,start_link,[fts]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:35.033Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:35.039Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.686.0>} needs_update
[error_logger:info,2020-06-16T21:33:35.041Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2126.0>},
                       {id,{service_cbas,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,
                               [service_cbas]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:35.072Z,ns_1@127.0.0.1:ns_ports_setup<0.465.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,indexer,projector,goxdcr,query,fts,
                  cbas,eventing]
[error_logger:info,2020-06-16T21:33:35.077Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.2129.0>},
                       {id,{service_agent,index}},
                       {mfargs,{service_agent,start_link,[index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:35.083Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.686.0>} needs_update
[ns_server:debug,2020-06-16T21:33:35.084Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.2133.0>:service_stats_collector:check_status:329]Checking if service service_eventing is started...
[error_logger:info,2020-06-16T21:33:35.086Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2133.0>},
                       {id,{service_eventing,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,
                               [service_eventing]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:35.085Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{3,63759562415}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2020-06-16T21:33:35.087Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2020-06-16T21:33:35.087Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{21,63759562415}}]}]
[ns_server:debug,2020-06-16T21:33:35.090Z,ns_1@127.0.0.1:ns_audit<0.489.0>:ns_audit:handle_call:125]Audit modify_index_storage_mode: [{storageMode,<<"memory_optimized">>},
                                  {real_userid,
                                      {[{domain,builtin},
                                        {user,<<"<ud>admin</ud>">>}]}},
                                  {sessionid,
                                      <<"f9f3a340cf7429ec95a89d7cd2175807">>},
                                  {remote,
                                      {[{ip,<<"172.19.0.1">>},{port,48822}]}},
                                  {timestamp,<<"2020-06-16T21:33:35.085Z">>}]
[ns_server:debug,2020-06-16T21:33:35.090Z,ns_1@127.0.0.1:service_stats_collector-fts<0.2136.0>:service_stats_collector:check_status:329]Checking if service service_fts is started...
[error_logger:info,2020-06-16T21:33:35.097Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2136.0>},
                       {id,{service_fts,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,[service_fts]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:35.114Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.2139.0>},
                       {id,{kv,kv_stats_monitor}},
                       {mfargs,{kv_stats_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:35.119Z,ns_1@127.0.0.1:service_stats_collector-index<0.2146.0>:service_stats_collector:check_status:329]Checking if service service_index is started...
[error_logger:info,2020-06-16T21:33:35.128Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2146.0>},
                       {id,{service_index,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,
                               [service_index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:35.170Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2150.0>},
                       {id,{service_cbas,stats_archiver,"@cbas"}},
                       {mfargs,{stats_archiver,start_link,["@cbas"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:35.187Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2152.0>},
                       {id,{service_cbas,stats_reader,"@cbas"}},
                       {mfargs,{stats_reader,start_link,["@cbas"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:35.207Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[error_logger:info,2020-06-16T21:33:35.209Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.2154.0>},
                       {id,{kv,kv_monitor}},
                       {mfargs,{kv_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:35.212Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.328.0>
[ns_server:debug,2020-06-16T21:33:35.212Z,ns_1@127.0.0.1:memcached_passwords<0.328.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:35.212Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.328.0>
[ns_server:debug,2020-06-16T21:33:35.225Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2020-06-16T21:33:35.227Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2153.0>},
                       {id,{service_eventing,stats_archiver,"@eventing"}},
                       {mfargs,{stats_archiver,start_link,["@eventing"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:35.229Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2158.0>},
                       {id,{service_eventing,stats_reader,"@eventing"}},
                       {mfargs,{stats_reader,start_link,["@eventing"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:35.234Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2159.0>},
                       {id,{service_fts,stats_archiver,"@fts"}},
                       {mfargs,{stats_archiver,start_link,["@fts"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:35.235Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2161.0>},
                       {id,{service_fts,stats_reader,"@fts"}},
                       {mfargs,{stats_reader,start_link,["@fts"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:35.241Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2162.0>},
                       {id,{service_index,stats_archiver,"@index"}},
                       {mfargs,{stats_archiver,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:35.243Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2164.0>},
                       {id,{service_index,stats_reader,"@index"}},
                       {mfargs,{stats_reader,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:35.273Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[ns_server:debug,2020-06-16T21:33:35.574Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2020-06-16T21:33:35.575Z,ns_1@127.0.0.1:json_rpc_connection-index-cbauth<0.2175.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-cbauth", handling process <0.2175.0>
[ns_server:debug,2020-06-16T21:33:35.575Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"index-cbauth",<0.2175.0>} started
[ns_server:debug,2020-06-16T21:33:35.624Z,ns_1@127.0.0.1:<0.415.0>:menelaus_web:check_bucket_uuid:1076]Attempt to access non existent bucket "null"
[ns_server:debug,2020-06-16T21:33:35.715Z,ns_1@127.0.0.1:<0.2037.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@127.0.0.1',<<"a771316973056c9759aee152c7c79f8b">>} state new -> up
[ns_server:error,2020-06-16T21:33:35.741Z,ns_1@127.0.0.1:query_stats_collector<0.514.0>:rest_utils:get_json:62]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:debug,2020-06-16T21:33:35.766Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:35.933Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_storage:handle_call:115]Writing interactively saved doc {docv2,
                                    {ui_profile,{"<ud>admin</ud>",admin}},
                                    {[{<<"version">>,393221},
                                      {<<"scenarios">>,
                                       [{[{<<"name">>,<<"Cluster Overview">>},
                                          {<<"desc">>,
                                           <<"Stats showing the general health of your cluster.">>},
                                          {<<"groups">>,
                                           [<<"o4zrcla1f">>,<<"bby56yepr">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"wp6s9m9k5">>}]},
                                        {[{<<"name">>,<<"All Services">>},
                                          {<<"desc">>,
                                           <<"Most common stats, arranged per service. Customize and make your own dashboard with \"new dashboard... \" below.">>},
                                          {<<"groups">>,
                                           [<<"5uscfic8f">>,<<"uh88h42j4">>,
                                            <<"00zuohjxi">>,<<"14dln97e7">>,
                                            <<"60ygqe6b3">>,<<"23q4vnzhr">>,
                                            <<"v8thoqmkt">>,<<"eurhs73z0">>,
                                            <<"3r08aqho9">>,<<"i6w48y6kf">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"2pawwjfwu">>}]}]},
                                      {<<"groups">>,
                                       [{[{<<"name">>,<<"Cluster Overview">>},
                                          {<<"isOpen">>,true},
                                          {<<"charts">>,
                                           [<<"qmxbapmaf">>,<<"enk300951">>,
                                            <<"lnbomch46">>,<<"rui88haqy">>,
                                            <<"1rohy5lh0">>,<<"y19iuj8sg">>,
                                            <<"n9a3cjemi">>,<<"3k630c4ah">>,
                                            <<"ka7nluoad">>,<<"m0ngnpqbh">>,
                                            <<"mof9q58r8">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"o4zrcla1f">>}]},
                                        {[{<<"name">>,<<"Node Resources">>},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"abqd833x6">>,<<"53dxf0rvk">>,
                                            <<"6gi9b9jow">>,<<"3c9v8c44g">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bby56yepr">>}]},
                                        {[{<<"name">>,
                                           <<"Data (Docs/Views/XDCR)">>},
                                          {<<"isOpen">>,true},
                                          {<<"charts">>,
                                           [<<"v44g71jmp">>,<<"ycl3axu7k">>,
                                            <<"clzmap93y">>,<<"a3uzuk6rd">>,
                                            <<"zeu8g5x9w">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"5uscfic8f">>}]},
                                        {[{<<"name">>,<<"Query">>},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"0bg3p1cju">>,<<"lwzufnvxp">>,
                                            <<"eeqi1uhh8">>,<<"a2hmluk0h">>,
                                            <<"i3tfi7waw">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"uh88h42j4">>}]},
                                        {[{<<"name">>,<<"Index">>},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"z4h0ubjs3">>,<<"3h7voir98">>,
                                            <<"ceb9bzbh4">>,<<"gdu1th9nt">>,
                                            <<"f68rbcatb">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"00zuohjxi">>}]},
                                        {[{<<"name">>,<<"Search">>},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"2x3ujbzcn">>,<<"1q1iia9si">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"14dln97e7">>}]},
                                        {[{<<"name">>,<<"Analytics">>},
                                          {<<"enterprise">>,true},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"lvgyzcp2p">>,<<"32o29w1fh">>,
                                            <<"w4lewrxs3">>,<<"szigdqoir">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"60ygqe6b3">>}]},
                                        {[{<<"name">>,<<"Eventing">>},
                                          {<<"enterprise">>,true},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"xz5hxm71p">>,<<"22ysy4h6d">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"23q4vnzhr">>}]},
                                        {[{<<"name">>,<<"XDCR">>},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"uhmo07lyv">>,<<"jklr0gcz7">>,
                                            <<"ayfodpu7t">>,<<"3rgxlc8bd">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"v8thoqmkt">>}]},
                                        {[{<<"name">>,<<"vBucket Resources">>},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"k0xr9u958">>,<<"5u6pqb12x">>,
                                            <<"pt1s13t5e">>,<<"lz8erqhri">>,
                                            <<"0f7o2hnjp">>,<<"zk9gvhlc5">>,
                                            <<"2afdo8sj5">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"eurhs73z0">>}]},
                                        {[{<<"name">>,<<"DCP Queues">>},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"is21j7vyl">>,<<"2eom2qfto">>,
                                            <<"lest3zd2l">>,<<"k1lddokc2">>,
                                            <<"ulk9qxgg1">>,<<"0l6uft9cz">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3r08aqho9">>}]},
                                        {[{<<"name">>,<<"Disk Queues">>},
                                          {<<"isOpen">>,false},
                                          {<<"charts">>,
                                           [<<"180cy4luy">>,<<"11rc7l5e7">>,
                                            <<"kaoyllh0o">>,<<"t8jo080cu">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"i6w48y6kf">>}]}]},
                                      {<<"charts">>,
                                       [{[{<<"stats">>,
                                           {[{<<"@kv-.ops">>,true},
                                             {<<"@query.query_requests">>,
                                              true},
                                             {<<"@fts-.@items.total_queries">>,
                                              true},
                                             {<<"@kv-.ep_tmp_oom_errors">>,
                                              true},
                                             {<<"@kv-.ep_cache_miss_rate">>,
                                              true},
                                             {<<"@kv-.cmd_get">>,true},
                                             {<<"@kv-.cmd_set">>,true},
                                             {<<"@kv-.delete_hits">>,true},
                                             {<<"@kv-.@items.accesses">>,
                                              true}]}},
                                          {<<"size">>,<<"large">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"qmxbapmaf">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.mem_used">>,true},
                                             {<<"@kv-.ep_mem_low_wat">>,true},
                                             {<<"@kv-.ep_mem_high_wat">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"enk300951">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.curr_items">>,true},
                                             {<<"@kv-.vb_replica_curr_items">>,
                                              true},
                                             {<<"@kv-.vb_active_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.vb_replica_resident_items_ratio">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lnbomch46">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.disk_write_queue">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"rui88haqy">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_replica_items_remaining">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"1rohy5lh0">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_data_read_failed">>,
                                              true},
                                             {<<"@kv-.ep_data_write_failed">>,
                                              true},
                                             {<<"@query.query_errors">>,true},
                                             {<<"@query.total_queries_error">>,
                                              true},
                                             {<<"@eventing.eventing/failed_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"y19iuj8sg">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_requests_250ms">>,
                                              true},
                                             {<<"@query.query_requests_500ms">>,
                                              true},
                                             {<<"@query.query_requests_1000ms">>,
                                              true},
                                             {<<"@query.query_requests_5000ms">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"n9a3cjemi">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.replication_changes_left">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3k630c4ah">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.@items.num_docs_pending+queued">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ka7nluoad">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@fts-.@items.num_mutations_to_index">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"m0ngnpqbh">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@eventing.eventing/dcp_backlog">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"mof9q58r8">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.cpu_utilization_rate">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"abqd833x6">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.rest_requests">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"53dxf0rvk">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.mem_actual_free">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"6gi9b9jow">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.swap_used">>,true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3c9v8c44g">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.mem_used">>,true},
                                             {<<"@kv-.ep_mem_low_wat">>,true},
                                             {<<"@kv-.ep_mem_high_wat">>,true},
                                             {<<"@kv-.ep_kv_size">>,true},
                                             {<<"@kv-.ep_meta_data_memory">>,
                                              true},
                                             {<<"@kv-.vb_active_resident_items_ratio">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"v44g71jmp">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ops">>,true},
                                             {<<"@kv-.ep_cache_miss_rate">>,
                                              true},
                                             {<<"@kv-.cmd_get">>,true},
                                             {<<"@kv-.cmd_set">>,true},
                                             {<<"@kv-.delete_hits">>,true},
                                             {<<"@kv-.@items.accesses">>,true},
                                             {<<"@kv-.ep_num_ops_set_meta">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ycl3axu7k">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_items_remaining">>,
                                              true},
                                             {<<"@xdcr-.replication_changes_left">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"clzmap93y">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_bg_fetched">>,true},
                                             {<<"@kv-.ep_data_read_failed">>,
                                              true},
                                             {<<"@kv-.ep_data_write_failed">>,
                                              true},
                                             {<<"@kv-.ep_ops_create">>,true},
                                             {<<"@kv-.ep_ops_update">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"a3uzuk6rd">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_diskqueue_items">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"zeu8g5x9w">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_requests_1000ms">>,
                                              true},
                                             {<<"@query.query_requests_500ms">>,
                                              true},
                                             {<<"@query.query_requests_5000ms">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"0bg3p1cju">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_selects">>,true},
                                             {<<"@query.query_requests">>,
                                              true},
                                             {<<"@query.query_warnings">>,
                                              true},
                                             {<<"@query.query_invalid_requests">>,
                                              true},
                                             {<<"@query.query_errors">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lwzufnvxp">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_avg_req_time">>,
                                              true},
                                             {<<"@query.query_avg_svc_time">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"eeqi1uhh8">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_avg_result_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"a2hmluk0h">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_avg_response_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"i3tfi7waw">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index/num_rows_returned">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"z4h0ubjs3">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.@items.num_docs_pending+queued">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3h7voir98">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index/data_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ceb9bzbh4">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index/disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gdu1th9nt">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index.index_ram_percent">>,
                                              true},
                                             {<<"@index.index_remaining_ram">>,
                                              true},
                                             {<<"@index-.index/data_size">>,
                                              true},
                                             {<<"@index-.index/disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"f68rbcatb">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@fts-.fts/num_bytes_used_disk">>,
                                              true},
                                             {<<"@fts.fts_num_bytes_used_ram">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"2x3ujbzcn">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@fts-.@items.total_queries">>,
                                              true},
                                             {<<"@fts-.@items.total_queries_error">>,
                                              true},
                                             {<<"@fts-.@items.total_queries_slow">>,
                                              true},
                                             {<<"@fts-.@items.total_queries_timeout">>,
                                              true},
                                             {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"1q1iia9si">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas/incoming_records_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lvgyzcp2p">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_heap_used">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"32o29w1fh">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_disk_used">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"w4lewrxs3">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_system_load_average">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"szigdqoir">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@eventing.eventing/dcp_backlog">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"xz5hxm71p">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@eventing.eventing/failed_count">>,
                                              true},
                                             {<<"@eventing.eventing/timeout_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"22ysy4h6d">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.replication_changes_left">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"uhmo07lyv">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.changes_left">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"jklr0gcz7">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.wtavg_docs_latency">>,
                                              true},
                                             {<<"@xdcr-.@items.wtavg_meta_latency">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ayfodpu7t">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.docs_failed_cr_source">>,
                                              true},
                                             {<<"@xdcr-.@items.docs_filtered">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3rgxlc8bd">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_num">>,true},
                                             {<<"@kv-.vb_replica_num">>,true},
                                             {<<"@kv-.vb_pending_num">>,true},
                                             {<<"@kv-.ep_vb_total">>,true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"k0xr9u958">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.curr_items">>,true},
                                             {<<"@kv-.vb_replica_curr_items">>,
                                              true},
                                             {<<"@kv-.vb_pending_curr_items">>,
                                              true},
                                             {<<"@kv-.curr_items_tot">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"5u6pqb12x">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.vb_replica_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.vb_pending_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.ep_resident_items_rate">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"pt1s13t5e">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_ops_create">>,
                                              true},
                                             {<<"@kv-.vb_replica_ops_create">>,
                                              true},
                                             {<<"@kv-.vb_pending_ops_create">>,
                                              true},
                                             {<<"@kv-.ep_ops_create">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lz8erqhri">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_eject">>,true},
                                             {<<"@kv-.vb_replica_eject">>,
                                              true},
                                             {<<"@kv-.vb_pending_eject">>,
                                              true},
                                             {<<"@kv-.ep_num_value_ejects">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"0f7o2hnjp">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_itm_memory">>,
                                              true},
                                             {<<"@kv-.vb_replica_itm_memory">>,
                                              true},
                                             {<<"@kv-.vb_pending_itm_memory">>,
                                              true},
                                             {<<"@kv-.ep_kv_size">>,true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"zk9gvhlc5">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_meta_data_memory">>,
                                              true},
                                             {<<"@kv-.vb_replica_meta_data_memory">>,
                                              true},
                                             {<<"@kv-.vb_pending_meta_data_memory">>,
                                              true},
                                             {<<"@kv-.ep_meta_data_memory">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"2afdo8sj5">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_count">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"is21j7vyl">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_producer_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_producer_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_producer_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_producer_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_eventing_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_producer_count">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"2eom2qfto">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_items_remaining">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lest3zd2l">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_items_sent">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"k1lddokc2">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_total_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ulk9qxgg1">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_backoff">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"0l6uft9cz">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_diskqueue_fill">>,
                                              true},
                                             {<<"@kv-.ep_diskqueue_drain">>,
                                              true},
                                             {<<"@kv-.ep_diskqueue_items">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"180cy4luy">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_queue_fill">>,
                                              true},
                                             {<<"@kv-.vb_active_queue_drain">>,
                                              true},
                                             {<<"@kv-.vb_active_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"11rc7l5e7">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_replica_queue_fill">>,
                                              true},
                                             {<<"@kv-.vb_replica_queue_drain">>,
                                              true},
                                             {<<"@kv-.vb_replica_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"kaoyllh0o">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_pending_queue_fill">>,
                                              true},
                                             {<<"@kv-.vb_pending_queue_drain">>,
                                              true},
                                             {<<"@kv-.vb_pending_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"t8jo080cu">>}]}]}]},
                                    [{rev,{1,<<"aÏ-_">>}},
                                     {deleted,false},
                                     {last_modified,1592343215904}]}
[ns_server:debug,2020-06-16T21:33:35.962Z,ns_1@127.0.0.1:ns_audit<0.489.0>:ns_audit:handle_call:125]Audit set_user_profile: [{profile,{[{<<"version">>,393221},
                                    {<<"scenarios">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"desc">>,
                                         <<"Stats showing the general health of your cluster.">>},
                                        {<<"groups">>,
                                         [<<"o4zrcla1f">>,<<"bby56yepr">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"wp6s9m9k5">>}]},
                                      {[{<<"name">>,<<"All Services">>},
                                        {<<"desc">>,
                                         <<"Most common stats, arranged per service. Customize and make your own dashboard with \"new dashboard... \" below.">>},
                                        {<<"groups">>,
                                         [<<"5uscfic8f">>,<<"uh88h42j4">>,
                                          <<"00zuohjxi">>,<<"14dln97e7">>,
                                          <<"60ygqe6b3">>,<<"23q4vnzhr">>,
                                          <<"v8thoqmkt">>,<<"eurhs73z0">>,
                                          <<"3r08aqho9">>,<<"i6w48y6kf">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"2pawwjfwu">>}]}]},
                                    {<<"groups">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"isOpen">>,true},
                                        {<<"charts">>,
                                         [<<"qmxbapmaf">>,<<"enk300951">>,
                                          <<"lnbomch46">>,<<"rui88haqy">>,
                                          <<"1rohy5lh0">>,<<"y19iuj8sg">>,
                                          <<"n9a3cjemi">>,<<"3k630c4ah">>,
                                          <<"ka7nluoad">>,<<"m0ngnpqbh">>,
                                          <<"mof9q58r8">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"o4zrcla1f">>}]},
                                      {[{<<"name">>,<<"Node Resources">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"abqd833x6">>,<<"53dxf0rvk">>,
                                          <<"6gi9b9jow">>,<<"3c9v8c44g">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bby56yepr">>}]},
                                      {[{<<"name">>,
                                         <<"Data (Docs/Views/XDCR)">>},
                                        {<<"isOpen">>,true},
                                        {<<"charts">>,
                                         [<<"v44g71jmp">>,<<"ycl3axu7k">>,
                                          <<"clzmap93y">>,<<"a3uzuk6rd">>,
                                          <<"zeu8g5x9w">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"5uscfic8f">>}]},
                                      {[{<<"name">>,<<"Query">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"0bg3p1cju">>,<<"lwzufnvxp">>,
                                          <<"eeqi1uhh8">>,<<"a2hmluk0h">>,
                                          <<"i3tfi7waw">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"uh88h42j4">>}]},
                                      {[{<<"name">>,<<"Index">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"z4h0ubjs3">>,<<"3h7voir98">>,
                                          <<"ceb9bzbh4">>,<<"gdu1th9nt">>,
                                          <<"f68rbcatb">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"00zuohjxi">>}]},
                                      {[{<<"name">>,<<"Search">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"2x3ujbzcn">>,<<"1q1iia9si">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"14dln97e7">>}]},
                                      {[{<<"name">>,<<"Analytics">>},
                                        {<<"enterprise">>,true},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"lvgyzcp2p">>,<<"32o29w1fh">>,
                                          <<"w4lewrxs3">>,<<"szigdqoir">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"60ygqe6b3">>}]},
                                      {[{<<"name">>,<<"Eventing">>},
                                        {<<"enterprise">>,true},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"xz5hxm71p">>,<<"22ysy4h6d">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"23q4vnzhr">>}]},
                                      {[{<<"name">>,<<"XDCR">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"uhmo07lyv">>,<<"jklr0gcz7">>,
                                          <<"ayfodpu7t">>,<<"3rgxlc8bd">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"v8thoqmkt">>}]},
                                      {[{<<"name">>,<<"vBucket Resources">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"k0xr9u958">>,<<"5u6pqb12x">>,
                                          <<"pt1s13t5e">>,<<"lz8erqhri">>,
                                          <<"0f7o2hnjp">>,<<"zk9gvhlc5">>,
                                          <<"2afdo8sj5">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"eurhs73z0">>}]},
                                      {[{<<"name">>,<<"DCP Queues">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"is21j7vyl">>,<<"2eom2qfto">>,
                                          <<"lest3zd2l">>,<<"k1lddokc2">>,
                                          <<"ulk9qxgg1">>,<<"0l6uft9cz">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3r08aqho9">>}]},
                                      {[{<<"name">>,<<"Disk Queues">>},
                                        {<<"isOpen">>,false},
                                        {<<"charts">>,
                                         [<<"180cy4luy">>,<<"11rc7l5e7">>,
                                          <<"kaoyllh0o">>,<<"t8jo080cu">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"i6w48y6kf">>}]}]},
                                    {<<"charts">>,
                                     [{[{<<"stats">>,
                                         {[{<<"@kv-.ops">>,true},
                                           {<<"@query.query_requests">>,true},
                                           {<<"@fts-.@items.total_queries">>,
                                            true},
                                           {<<"@kv-.ep_tmp_oom_errors">>,true},
                                           {<<"@kv-.ep_cache_miss_rate">>,
                                            true},
                                           {<<"@kv-.cmd_get">>,true},
                                           {<<"@kv-.cmd_set">>,true},
                                           {<<"@kv-.delete_hits">>,true},
                                           {<<"@kv-.@items.accesses">>,
                                            true}]}},
                                        {<<"size">>,<<"large">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"qmxbapmaf">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.mem_used">>,true},
                                           {<<"@kv-.ep_mem_low_wat">>,true},
                                           {<<"@kv-.ep_mem_high_wat">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"enk300951">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.curr_items">>,true},
                                           {<<"@kv-.vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.vb_replica_resident_items_ratio">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lnbomch46">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.disk_write_queue">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"rui88haqy">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_replica_items_remaining">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"1rohy5lh0">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.ep_data_write_failed">>,
                                            true},
                                           {<<"@query.query_errors">>,true},
                                           {<<"@query.total_queries_error">>,
                                            true},
                                           {<<"@eventing.eventing/failed_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"y19iuj8sg">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_requests_250ms">>,
                                            true},
                                           {<<"@query.query_requests_500ms">>,
                                            true},
                                           {<<"@query.query_requests_1000ms">>,
                                            true},
                                           {<<"@query.query_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"n9a3cjemi">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.replication_changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3k630c4ah">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.@items.num_docs_pending+queued">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ka7nluoad">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.@items.num_mutations_to_index">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"m0ngnpqbh">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@eventing.eventing/dcp_backlog">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"mof9q58r8">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.cpu_utilization_rate">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"abqd833x6">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.rest_requests">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"53dxf0rvk">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.mem_actual_free">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"6gi9b9jow">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.swap_used">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3c9v8c44g">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.mem_used">>,true},
                                           {<<"@kv-.ep_mem_low_wat">>,true},
                                           {<<"@kv-.ep_mem_high_wat">>,true},
                                           {<<"@kv-.ep_kv_size">>,true},
                                           {<<"@kv-.ep_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.vb_active_resident_items_ratio">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"v44g71jmp">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ops">>,true},
                                           {<<"@kv-.ep_cache_miss_rate">>,
                                            true},
                                           {<<"@kv-.cmd_get">>,true},
                                           {<<"@kv-.cmd_set">>,true},
                                           {<<"@kv-.delete_hits">>,true},
                                           {<<"@kv-.@items.accesses">>,true},
                                           {<<"@kv-.ep_num_ops_set_meta">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ycl3axu7k">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_items_remaining">>,
                                            true},
                                           {<<"@xdcr-.replication_changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"clzmap93y">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_bg_fetched">>,true},
                                           {<<"@kv-.ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.ep_data_write_failed">>,
                                            true},
                                           {<<"@kv-.ep_ops_create">>,true},
                                           {<<"@kv-.ep_ops_update">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"a3uzuk6rd">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"zeu8g5x9w">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_requests_1000ms">>,
                                            true},
                                           {<<"@query.query_requests_500ms">>,
                                            true},
                                           {<<"@query.query_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0bg3p1cju">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_selects">>,true},
                                           {<<"@query.query_requests">>,true},
                                           {<<"@query.query_warnings">>,true},
                                           {<<"@query.query_invalid_requests">>,
                                            true},
                                           {<<"@query.query_errors">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lwzufnvxp">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_avg_req_time">>,
                                            true},
                                           {<<"@query.query_avg_svc_time">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"eeqi1uhh8">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_avg_result_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"a2hmluk0h">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_avg_response_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"i3tfi7waw">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index/num_rows_returned">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"z4h0ubjs3">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.@items.num_docs_pending+queued">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3h7voir98">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index/data_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ceb9bzbh4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index/disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gdu1th9nt">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index.index_ram_percent">>,
                                            true},
                                           {<<"@index.index_remaining_ram">>,
                                            true},
                                           {<<"@index-.index/data_size">>,
                                            true},
                                           {<<"@index-.index/disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"f68rbcatb">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.fts/num_bytes_used_disk">>,
                                            true},
                                           {<<"@fts.fts_num_bytes_used_ram">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"2x3ujbzcn">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.@items.total_queries">>,
                                            true},
                                           {<<"@fts-.@items.total_queries_error">>,
                                            true},
                                           {<<"@fts-.@items.total_queries_slow">>,
                                            true},
                                           {<<"@fts-.@items.total_queries_timeout">>,
                                            true},
                                           {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"1q1iia9si">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas/incoming_records_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lvgyzcp2p">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_heap_used">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"32o29w1fh">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_disk_used">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"w4lewrxs3">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_system_load_average">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"szigdqoir">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@eventing.eventing/dcp_backlog">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"xz5hxm71p">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@eventing.eventing/failed_count">>,
                                            true},
                                           {<<"@eventing.eventing/timeout_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"22ysy4h6d">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.replication_changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"uhmo07lyv">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"jklr0gcz7">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.wtavg_docs_latency">>,
                                            true},
                                           {<<"@xdcr-.@items.wtavg_meta_latency">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ayfodpu7t">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.docs_failed_cr_source">>,
                                            true},
                                           {<<"@xdcr-.@items.docs_filtered">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3rgxlc8bd">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_num">>,true},
                                           {<<"@kv-.vb_replica_num">>,true},
                                           {<<"@kv-.vb_pending_num">>,true},
                                           {<<"@kv-.ep_vb_total">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"k0xr9u958">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.curr_items">>,true},
                                           {<<"@kv-.vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.vb_pending_curr_items">>,
                                            true},
                                           {<<"@kv-.curr_items_tot">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"5u6pqb12x">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.vb_replica_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.vb_pending_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.ep_resident_items_rate">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"pt1s13t5e">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_ops_create">>,
                                            true},
                                           {<<"@kv-.vb_replica_ops_create">>,
                                            true},
                                           {<<"@kv-.vb_pending_ops_create">>,
                                            true},
                                           {<<"@kv-.ep_ops_create">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lz8erqhri">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_eject">>,true},
                                           {<<"@kv-.vb_replica_eject">>,true},
                                           {<<"@kv-.vb_pending_eject">>,true},
                                           {<<"@kv-.ep_num_value_ejects">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0f7o2hnjp">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_itm_memory">>,
                                            true},
                                           {<<"@kv-.vb_replica_itm_memory">>,
                                            true},
                                           {<<"@kv-.vb_pending_itm_memory">>,
                                            true},
                                           {<<"@kv-.ep_kv_size">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"zk9gvhlc5">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.vb_replica_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.vb_pending_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.ep_meta_data_memory">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"2afdo8sj5">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_count">>,true},
                                           {<<"@kv-.ep_dcp_replica_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_count">>,true},
                                           {<<"@kv-.ep_dcp_eventing_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_count">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"is21j7vyl">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_eventing_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_producer_count">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"2eom2qfto">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_items_remaining">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lest3zd2l">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_items_sent">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"k1lddokc2">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_total_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ulk9qxgg1">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_backoff">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0l6uft9cz">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_diskqueue_fill">>,true},
                                           {<<"@kv-.ep_diskqueue_drain">>,
                                            true},
                                           {<<"@kv-.ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"180cy4luy">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_queue_fill">>,
                                            true},
                                           {<<"@kv-.vb_active_queue_drain">>,
                                            true},
                                           {<<"@kv-.vb_active_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"11rc7l5e7">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_replica_queue_fill">>,
                                            true},
                                           {<<"@kv-.vb_replica_queue_drain">>,
                                            true},
                                           {<<"@kv-.vb_replica_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"kaoyllh0o">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_pending_queue_fill">>,
                                            true},
                                           {<<"@kv-.vb_pending_queue_drain">>,
                                            true},
                                           {<<"@kv-.vb_pending_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"t8jo080cu">>}]}]}]}},
                         {identity,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                         {real_userid,{[{domain,builtin},
                                        {user,<<"<ud>admin</ud>">>}]}},
                         {sessionid,<<"f9f3a340cf7429ec95a89d7cd2175807">>},
                         {remote,{[{ip,<<"172.19.0.1">>},{port,48822}]}},
                         {timestamp,<<"2020-06-16T21:33:35.959Z">>}]
[ns_server:debug,2020-06-16T21:33:36.012Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@index-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:36.089Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:36.094Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.2133.0>:service_stats_collector:check_status:329]Checking if service service_eventing is started...
[ns_server:debug,2020-06-16T21:33:36.119Z,ns_1@127.0.0.1:service_stats_collector-fts<0.2136.0>:service_stats_collector:check_status:329]Checking if service service_fts is started...
[ns_server:debug,2020-06-16T21:33:36.159Z,ns_1@127.0.0.1:service_stats_collector-index<0.2146.0>:service_stats_collector:check_status:329]Checking if service service_index is started...
[ns_server:debug,2020-06-16T21:33:36.252Z,ns_1@127.0.0.1:json_rpc_connection-projector-cbauth<0.2201.0>:json_rpc_connection:init:73]Observed revrpc connection: label "projector-cbauth", handling process <0.2201.0>
[ns_server:debug,2020-06-16T21:33:36.252Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"projector-cbauth",<0.2201.0>} started
[ns_server:debug,2020-06-16T21:33:36.256Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:36.430Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-cbauth<0.2206.0>:json_rpc_connection:init:73]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.2206.0>
[ns_server:debug,2020-06-16T21:33:36.430Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"cbq-engine-cbauth",<0.2206.0>} started
[ns_server:debug,2020-06-16T21:33:36.481Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@cbq-engine-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:36.509Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/dictionary_cache/counter">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562416}}]}|
 <<":0">>]
[ns_server:debug,2020-06-16T21:33:36.510Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{22,63759562416}}]}]
[ns_server:debug,2020-06-16T21:33:36.515Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/query/dictionary_cache/counter">>}]..)
[ns_server:debug,2020-06-16T21:33:36.695Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/functions_cache/counter">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562416}}]}|
 <<"0">>]
[ns_server:debug,2020-06-16T21:33:36.695Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{23,63759562416}}]}]
[ns_server:debug,2020-06-16T21:33:36.702Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/query/functions_cache/counter">>}]..)
[ns_server:debug,2020-06-16T21:33:36.857Z,ns_1@127.0.0.1:json_rpc_connection-fts-cbauth<0.2249.0>:json_rpc_connection:init:73]Observed revrpc connection: label "fts-cbauth", handling process <0.2249.0>
[ns_server:debug,2020-06-16T21:33:36.858Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"fts-cbauth",<0.2249.0>} started
[ns_server:debug,2020-06-16T21:33:37.013Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@fts-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:37.074Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:37.078Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.2133.0>:service_stats_collector:check_status:329]Checking if service service_eventing is started...
[ns_server:debug,2020-06-16T21:33:37.100Z,ns_1@127.0.0.1:service_stats_collector-fts<0.2136.0>:service_stats_collector:check_status:329]Checking if service service_fts is started...
[ns_server:debug,2020-06-16T21:33:37.153Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{24,63759562417}}]}]
[ns_server:debug,2020-06-16T21:33:37.158Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-known/a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562417}}]}|
 <<"{\"uuid\":\"28688671243dc339\",\"nodeDefs\":{\"a771316973056c9759aee152c7c79f8b\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"a771316973056c9759aee152c7c79f8b\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"bindGRPC\\\":\\\"127.0.0.1:9130\\\",\\\"bindGRPCSSL\\\":\\\"127.0.0.1:19130\\\",\\\"bindHTTPS\\\":\\\":18094\\\",\\\"features\\\":\\\"leanPlan,index"...>>]
[ns_server:debug,2020-06-16T21:33:37.158Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-known/a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:37.178Z,ns_1@127.0.0.1:service_stats_collector-index<0.2146.0>:service_stats_collector:check_status:329]Checking if service service_index is started...
[ns_server:debug,2020-06-16T21:33:37.198Z,ns_1@127.0.0.1:service_stats_collector-index<0.2146.0>:service_stats_collector:check_status:333]Service service_index is started
[ns_server:debug,2020-06-16T21:33:37.241Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{25,63759562417}}]}]
[ns_server:debug,2020-06-16T21:33:37.241Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-wanted/a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562417}}]}|
 <<"{\"uuid\":\"594ffb62f6f80929\",\"nodeDefs\":{\"a771316973056c9759aee152c7c79f8b\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"a771316973056c9759aee152c7c79f8b\",\"implVersion\":\"5.5.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"bindGRPC\\\":\\\"127.0.0.1:9130\\\",\\\"bindGRPCSSL\\\":\\\"127.0.0.1:19130\\\",\\\"bindHTTPS\\\":\\\":18094\\\",\\\"features\\\":\\\"leanPlan,index"...>>]
[ns_server:debug,2020-06-16T21:33:37.252Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-wanted/a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:37.556Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{26,63759562417}}]}]
[ns_server:debug,2020-06-16T21:33:37.561Z,ns_1@127.0.0.1:json_rpc_connection-cbas-cbauth<0.2310.0>:json_rpc_connection:init:73]Observed revrpc connection: label "cbas-cbauth", handling process <0.2310.0>
[ns_server:debug,2020-06-16T21:33:37.558Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/fts/cbgt/cfg/version">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562417}}]}|
 <<"5.5.0">>]
[ns_server:debug,2020-06-16T21:33:37.578Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/fts/cbgt/cfg/version">>}]..)
[ns_server:debug,2020-06-16T21:33:37.583Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"cbas-cbauth",<0.2310.0>} started
[ns_server:debug,2020-06-16T21:33:37.654Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@cbas-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:37.677Z,ns_1@127.0.0.1:json_rpc_connection-fts-service_api<0.2327.0>:json_rpc_connection:init:73]Observed revrpc connection: label "fts-service_api", handling process <0.2327.0>
[ns_server:debug,2020-06-16T21:33:37.680Z,ns_1@127.0.0.1:service_agent-fts<0.2119.0>:service_agent:do_handle_connection:376]Observed new json rpc connection for fts: <0.2327.0>
[ns_server:debug,2020-06-16T21:33:37.690Z,ns_1@127.0.0.1:<0.2124.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.2120.0>} exited with reason normal
[ns_server:debug,2020-06-16T21:33:37.810Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2020-06-16T21:33:37.811Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:33:37.818Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2020-06-16T21:33:37.819Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:error,2020-06-16T21:33:37.891Z,ns_1@127.0.0.1:service_status_keeper_worker<0.530.0>:rest_utils:get_json:62]Request to (eventing) api/v1/functions failed: {error,
                                                {econnrefused,
                                                 [{lhttpc_client,
                                                   send_request,1,
                                                   [{file,
                                                     "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                    {line,220}]},
                                                  {lhttpc_client,execute,9,
                                                   [{file,
                                                     "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                    {line,169}]},
                                                  {lhttpc_client,request,9,
                                                   [{file,
                                                     "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                    {line,92}]}]}}
[ns_server:error,2020-06-16T21:33:37.894Z,ns_1@127.0.0.1:service_status_keeper-eventing<0.538.0>:service_status_keeper:handle_cast:119]Service service_eventing returned incorrect status
[ns_server:debug,2020-06-16T21:33:38.003Z,ns_1@127.0.0.1:json_rpc_connection-eventing-cbauth<0.2356.0>:json_rpc_connection:init:73]Observed revrpc connection: label "eventing-cbauth", handling process <0.2356.0>
[ns_server:debug,2020-06-16T21:33:38.004Z,ns_1@127.0.0.1:menelaus_cbauth<0.459.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"eventing-cbauth",<0.2356.0>} started
[ns_server:debug,2020-06-16T21:33:38.020Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@eventing-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:38.064Z,ns_1@127.0.0.1:json_rpc_connection-eventing-service_api<0.2360.0>:json_rpc_connection:init:73]Observed revrpc connection: label "eventing-service_api", handling process <0.2360.0>
[ns_server:debug,2020-06-16T21:33:38.065Z,ns_1@127.0.0.1:service_agent-eventing<0.2115.0>:service_agent:do_handle_connection:376]Observed new json rpc connection for eventing: <0.2360.0>
[ns_server:debug,2020-06-16T21:33:38.065Z,ns_1@127.0.0.1:<0.2118.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.2116.0>} exited with reason normal
[ns_server:debug,2020-06-16T21:33:38.087Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{27,63759562418}}]}]
[ns_server:debug,2020-06-16T21:33:38.087Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/eventing/settings/config">>}]..)
[ns_server:debug,2020-06-16T21:33:38.087Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562418}}]}|
 <<"{\n \"enable_debugger\": false,\n \"ram_quota\": 256\n}">>]
[ns_server:debug,2020-06-16T21:33:38.099Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.2133.0>:service_stats_collector:check_status:329]Checking if service service_eventing is started...
[ns_server:debug,2020-06-16T21:33:38.099Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:38.132Z,ns_1@127.0.0.1:service_stats_collector-fts<0.2136.0>:service_stats_collector:check_status:329]Checking if service service_fts is started...
[ns_server:debug,2020-06-16T21:33:38.151Z,ns_1@127.0.0.1:service_stats_collector-fts<0.2136.0>:service_stats_collector:check_status:333]Service service_fts is started
[ns_server:debug,2020-06-16T21:33:38.362Z,ns_1@127.0.0.1:json_rpc_connection-cbas-service_api<0.2391.0>:json_rpc_connection:init:73]Observed revrpc connection: label "cbas-service_api", handling process <0.2391.0>
[ns_server:debug,2020-06-16T21:33:38.363Z,ns_1@127.0.0.1:<0.2110.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.2108.0>} exited with reason normal
[ns_server:debug,2020-06-16T21:33:38.368Z,ns_1@127.0.0.1:service_agent-cbas<0.2107.0>:service_agent:do_handle_connection:376]Observed new json rpc connection for cbas: <0.2391.0>
[ns_server:debug,2020-06-16T21:33:39.104Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:39.104Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.2133.0>:service_stats_collector:check_status:329]Checking if service service_eventing is started...
[ns_server:debug,2020-06-16T21:33:39.105Z,ns_1@127.0.0.1:service_stats_collector-eventing<0.2133.0>:service_stats_collector:check_status:333]Service service_eventing is started
[ns_server:debug,2020-06-16T21:33:39.656Z,ns_1@127.0.0.1:cleanup_process<0.2433.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `cbas'
[ns_server:debug,2020-06-16T21:33:39.662Z,ns_1@127.0.0.1:service_rebalancer-cbas<0.2438.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service cbas to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:39.663Z,ns_1@127.0.0.1:service_rebalancer-cbas<0.2438.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for cbas
[rebalance:info,2020-06-16T21:33:39.663Z,ns_1@127.0.0.1:service_rebalancer-cbas-worker<0.2448.0>:service_rebalancer:rebalance_worker:153]Rebalancing service cbas with id <<"bb4865b95ea66ce628f695b470cb4f77">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2020-06-16T21:33:39.675Z,ns_1@127.0.0.1:service_rebalancer-cbas-worker<0.2448.0>:service_rebalancer:rebalance_worker:159]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"a771316973056c9759aee152c7c79f8b">>},
                    {priority,0},
                    {opaque,{[{<<"cbas-version">>,<<"6.5.1-6299">>},
                              {<<"cc-http-port">>,<<"9111">>},
                              {<<"host">>,<<"127.0.0.1">>},
                              {<<"num-iodevices">>,<<"1">>},
                              {<<"svc-http-port">>,<<"8095">>}]}}]}]
[ns_server:debug,2020-06-16T21:33:39.679Z,ns_1@127.0.0.1:service_rebalancer-cbas-worker<0.2448.0>:service_rebalancer:rebalance_worker:168]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2020-06-16T21:33:39.715Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{28,63759562419}}]}]
[ns_server:debug,2020-06-16T21:33:39.715Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/cbas/nextPartitionId">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562419}}]}|
 <<"1">>]
[ns_server:debug,2020-06-16T21:33:39.716Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/cbas/nextPartitionId">>}]..)
[ns_server:debug,2020-06-16T21:33:39.717Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/cbas/nextControllerId">>}]..)
[ns_server:debug,2020-06-16T21:33:39.719Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{29,63759562419}}]}]
[ns_server:debug,2020-06-16T21:33:39.719Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/cbas/nextControllerId">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562419}}]}|
 <<"1">>]
[ns_server:debug,2020-06-16T21:33:39.730Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{30,63759562419}}]}]
[ns_server:debug,2020-06-16T21:33:39.732Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/cbas/topology">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562419}}]}|
 <<"{\"nodes\":[{\"nodeId\":\"a771316973056c9759aee152c7c79f8b\",\"priority\":0,\"opaque\":{\"cbas-version\":\"6.5.1-6299\",\"cc-http-port\":\"9111\",\"controller-id\":\"0\",\"host\":\"127.0.0.1\",\"num-iodevices\":\"1\",\"starting-partition-id\":\"0\",\"svc-http-port\":\"8095\"}}],\"id\":\"bb4865b95ea66ce628f695b470cb4f77\",\"type\":\"topology-change-rebalance\",\"ccNodeId\":\"a771316973056c9759aee152c7c79f8b\",\"metadataNodeId\":\"a7713169"...>>]
[ns_server:debug,2020-06-16T21:33:39.733Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/cbas/topology">>}]..)
[ns_server:debug,2020-06-16T21:33:39.740Z,ns_1@127.0.0.1:service_rebalancer-cbas<0.2438.0>:service_rebalancer:run_rebalance_worker:122]Worker terminated normally
[ns_server:debug,2020-06-16T21:33:39.747Z,ns_1@127.0.0.1:service_agent-cbas<0.2107.0>:service_agent:cleanup_service:553]Cleaning up stale tasks:
[[{<<"rev">>,<<"NA==">>},
  {<<"id">>,<<"prepare/bb4865b95ea66ce628f695b470cb4f77">>},
  {<<"type">>,<<"task-prepared">>},
  {<<"status">>,<<"task-running">>},
  {<<"isCancelable">>,true},
  {<<"progress">>,0},
  {<<"extra">>,
   {[{<<"rebalanceId">>,<<"bb4865b95ea66ce628f695b470cb4f77">>}]}}]]
[ns_server:debug,2020-06-16T21:33:39.754Z,ns_1@127.0.0.1:cleanup_process<0.2433.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `cbas` finished successfully
[ns_server:debug,2020-06-16T21:33:39.755Z,ns_1@127.0.0.1:cleanup_process<0.2433.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `eventing'
[ns_server:debug,2020-06-16T21:33:39.755Z,ns_1@127.0.0.1:service_rebalancer-eventing<0.2482.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service eventing to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:39.755Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{31,63759562419}}]}]
[ns_server:debug,2020-06-16T21:33:39.755Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{service_map,cbas} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562419}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:39.757Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {service_map,cbas}]..)
[ns_server:debug,2020-06-16T21:33:39.757Z,ns_1@127.0.0.1:service_rebalancer-eventing<0.2482.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for eventing
[ns_server:debug,2020-06-16T21:33:39.757Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":31,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"cbas\":8095,\"cbasSSL\":18095,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[rebalance:info,2020-06-16T21:33:39.760Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.2498.0>:service_rebalancer:rebalance_worker:153]Rebalancing service eventing with id <<"2be3f7f94ba8bb1cd39fba0269555776">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2020-06-16T21:33:39.763Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.2498.0>:service_rebalancer:rebalance_worker:159]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"a771316973056c9759aee152c7c79f8b">>},
                    {priority,0},
                    {opaque,null}]}]
[ns_server:debug,2020-06-16T21:33:39.777Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.2498.0>:service_rebalancer:rebalance_worker:168]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2020-06-16T21:33:39.841Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{32,63759562419}}]}]
[ns_server:debug,2020-06-16T21:33:39.841Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/config/keepNodes">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562419}}]}|
 <<"[\"a771316973056c9759aee152c7c79f8b\"]">>]
[ns_server:debug,2020-06-16T21:33:39.845Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/eventing/config/keepNodes">>}]..)
[ns_server:debug,2020-06-16T21:33:39.850Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{33,63759562419}}]}]
[ns_server:debug,2020-06-16T21:33:39.852Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:39.861Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{34,63759562419}}]}]
[ns_server:debug,2020-06-16T21:33:39.861Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/rebalanceToken/2be3f7f94ba8bb1cd39fba0269555776">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562419}}]}|
 <<"2be3f7f94ba8bb1cd39fba0269555776">>]
[ns_server:debug,2020-06-16T21:33:39.870Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/eventing/rebalanceToken/2be3f7f94ba8bb1cd39fba0269555776">>}]..)
[ns_server:debug,2020-06-16T21:33:39.879Z,ns_1@127.0.0.1:service_rebalancer-eventing<0.2482.0>:service_rebalancer:run_rebalance_worker:122]Worker terminated normally
[ns_server:debug,2020-06-16T21:33:39.888Z,ns_1@127.0.0.1:cleanup_process<0.2433.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `eventing` finished successfully
[ns_server:debug,2020-06-16T21:33:39.889Z,ns_1@127.0.0.1:cleanup_process<0.2433.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `fts'
[ns_server:debug,2020-06-16T21:33:39.891Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{35,63759562419}}]}]
[ns_server:debug,2020-06-16T21:33:39.891Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{service_map,eventing} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562419}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:39.892Z,ns_1@127.0.0.1:service_rebalancer-fts<0.2532.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service fts to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:39.893Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {service_map,eventing}]..)
[ns_server:debug,2020-06-16T21:33:39.894Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":35,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"cbas\":8095,\"cbasSSL\":18095,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-06-16T21:33:39.894Z,ns_1@127.0.0.1:service_rebalancer-fts<0.2532.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for fts
[rebalance:info,2020-06-16T21:33:39.895Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.2548.0>:service_rebalancer:rebalance_worker:153]Rebalancing service fts with id <<"6380eda373359402d74599e96142c139">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2020-06-16T21:33:39.907Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.2548.0>:service_rebalancer:rebalance_worker:159]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"a771316973056c9759aee152c7c79f8b">>},
                    {priority,0},
                    {opaque,null}]}]
[ns_server:debug,2020-06-16T21:33:40.005Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.2548.0>:service_rebalancer:rebalance_worker:168]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2020-06-16T21:33:40.088Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.2548.0>:service_janitor:do_orchestrate_initial_rebalance:101]Initial rebalance progress for `fts': [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2020-06-16T21:33:40.113Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:40.355Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.355Z,ns_1@127.0.0.1:<0.2390.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.357Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.359Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.359Z,ns_1@127.0.0.1:<0.2393.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.359Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.359Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.362Z,ns_1@127.0.0.1:<0.2194.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.376Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.376Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.379Z,ns_1@127.0.0.1:<0.416.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.395Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.401Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2435.0>
[ns_server:debug,2020-06-16T21:33:40.404Z,ns_1@127.0.0.1:<0.2435.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.405Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2435.0>
[ns_server:debug,2020-06-16T21:33:40.406Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.407Z,ns_1@127.0.0.1:<0.2390.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.415Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.425Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.425Z,ns_1@127.0.0.1:<0.2194.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.426Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.426Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.429Z,ns_1@127.0.0.1:<0.416.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.434Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.435Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.437Z,ns_1@127.0.0.1:<0.2393.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.441Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.441Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.441Z,ns_1@127.0.0.1:<0.2390.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.451Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.451Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2435.0>
[ns_server:debug,2020-06-16T21:33:40.460Z,ns_1@127.0.0.1:<0.2435.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.463Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2435.0>
[ns_server:debug,2020-06-16T21:33:40.489Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.491Z,ns_1@127.0.0.1:<0.416.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.492Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.492Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.497Z,ns_1@127.0.0.1:<0.2393.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.500Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.500Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.501Z,ns_1@127.0.0.1:<0.2390.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.503Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.503Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.508Z,ns_1@127.0.0.1:<0.2194.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.515Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.515Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2435.0>
[ns_server:debug,2020-06-16T21:33:40.522Z,ns_1@127.0.0.1:<0.2435.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.524Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2435.0>
[ns_server:debug,2020-06-16T21:33:40.536Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.536Z,ns_1@127.0.0.1:<0.416.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.537Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.537Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.538Z,ns_1@127.0.0.1:<0.2393.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.547Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.548Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.549Z,ns_1@127.0.0.1:<0.2194.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.550Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.571Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.572Z,ns_1@127.0.0.1:<0.2390.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.572Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.573Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.573Z,ns_1@127.0.0.1:<0.2194.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.574Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.594Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.594Z,ns_1@127.0.0.1:<0.2393.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.594Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.596Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2435.0>
[ns_server:debug,2020-06-16T21:33:40.605Z,ns_1@127.0.0.1:<0.2435.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.606Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2435.0>
[ns_server:debug,2020-06-16T21:33:40.606Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.608Z,ns_1@127.0.0.1:<0.416.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.614Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.416.0>
[ns_server:debug,2020-06-16T21:33:40.614Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.615Z,ns_1@127.0.0.1:<0.2194.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.616Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2194.0>
[ns_server:debug,2020-06-16T21:33:40.623Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.623Z,ns_1@127.0.0.1:<0.2393.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.627Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2393.0>
[ns_server:debug,2020-06-16T21:33:40.634Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.636Z,ns_1@127.0.0.1:<0.2390.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:40.637Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.2390.0>
[ns_server:debug,2020-06-16T21:33:40.819Z,ns_1@127.0.0.1:service_rebalancer-fts<0.2532.0>:service_rebalancer:run_rebalance_worker:122]Worker terminated normally
[ns_server:debug,2020-06-16T21:33:40.824Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{36,63759562420}}]}]
[ns_server:debug,2020-06-16T21:33:40.825Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{service_map,fts} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562420}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:40.825Z,ns_1@127.0.0.1:cleanup_process<0.2433.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `fts` finished successfully
[ns_server:debug,2020-06-16T21:33:40.825Z,ns_1@127.0.0.1:cleanup_process<0.2433.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `index'
[ns_server:debug,2020-06-16T21:33:40.826Z,ns_1@127.0.0.1:service_rebalancer-index<0.2602.0>:service_agent:wait_for_agents:74]Waiting for the service agents for service index to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:40.829Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {service_map,fts}]..)
[ns_server:debug,2020-06-16T21:33:40.833Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":36,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"cbas\":8095,\"cbasSSL\":18095,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsSSL\":18094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-06-16T21:33:40.837Z,ns_1@127.0.0.1:service_rebalancer-index<0.2602.0>:service_agent:wait_for_agents_loop:92]All service agents are ready for index
[rebalance:info,2020-06-16T21:33:40.837Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.2618.0>:service_rebalancer:rebalance_worker:153]Rebalancing service index with id <<"66b1c2f5df6f8321f1085d6c99d54569">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2020-06-16T21:33:41.215Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:41.752Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{37,63759562421}}]}]
[ns_server:debug,2020-06-16T21:33:41.752Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/cbas/config/node/a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:41.753Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/cbas/config/node/a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562421}}]}|
 <<"{\"configVersion\":1,\"config\":{\"additionalCcNodes\":[],\"address\":\"0.0.0.0\",\"analyticsCbHome\":\"/opt/couchbase\",\"analyticsCcHttpPort\":\"9111\",\"analyticsHttpAdminListenPort\":\"9110\",\"analyticsHttpListenAddress\":\"0.0.0.0\",\"analyticsHttpListenPort\":\"8095\",\"analyticsHttpsListenAddress\":\"0.0.0.0\",\"analyticsHttpsListenPort\":\"18095\",\"analyticsNodeName\":\"127.0.0.1:8091\",\"analyticsSslEnabled\":true,\"cb"...>>]
[ns_server:debug,2020-06-16T21:33:42.217Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:42.347Z,ns_1@127.0.0.1:json_rpc_connection-index-service_api<0.2657.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-service_api", handling process <0.2657.0>
[ns_server:debug,2020-06-16T21:33:42.352Z,ns_1@127.0.0.1:service_agent-index<0.2129.0>:service_agent:do_handle_connection:376]Observed new json rpc connection for index: <0.2657.0>
[ns_server:debug,2020-06-16T21:33:42.352Z,ns_1@127.0.0.1:<0.2132.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {json_rpc_events,<0.2130.0>} exited with reason normal
[ns_server:debug,2020-06-16T21:33:42.471Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.2618.0>:service_rebalancer:rebalance_worker:159]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"a771316973056c9759aee152c7c79f8b">>},
                    {priority,4},
                    {opaque,null}]}]
[ns_server:debug,2020-06-16T21:33:42.574Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.2618.0>:service_rebalancer:rebalance_worker:168]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2020-06-16T21:33:42.668Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{38,63759562422}}]}]
[ns_server:debug,2020-06-16T21:33:42.681Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562422}}]}|
 <<"{\"MasterId\":\"a771316973056c9759aee152c7c79f8b\",\"RebalId\":\"66b1c2f5df6f8321f1085d6c99d54569\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"127.0.0.1\"}">>]
[ns_server:debug,2020-06-16T21:33:42.684Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2020-06-16T21:33:42.757Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{39,63759562422}}]}]
[ns_server:debug,2020-06-16T21:33:42.757Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562422}}]}|
 '_deleted']
[ns_server:debug,2020-06-16T21:33:42.759Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2020-06-16T21:33:42.782Z,ns_1@127.0.0.1:service_rebalancer-index<0.2602.0>:service_rebalancer:run_rebalance_worker:122]Worker terminated normally
[ns_server:debug,2020-06-16T21:33:42.822Z,ns_1@127.0.0.1:cleanup_process<0.2433.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `index` finished successfully
[ns_server:debug,2020-06-16T21:33:42.824Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{40,63759562422}}]}]
[ns_server:debug,2020-06-16T21:33:42.824Z,ns_1@127.0.0.1:cleanup_process<0.2433.0>:service_janitor:maybe_init_simple_service:71]Created initial service map for service `n1ql'
[ns_server:debug,2020-06-16T21:33:42.824Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{service_map,index} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562422}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:42.826Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{41,63759562422}}]}]
[ns_server:debug,2020-06-16T21:33:42.826Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{service_map,n1ql} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562422}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:42.830Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":41,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"cbas\":8095,\"cbasSSL\":18095,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsSSL\":18094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"indexAdmin\":9100,\"indexScan\":9101,\"indexHttp\":9102,\"indexStreamInit\":9103,\"indexStreamCatchup\":9104,\"indexStreamMaint\":9105,\"indexHttps\":19102,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999,\"n1ql\":8093,\"n1qlSSL\":18093},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-06-16T21:33:42.836Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {service_map,index},
                               {service_map,n1ql}]..)
[ns_server:debug,2020-06-16T21:33:42.838Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.493.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":41,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"cbas\":8095,\"cbasSSL\":18095,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsSSL\":18094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"indexAdmin\":9100,\"indexScan\":9101,\"indexHttp\":9102,\"indexStreamInit\":9103,\"indexStreamCatchup\":9104,\"indexStreamMaint\":9105,\"indexHttps\":19102,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999,\"n1ql\":8093,\"n1qlSSL\":18093},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2020-06-16T21:33:43.219Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:44.220Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:45.222Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:46.224Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:47.226Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:47.469Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2020-06-16T21:33:47.638Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {[6,5],
                                                                {0,2078178643},
                                                                {0,2078178643},
                                                                true,[]} to {[6,
                                                                              5],
                                                                             {0,
                                                                              2078178643},
                                                                             {0,
                                                                              2078178643},
                                                                             true,
                                                                             [{"platzi",
                                                                               <<"afd2b81586cc3bbf24b79179d05c51d0">>}]}
[ns_server:debug,2020-06-16T21:33:47.639Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:47.644Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{42,63759562427}}]}]
[ns_server:debug,2020-06-16T21:33:47.647Z,ns_1@127.0.0.1:ns_audit<0.489.0>:ns_audit:handle_call:125]Audit create_bucket: [{props,{[{compression_mode,passive},
                               {max_ttl,0},
                               {storage_mode,couchstore},
                               {conflict_resolution_type,seqno},
                               {eviction_policy,value_only},
                               {num_threads,3},
                               {flush_enabled,false},
                               {purge_interval,undefined},
                               {ram_quota,1151336448},
                               {replica_index,false},
                               {num_replicas,0}]}},
                      {type,membase},
                      {bucket_name,<<"platzi">>},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                      {sessionid,<<"f9f3a340cf7429ec95a89d7cd2175807">>},
                      {remote,{[{ip,<<"172.19.0.1">>},{port,49094}]}},
                      {timestamp,<<"2020-06-16T21:33:47.642Z">>}]
[ns_server:debug,2020-06-16T21:33:47.653Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"a771316973056c9759aee152c7c79f8b">>,{3,63759562427}}],
 {configs,[[{map,[]},
            {fastForwardMap,[]},
            {uuid,<<"afd2b81586cc3bbf24b79179d05c51d0">>},
            {auth_type,sasl},
            {num_replicas,0},
            {replica_index,false},
            {ram_quota,1151336448},
            {autocompaction,false},
            {purge_interval,undefined},
            {flush_enabled,false},
            {num_threads,3},
            {eviction_policy,value_only},
            {conflict_resolution_type,seqno},
            {storage_mode,couchstore},
            {max_ttl,0},
            {compression_mode,passive},
            {type,membase},
            {num_vbuckets,1024},
            {replication_topology,star},
            {repl_type,dcp},
            {servers,[]},
            {sasl_password,"*****"}]]}]
[menelaus:info,2020-06-16T21:33:47.655Z,ns_1@127.0.0.1:<0.2435.0>:menelaus_web_buckets:do_bucket_create:645]Created bucket "platzi" of type: couchbase
[{num_replicas,0},
 {replica_index,false},
 {ram_quota,1151336448},
 {autocompaction,false},
 {purge_interval,undefined},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive}]
[ns_server:debug,2020-06-16T21:33:47.659Z,ns_1@127.0.0.1:memcached_permissions<0.333.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2020-06-16T21:33:47.679Z,ns_1@127.0.0.1:memcached_passwords<0.328.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2020-06-16T21:33:47.724Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@eventing-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:47.724Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@cbq-engine-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:47.724Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@index-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:47.765Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2020-06-16T21:33:47.765Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2020-06-16T21:33:47.777Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.333.0>
[ns_server:debug,2020-06-16T21:33:47.786Z,ns_1@127.0.0.1:memcached_permissions<0.333.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2020-06-16T21:33:47.794Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.333.0>
[ns_server:debug,2020-06-16T21:33:47.919Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2020-06-16T21:33:47.942Z,ns_1@127.0.0.1:<0.2929.0>:ns_janitor:update_servers:86]janitor decided to update servers list for bucket "platzi" to ['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:47.958Z,ns_1@127.0.0.1:<0.2929.0>:ns_janitor:config_sync:259]Going to push config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:47.979Z,ns_1@127.0.0.1:ns_bucket_worker<0.499.0>:ns_bucket_worker:start_one_bucket:127]Starting new bucket: "platzi"
[ns_server:debug,2020-06-16T21:33:47.992Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{43,63759562427}}]}]
[ns_server:debug,2020-06-16T21:33:47.996Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:48.013Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"a771316973056c9759aee152c7c79f8b">>,{4,63759562427}}],
 {configs,[{"platzi",
            [{map,[]},
             {fastForwardMap,[]},
             {uuid,<<"afd2b81586cc3bbf24b79179d05c51d0">>},
             {auth_type,sasl},
             {num_replicas,0},
             {replica_index,false},
             {ram_quota,1151336448},
             {autocompaction,false},
             {purge_interval,undefined},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,passive},
             {type,membase},
             {num_vbuckets,1024},
             {replication_topology,star},
             {repl_type,dcp},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"}]}]}]
[ns_server:debug,2020-06-16T21:33:48.089Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2020-06-16T21:33:48.128Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2020-06-16T21:33:48.129Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@fts-cbauth",admin}
[error_logger:info,2020-06-16T21:33:48.135Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2907.0>},
                       {id,{service_cbas,stats_archiver,"platzi"}},
                       {mfargs,{stats_archiver,start_link,["@cbas-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:48.145Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:handle_call:128]Fully synchronized config in 16865 us
[error_logger:info,2020-06-16T21:33:48.146Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2946.0>},
                       {id,{service_cbas,stats_reader,"platzi"}},
                       {mfargs,{stats_reader,start_link,["@cbas-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:48.147Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.328.0>
[ns_server:debug,2020-06-16T21:33:48.151Z,ns_1@127.0.0.1:memcached_passwords<0.328.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:info,2020-06-16T21:33:48.153Z,ns_1@127.0.0.1:<0.2950.0>:ns_janitor:cleanup_with_membase_bucket_check_map:98]janitor decided to generate initial vbucket map
[ns_server:debug,2020-06-16T21:33:48.153Z,ns_1@127.0.0.1:single_bucket_kv_sup-platzi<0.2947.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:85]Syncing config to couchdb node
[ns_server:debug,2020-06-16T21:33:48.155Z,ns_1@127.0.0.1:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.328.0>
[ns_server:debug,2020-06-16T21:33:48.221Z,ns_1@127.0.0.1:single_bucket_kv_sup-platzi<0.2947.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:91]Synced config to couchdb node successfully
[ns_server:debug,2020-06-16T21:33:48.222Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2020-06-16T21:33:48.229Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[stats:error,2020-06-16T21:33:48.237Z,ns_1@127.0.0.1:<0.416.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[stats:error,2020-06-16T21:33:48.249Z,ns_1@127.0.0.1:<0.2371.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-06-16T21:33:48.293Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2949.0>},
                       {id,{service_eventing,stats_archiver,"platzi"}},
                       {mfargs,
                           {stats_archiver,start_link,["@eventing-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:48.295Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2962.0>},
                       {id,{service_eventing,stats_reader,"platzi"}},
                       {mfargs,{stats_reader,start_link,["@eventing-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:48.314Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:48.351Z,ns_1@127.0.0.1:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[stats:error,2020-06-16T21:33:48.380Z,ns_1@127.0.0.1:<0.2376.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[stats:error,2020-06-16T21:33:48.423Z,ns_1@127.0.0.1:<0.2389.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-06-16T21:33:48.428Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2963.0>},
                       {id,{service_fts,stats_archiver,"platzi"}},
                       {mfargs,{stats_archiver,start_link,["@fts-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:48.433Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2968.0>},
                       {id,{service_fts,stats_reader,"platzi"}},
                       {mfargs,{stats_reader,start_link,["@fts-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[stats:error,2020-06-16T21:33:48.472Z,ns_1@127.0.0.1:<0.2371.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-06-16T21:33:48.538Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2969.0>},
                       {id,{service_index,stats_archiver,"platzi"}},
                       {mfargs,{stats_archiver,start_link,["@index-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:48.540Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.2972.0>},
                       {id,{service_index,stats_reader,"platzi"}},
                       {mfargs,{stats_reader,start_link,["@index-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:48.541Z,ns_1@127.0.0.1:<0.2950.0>:mb_map:generate_map_old:362]Natural map score: {1024,0}
[stats:error,2020-06-16T21:33:48.548Z,ns_1@127.0.0.1:<0.2376.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[stats:error,2020-06-16T21:33:48.551Z,ns_1@127.0.0.1:<0.2389.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:48.565Z,ns_1@127.0.0.1:capi_doc_replicator-platzi<0.2975.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[error_logger:info,2020-06-16T21:33:48.566Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2967.0>,docs_kv_sup}
             started: [{pid,<0.2975.0>},
                       {id,doc_replicator},
                       {mfargs,
                           {capi_ddoc_manager,start_replicator,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:48.566Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-platzi<0.2976.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[error_logger:info,2020-06-16T21:33:48.568Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2967.0>,docs_kv_sup}
             started: [{pid,<0.2976.0>},
                       {id,doc_replication_srv},
                       {mfargs,{doc_replication_srv,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:48.571Z,ns_1@127.0.0.1:<0.2950.0>:mb_map:generate_map_old:369]Rnd maps scores: {1024,0}, {1024,0}
[ns_server:debug,2020-06-16T21:33:48.571Z,ns_1@127.0.0.1:<0.2950.0>:mb_map:generate_map_old:376]Considering 1 maps:
[{1024,0}]
[ns_server:debug,2020-06-16T21:33:48.571Z,ns_1@127.0.0.1:<0.2950.0>:mb_map:generate_map_old:381]Best map score: {1024,0} (true,true,true)
[ns_server:debug,2020-06-16T21:33:48.584Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{44,63759562428}}]}]
[ns_server:debug,2020-06-16T21:33:48.591Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
vbucket_map_history ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562428}}]},
 {[['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   [...]|...],
  [{replication_topology,star},{tags,undefined},{max_slaves,10}]}]
[ns_server:debug,2020-06-16T21:33:48.592Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([buckets,vbucket_map_history,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:48.605Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{45,63759562428}}]}]
[error_logger:info,2020-06-16T21:33:48.605Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-platzi'}
             started: [{pid,<12941.465.0>},
                       {id,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:48.612Z,ns_1@127.0.0.1:<0.2950.0>:ns_janitor:config_sync:259]Going to push config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:48.615Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>}]..)
[ns_server:debug,2020-06-16T21:33:48.630Z,ns_1@127.0.0.1:capi_doc_replicator-platzi<0.2975.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <12941.467.0>
[ns_server:debug,2020-06-16T21:33:48.630Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-platzi<0.2976.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <12941.467.0>
[error_logger:info,2020-06-16T21:33:48.640Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-platzi'}
             started: [{pid,<12941.467.0>},
                       {id,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["platzi",<0.2975.0>,<0.2976.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:48.640Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2967.0>,docs_kv_sup}
             started: [{pid,<12941.464.0>},
                       {id,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@cb.local',"platzi"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:33:48.673Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"a771316973056c9759aee152c7c79f8b">>,{5,63759562428}}],
 {configs,[{"platzi",
            [{map,[{0,[],['ns_1@127.0.0.1']},
                   {1,[],['ns_1@127.0.0.1']},
                   {2,[],['ns_1@127.0.0.1']},
                   {3,[],['ns_1@127.0.0.1']},
                   {4,[],['ns_1@127.0.0.1']},
                   {5,[],['ns_1@127.0.0.1']},
                   {6,[],['ns_1@127.0.0.1']},
                   {7,[],['ns_1@127.0.0.1']},
                   {8,[],['ns_1@127.0.0.1']},
                   {9,[],['ns_1@127.0.0.1']},
                   {10,[],['ns_1@127.0.0.1']},
                   {11,[],['ns_1@127.0.0.1']},
                   {12,[],['ns_1@127.0.0.1']},
                   {13,[],['ns_1@127.0.0.1']},
                   {14,[],['ns_1@127.0.0.1']},
                   {15,[],['ns_1@127.0.0.1']},
                   {16,[],['ns_1@127.0.0.1']},
                   {17,[],['ns_1@127.0.0.1']},
                   {18,[],['ns_1@127.0.0.1']},
                   {19,[],['ns_1@127.0.0.1']},
                   {20,[],['ns_1@127.0.0.1']},
                   {21,[],['ns_1@127.0.0.1']},
                   {22,[],['ns_1@127.0.0.1']},
                   {23,[],['ns_1@127.0.0.1']},
                   {24,[],['ns_1@127.0.0.1']},
                   {25,[],['ns_1@127.0.0.1']},
                   {26,[],['ns_1@127.0.0.1']},
                   {27,[],['ns_1@127.0.0.1']},
                   {28,[],['ns_1@127.0.0.1']},
                   {29,[],['ns_1@127.0.0.1']},
                   {30,[],['ns_1@127.0.0.1']},
                   {31,[],['ns_1@127.0.0.1']},
                   {32,[],['ns_1@127.0.0.1']},
                   {33,[],['ns_1@127.0.0.1']},
                   {34,[],['ns_1@127.0.0.1']},
                   {35,[],['ns_1@127.0.0.1']},
                   {36,[],['ns_1@127.0.0.1']},
                   {37,[],['ns_1@127.0.0.1']},
                   {38,[],['ns_1@127.0.0.1']},
                   {39,[],['ns_1@127.0.0.1']},
                   {40,[],['ns_1@127.0.0.1']},
                   {41,[],['ns_1@127.0.0.1']},
                   {42,[],['ns_1@127.0.0.1']},
                   {43,[],['ns_1@127.0.0.1']},
                   {44,[],['ns_1@127.0.0.1']},
                   {45,[],['ns_1@127.0.0.1']},
                   {46,[],['ns_1@127.0.0.1']},
                   {47,[],['ns_1@127.0.0.1']},
                   {48,[],['ns_1@127.0.0.1']},
                   {49,[],['ns_1@127.0.0.1']},
                   {50,[],['ns_1@127.0.0.1']},
                   {51,[],['ns_1@127.0.0.1']},
                   {52,[],['ns_1@127.0.0.1']},
                   {53,[],['ns_1@127.0.0.1']},
                   {54,[],['ns_1@127.0.0.1']},
                   {55,[],['ns_1@127.0.0.1']},
                   {56,[],['ns_1@127.0.0.1']},
                   {57,[],['ns_1@127.0.0.1']},
                   {58,[],['ns_1@127.0.0.1']},
                   {59,[],['ns_1@127.0.0.1']},
                   {60,[],['ns_1@127.0.0.1']},
                   {61,[],['ns_1@127.0.0.1']},
                   {62,[],['ns_1@127.0.0.1']},
                   {63,[],['ns_1@127.0.0.1']},
                   {64,[],['ns_1@127.0.0.1']},
                   {65,[],['ns_1@127.0.0.1']},
                   {66,[],['ns_1@127.0.0.1']},
                   {67,[],['ns_1@127.0.0.1']},
                   {68,[],['ns_1@127.0.0.1']},
                   {69,[],['ns_1@127.0.0.1']},
                   {70,[],['ns_1@127.0.0.1']},
                   {71,[],['ns_1@127.0.0.1']},
                   {72,[],['ns_1@127.0.0.1']},
                   {73,[],['ns_1@127.0.0.1']},
                   {74,[],['ns_1@127.0.0.1']},
                   {75,[],['ns_1@127.0.0.1']},
                   {76,[],['ns_1@127.0.0.1']},
                   {77,[],['ns_1@127.0.0.1']},
                   {78,[],['ns_1@127.0.0.1']},
                   {79,[],['ns_1@127.0.0.1']},
                   {80,[],['ns_1@127.0.0.1']},
                   {81,[],['ns_1@127.0.0.1']},
                   {82,[],['ns_1@127.0.0.1']},
                   {83,[],['ns_1@127.0.0.1']},
                   {84,[],['ns_1@127.0.0.1']},
                   {85,[],[...]},
                   {86,[],...},
                   {87,...},
                   {...}|...]},
             {fastForwardMap,[]},
             {uuid,<<"afd2b81586cc3bbf24b79179d05c51d0">>},
             {auth_type,sasl},
             {num_replicas,0},
             {replica_index,false},
             {ram_quota,1151336448},
             {autocompaction,false},
             {purge_interval,undefined},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,passive},
             {type,membase},
             {num_vbuckets,1024},
             {replication_topology,star},
             {repl_type,dcp},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"}]}]}]
[ns_server:debug,2020-06-16T21:33:48.673Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{46,63759562428}}]}]
[ns_server:debug,2020-06-16T21:33:48.674Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"a771316973056c9759aee152c7c79f8b">>,{6,63759562428}}],
 {configs,[{"platzi",
            [{map,[]},
             {fastForwardMap,[]},
             {uuid,<<"afd2b81586cc3bbf24b79179d05c51d0">>},
             {auth_type,sasl},
             {num_replicas,0},
             {replica_index,false},
             {ram_quota,1151336448},
             {autocompaction,false},
             {purge_interval,undefined},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,passive},
             {type,membase},
             {num_vbuckets,1024},
             {replication_topology,star},
             {repl_type,dcp},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"},
             {map_opts_hash,133465355}]}]}]
[ns_server:debug,2020-06-16T21:33:48.682Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2020-06-16T21:33:48.705Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:handle_call:128]Fully synchronized config in 22777 us
[ns_server:debug,2020-06-16T21:33:48.754Z,ns_1@127.0.0.1:<0.3017.0>:janitor_agent:query_vbuckets_loop:106]Exception from {query_vbuckets,all,[],[]} of "platzi":'ns_1@127.0.0.1'
{'EXIT',{noproc,{gen_server,call,
                            [{'janitor_agent-platzi','ns_1@127.0.0.1'},
                             {query_vbuckets,all,[],[]},
                             infinity]}}}
[ns_server:debug,2020-06-16T21:33:48.755Z,ns_1@127.0.0.1:<0.3017.0>:janitor_agent:query_vbuckets_loop_next_step:117]Waiting for "platzi" on 'ns_1@127.0.0.1'
[error_logger:info,2020-06-16T21:33:48.766Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2967.0>,docs_kv_sup}
             started: [{pid,<12941.476.0>},
                       {id,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@cb.local',"platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[stats:error,2020-06-16T21:33:48.811Z,ns_1@127.0.0.1:<0.2371.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-06-16T21:33:48.868Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.2967.0>,docs_kv_sup}
             started: [{pid,<12941.477.0>},
                       {id,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@cb.local',"platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[stats:error,2020-06-16T21:33:48.886Z,ns_1@127.0.0.1:<0.2376.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[stats:error,2020-06-16T21:33:48.889Z,ns_1@127.0.0.1:<0.2389.0>:stats_reader:log_bad_responses:238]Some nodes didn't respond: ['ns_1@127.0.0.1']
[error_logger:info,2020-06-16T21:33:48.904Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.2967.0>},
                       {id,{docs_kv_sup,"platzi"}},
                       {mfargs,{docs_kv_sup,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:33:48.906Z,ns_1@127.0.0.1:capi_doc_replicator-platzi<0.2975.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-06-16T21:33:48.906Z,ns_1@127.0.0.1:capi_doc_replicator-platzi<0.2975.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2020-06-16T21:33:48.959Z,ns_1@127.0.0.1:ns_memcached-platzi<0.3028.0>:ns_memcached:init:146]Starting ns_memcached
[ns_server:debug,2020-06-16T21:33:48.959Z,ns_1@127.0.0.1:<0.3029.0>:ns_memcached:run_connect_phase:201]Started 'connecting' phase of ns_memcached-platzi. Parent is <0.3028.0>
[error_logger:info,2020-06-16T21:33:48.967Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.3027.0>,ns_memcached_sup}
             started: [{pid,<0.3028.0>},
                       {id,{ns_memcached,"platzi"}},
                       {mfargs,{ns_memcached,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.000Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.3027.0>,ns_memcached_sup}
             started: [{pid,<0.3032.0>},
                       {id,{terse_bucket_info_uploader,"platzi"}},
                       {mfargs,
                           {terse_bucket_info_uploader,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.001Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3027.0>},
                       {id,{ns_memcached_sup,"platzi"}},
                       {mfargs,{ns_memcached_sup,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:33:49.013Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3034.0>},
                       {id,{dcp_sup,"platzi"}},
                       {mfargs,{dcp_sup,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:33:49.017Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3035.0>},
                       {id,{dcp_replication_manager,"platzi"}},
                       {mfargs,
                           {dcp_replication_manager,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.028Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3036.0>},
                       {id,{replication_manager,"platzi"}},
                       {mfargs,{replication_manager,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2020-06-16T21:33:49.058Z,ns_1@127.0.0.1:janitor_agent-platzi<0.3039.0>:dcp_sup:nuke:113]Nuking DCP replicators for bucket "platzi":
[]
[error_logger:info,2020-06-16T21:33:49.058Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-platzi'}
             started: [{pid,<0.3038.0>},
                       {id,rebalance_subprocesses_registry},
                       {mfargs,
                           {ns_process_registry,start_link,
                               ['rebalance_subprocesses_registry-platzi',
                                [{terminate_command,kill}]]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.059Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-platzi'}
             started: [{pid,<0.3039.0>},
                       {id,janitor_agent},
                       {mfargs,{janitor_agent,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.059Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3037.0>},
                       {id,{janitor_agent_sup,"platzi"}},
                       {mfargs,{janitor_agent_sup,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2020-06-16T21:33:49.069Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3040.0>},
                       {id,{stats_collector,"platzi"}},
                       {mfargs,{stats_collector,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:33:49.085Z,ns_1@127.0.0.1:ns_memcached-platzi<0.3028.0>:ns_memcached:do_ensure_bucket:1181]Created bucket "platzi" with config string "max_size=1151336448;dbname=/opt/couchbase/var/lib/couchbase/data/platzi;backend=couchdb;couch_bucket=platzi;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/platzi/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=afd2b81586cc3bbf24b79179d05c51d0;conflict_resolution_type=seqno;bucket_type=persistent;item_eviction_policy=value_only;max_ttl=0;ht_locks=47;compression_mode=passive;failpartialwarmup=false"
[ns_server:info,2020-06-16T21:33:49.088Z,ns_1@127.0.0.1:ns_memcached-platzi<0.3028.0>:ns_memcached:handle_cast:632]Main ns_memcached connection established: {ok,#Port<0.7163>}
[error_logger:info,2020-06-16T21:33:49.097Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3043.0>},
                       {id,{stats_archiver,"platzi"}},
                       {mfargs,{stats_archiver,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.098Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3049.0>},
                       {id,{stats_reader,"platzi"}},
                       {mfargs,{stats_reader,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2020-06-16T21:33:49.115Z,ns_1@127.0.0.1:janitor_agent-platzi<0.3039.0>:janitor_agent:read_flush_counter:960]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2020-06-16T21:33:49.115Z,ns_1@127.0.0.1:janitor_agent-platzi<0.3039.0>:janitor_agent:read_flush_counter_from_config:968]Initialized flushseq 0 from bucket config
[error_logger:info,2020-06-16T21:33:49.122Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3052.0>},
                       {id,{goxdcr_stats_collector,"platzi"}},
                       {mfargs,{goxdcr_stats_collector,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.125Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3054.0>},
                       {id,{goxdcr_stats_archiver,"platzi"}},
                       {mfargs,{stats_archiver,start_link,["@xdcr-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.126Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3056.0>},
                       {id,{goxdcr_stats_reader,"platzi"}},
                       {mfargs,{stats_reader,start_link,["@xdcr-platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.126Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-platzi'}
             started: [{pid,<0.3057.0>},
                       {id,{failover_safeness_level,"platzi"}},
                       {mfargs,
                           {failover_safeness_level,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2020-06-16T21:33:49.127Z,ns_1@127.0.0.1:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_sup}
             started: [{pid,<0.2947.0>},
                       {id,{single_bucket_kv_sup,"platzi"}},
                       {mfargs,{single_bucket_kv_sup,start_link,["platzi"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2020-06-16T21:33:49.198Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.373.0>:ns_heart:grab_latest_stats:264]Ignoring failure to grab "platzi" stats:
{error,no_samples}

[ns_server:debug,2020-06-16T21:33:49.309Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:49.421Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@cbas-cbauth",admin}
[ns_server:debug,2020-06-16T21:33:49.422Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@cbas",admin}
[ns_server:debug,2020-06-16T21:33:49.561Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{47,63759562429}}]}]
[ns_server:debug,2020-06-16T21:33:49.561Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/cbas/bootstrap/ensureCc/a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562429}}]}|
 <<"{}">>]
[ns_server:debug,2020-06-16T21:33:49.562Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/cbas/bootstrap/ensureCc/a771316973056c9759aee152c7c79f8b">>}]..)
[user:info,2020-06-16T21:33:49.595Z,ns_1@127.0.0.1:ns_memcached-platzi<0.3028.0>:ns_memcached:handle_cast:664]Bucket "platzi" loaded on node 'ns_1@127.0.0.1' in 0 seconds.
[ns_server:info,2020-06-16T21:33:49.667Z,ns_1@127.0.0.1:<0.2035.0>:ns_orchestrator:handle_info:523]Skipping janitor in state janitor_running
[ns_server:debug,2020-06-16T21:33:49.769Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:maybe_config_sync:242]Found states mismatch in bucket "platzi":
[{0,['ns_1@127.0.0.1'],[]},
 {1,['ns_1@127.0.0.1'],[]},
 {2,['ns_1@127.0.0.1'],[]},
 {3,['ns_1@127.0.0.1'],[]},
 {4,['ns_1@127.0.0.1'],[]},
 {5,['ns_1@127.0.0.1'],[]},
 {6,['ns_1@127.0.0.1'],[]},
 {7,['ns_1@127.0.0.1'],[]},
 {8,['ns_1@127.0.0.1'],[]},
 {9,['ns_1@127.0.0.1'],[]},
 {10,['ns_1@127.0.0.1'],[]},
 {11,['ns_1@127.0.0.1'],[]},
 {12,['ns_1@127.0.0.1'],[]},
 {13,['ns_1@127.0.0.1'],[]},
 {14,['ns_1@127.0.0.1'],[]},
 {15,['ns_1@127.0.0.1'],[]},
 {16,['ns_1@127.0.0.1'],[]},
 {17,['ns_1@127.0.0.1'],[]},
 {18,['ns_1@127.0.0.1'],[]},
 {19,['ns_1@127.0.0.1'],[]},
 {20,['ns_1@127.0.0.1'],[]},
 {21,['ns_1@127.0.0.1'],[]},
 {22,['ns_1@127.0.0.1'],[]},
 {23,['ns_1@127.0.0.1'],[]},
 {24,['ns_1@127.0.0.1'],[]},
 {25,['ns_1@127.0.0.1'],[]},
 {26,['ns_1@127.0.0.1'],[]},
 {27,['ns_1@127.0.0.1'],[]},
 {28,['ns_1@127.0.0.1'],[]},
 {29,['ns_1@127.0.0.1'],[]},
 {30,['ns_1@127.0.0.1'],[]},
 {31,['ns_1@127.0.0.1'],[]},
 {32,['ns_1@127.0.0.1'],[]},
 {33,['ns_1@127.0.0.1'],[]},
 {34,['ns_1@127.0.0.1'],[]},
 {35,['ns_1@127.0.0.1'],[]},
 {36,['ns_1@127.0.0.1'],[]},
 {37,['ns_1@127.0.0.1'],[]},
 {38,['ns_1@127.0.0.1'],[]},
 {39,['ns_1@127.0.0.1'],[]},
 {40,['ns_1@127.0.0.1'],[]},
 {41,['ns_1@127.0.0.1'],[]},
 {42,['ns_1@127.0.0.1'],[]},
 {43,['ns_1@127.0.0.1'],[]},
 {44,['ns_1@127.0.0.1'],[]},
 {45,['ns_1@127.0.0.1'],[]},
 {46,['ns_1@127.0.0.1'],[]},
 {47,['ns_1@127.0.0.1'],[]},
 {48,['ns_1@127.0.0.1'],[]},
 {49,['ns_1@127.0.0.1'],[]},
 {50,['ns_1@127.0.0.1'],[]},
 {51,['ns_1@127.0.0.1'],[]},
 {52,['ns_1@127.0.0.1'],[]},
 {53,['ns_1@127.0.0.1'],[]},
 {54,['ns_1@127.0.0.1'],[]},
 {55,['ns_1@127.0.0.1'],[]},
 {56,['ns_1@127.0.0.1'],[]},
 {57,['ns_1@127.0.0.1'],[]},
 {58,['ns_1@127.0.0.1'],[]},
 {59,['ns_1@127.0.0.1'],[]},
 {60,['ns_1@127.0.0.1'],[]},
 {61,['ns_1@127.0.0.1'],[]},
 {62,['ns_1@127.0.0.1'],[]},
 {63,['ns_1@127.0.0.1'],[]},
 {64,['ns_1@127.0.0.1'],[]},
 {65,['ns_1@127.0.0.1'],[]},
 {66,['ns_1@127.0.0.1'],[]},
 {67,['ns_1@127.0.0.1'],[]},
 {68,['ns_1@127.0.0.1'],[]},
 {69,['ns_1@127.0.0.1'],[]},
 {70,['ns_1@127.0.0.1'],[]},
 {71,['ns_1@127.0.0.1'],[]},
 {72,['ns_1@127.0.0.1'],[]},
 {73,['ns_1@127.0.0.1'],[]},
 {74,['ns_1@127.0.0.1'],[]},
 {75,['ns_1@127.0.0.1'],[]},
 {76,['ns_1@127.0.0.1'],[]},
 {77,['ns_1@127.0.0.1'],[]},
 {78,['ns_1@127.0.0.1'],[]},
 {79,['ns_1@127.0.0.1'],[]},
 {80,['ns_1@127.0.0.1'],[]},
 {81,['ns_1@127.0.0.1'],[]},
 {82,['ns_1@127.0.0.1'],[]},
 {83,['ns_1@127.0.0.1'],[]},
 {84,['ns_1@127.0.0.1'],[]},
 {85,['ns_1@127.0.0.1'],[]},
 {86,['ns_1@127.0.0.1'],[]},
 {87,['ns_1@127.0.0.1'],[]},
 {88,['ns_1@127.0.0.1'],[]},
 {89,['ns_1@127.0.0.1'],[]},
 {90,['ns_1@127.0.0.1'],[]},
 {91,['ns_1@127.0.0.1'],[]},
 {92,['ns_1@127.0.0.1'],[]},
 {93,['ns_1@127.0.0.1'],[]},
 {94,['ns_1@127.0.0.1'],[]},
 {95,['ns_1@127.0.0.1'],[]},
 {96,['ns_1@127.0.0.1'],[]},
 {97,['ns_1@127.0.0.1'],[]},
 {98,['ns_1@127.0.0.1'],[]},
 {99,['ns_1@127.0.0.1'],[]},
 {100,['ns_1@127.0.0.1'],[]},
 {101,['ns_1@127.0.0.1'],[]},
 {102,['ns_1@127.0.0.1'],[]},
 {103,['ns_1@127.0.0.1'],[]},
 {104,['ns_1@127.0.0.1'],[]},
 {105,['ns_1@127.0.0.1'],[]},
 {106,['ns_1@127.0.0.1'],[]},
 {107,['ns_1@127.0.0.1'],[]},
 {108,['ns_1@127.0.0.1'],[]},
 {109,['ns_1@127.0.0.1'],[]},
 {110,['ns_1@127.0.0.1'],[]},
 {111,['ns_1@127.0.0.1'],[]},
 {112,['ns_1@127.0.0.1'],[]},
 {113,['ns_1@127.0.0.1'],[]},
 {114,['ns_1@127.0.0.1'],[]},
 {115,['ns_1@127.0.0.1'],[]},
 {116,['ns_1@127.0.0.1'],[]},
 {117,['ns_1@127.0.0.1'],[]},
 {118,['ns_1@127.0.0.1'],[]},
 {119,['ns_1@127.0.0.1'],[]},
 {120,['ns_1@127.0.0.1'],[]},
 {121,['ns_1@127.0.0.1'],[]},
 {122,['ns_1@127.0.0.1'],[]},
 {123,['ns_1@127.0.0.1'],[]},
 {124,['ns_1@127.0.0.1'],[]},
 {125,['ns_1@127.0.0.1'],[]},
 {126,['ns_1@127.0.0.1'],[]},
 {127,['ns_1@127.0.0.1'],[]},
 {128,['ns_1@127.0.0.1'],[]},
 {129,['ns_1@127.0.0.1'],[]},
 {130,['ns_1@127.0.0.1'],[]},
 {131,['ns_1@127.0.0.1'],[]},
 {132,['ns_1@127.0.0.1'],[]},
 {133,['ns_1@127.0.0.1'],[]},
 {134,['ns_1@127.0.0.1'],[]},
 {135,['ns_1@127.0.0.1'],[]},
 {136,['ns_1@127.0.0.1'],[]},
 {137,['ns_1@127.0.0.1'],[]},
 {138,['ns_1@127.0.0.1'],[]},
 {139,['ns_1@127.0.0.1'],[]},
 {140,['ns_1@127.0.0.1'],[]},
 {141,['ns_1@127.0.0.1'],[]},
 {142,['ns_1@127.0.0.1'],[]},
 {143,['ns_1@127.0.0.1'],[]},
 {144,['ns_1@127.0.0.1'],[]},
 {145,['ns_1@127.0.0.1'],[]},
 {146,['ns_1@127.0.0.1'],[]},
 {147,['ns_1@127.0.0.1'],[]},
 {148,['ns_1@127.0.0.1'],[]},
 {149,['ns_1@127.0.0.1'],[]},
 {150,['ns_1@127.0.0.1'],[]},
 {151,['ns_1@127.0.0.1'],[]},
 {152,['ns_1@127.0.0.1'],[]},
 {153,['ns_1@127.0.0.1'],[]},
 {154,['ns_1@127.0.0.1'],[]},
 {155,['ns_1@127.0.0.1'],[]},
 {156,['ns_1@127.0.0.1'],[]},
 {157,['ns_1@127.0.0.1'],[]},
 {158,['ns_1@127.0.0.1'],[]},
 {159,['ns_1@127.0.0.1'],[]},
 {160,['ns_1@127.0.0.1'],[]},
 {161,['ns_1@127.0.0.1'],[]},
 {162,['ns_1@127.0.0.1'],[]},
 {163,['ns_1@127.0.0.1'],[]},
 {164,['ns_1@127.0.0.1'],[]},
 {165,['ns_1@127.0.0.1'],[]},
 {166,['ns_1@127.0.0.1'],[]},
 {167,['ns_1@127.0.0.1'],[]},
 {168,['ns_1@127.0.0.1'],[]},
 {169,['ns_1@127.0.0.1'],[]},
 {170,['ns_1@127.0.0.1'],[]},
 {171,['ns_1@127.0.0.1'],[]},
 {172,['ns_1@127.0.0.1'],[]},
 {173,['ns_1@127.0.0.1'],[]},
 {174,['ns_1@127.0.0.1'],[]},
 {175,['ns_1@127.0.0.1'],[]},
 {176,['ns_1@127.0.0.1'],[]},
 {177,['ns_1@127.0.0.1'],[]},
 {178,['ns_1@127.0.0.1'],[]},
 {179,['ns_1@127.0.0.1'],[]},
 {180,['ns_1@127.0.0.1'],[]},
 {181,['ns_1@127.0.0.1'],[]},
 {182,['ns_1@127.0.0.1'],[]},
 {183,['ns_1@127.0.0.1'],[]},
 {184,['ns_1@127.0.0.1'],[]},
 {185,['ns_1@127.0.0.1'],[]},
 {186,['ns_1@127.0.0.1'],[]},
 {187,['ns_1@127.0.0.1'],[]},
 {188,['ns_1@127.0.0.1'],[]},
 {189,['ns_1@127.0.0.1'],[]},
 {190,['ns_1@127.0.0.1'],[]},
 {191,['ns_1@127.0.0.1'],[]},
 {192,['ns_1@127.0.0.1'],[]},
 {193,['ns_1@127.0.0.1'],[]},
 {194,['ns_1@127.0.0.1'],[]},
 {195,['ns_1@127.0.0.1'],[]},
 {196,['ns_1@127.0.0.1'],[]},
 {197,['ns_1@127.0.0.1'],[]},
 {198,['ns_1@127.0.0.1'],[]},
 {199,['ns_1@127.0.0.1'],[]},
 {200,['ns_1@127.0.0.1'],[]},
 {201,['ns_1@127.0.0.1'],[]},
 {202,['ns_1@127.0.0.1'],[]},
 {203,['ns_1@127.0.0.1'],[]},
 {204,['ns_1@127.0.0.1'],[]},
 {205,['ns_1@127.0.0.1'],[]},
 {206,['ns_1@127.0.0.1'],[]},
 {207,['ns_1@127.0.0.1'],[]},
 {208,['ns_1@127.0.0.1'],[]},
 {209,['ns_1@127.0.0.1'],[]},
 {210,['ns_1@127.0.0.1'],[]},
 {211,['ns_1@127.0.0.1'],[]},
 {212,['ns_1@127.0.0.1'],[]},
 {213,['ns_1@127.0.0.1'],[]},
 {214,['ns_1@127.0.0.1'],[]},
 {215,['ns_1@127.0.0.1'],[]},
 {216,['ns_1@127.0.0.1'],[]},
 {217,['ns_1@127.0.0.1'],[]},
 {218,['ns_1@127.0.0.1'],[]},
 {219,['ns_1@127.0.0.1'],[]},
 {220,['ns_1@127.0.0.1'],[]},
 {221,['ns_1@127.0.0.1'],[]},
 {222,['ns_1@127.0.0.1'],[]},
 {223,['ns_1@127.0.0.1'],[]},
 {224,['ns_1@127.0.0.1'],[]},
 {225,['ns_1@127.0.0.1'],[]},
 {226,['ns_1@127.0.0.1'],[]},
 {227,['ns_1@127.0.0.1'],[]},
 {228,['ns_1@127.0.0.1'],[]},
 {229,['ns_1@127.0.0.1'],[]},
 {230,['ns_1@127.0.0.1'],[]},
 {231,['ns_1@127.0.0.1'],[]},
 {232,['ns_1@127.0.0.1'],[]},
 {233,['ns_1@127.0.0.1'],[]},
 {234,['ns_1@127.0.0.1'],[]},
 {235,['ns_1@127.0.0.1'],[]},
 {236,['ns_1@127.0.0.1'],[]},
 {237,['ns_1@127.0.0.1'],[]},
 {238,['ns_1@127.0.0.1'],[]},
 {239,['ns_1@127.0.0.1'],[]},
 {240,['ns_1@127.0.0.1'],[]},
 {241,['ns_1@127.0.0.1'],[]},
 {242,['ns_1@127.0.0.1'],[]},
 {243,['ns_1@127.0.0.1'],[]},
 {244,['ns_1@127.0.0.1'],[]},
 {245,['ns_1@127.0.0.1'],[]},
 {246,['ns_1@127.0.0.1'],[]},
 {247,['ns_1@127.0.0.1'],[]},
 {248,['ns_1@127.0.0.1'],[]},
 {249,['ns_1@127.0.0.1'],[]},
 {250,['ns_1@127.0.0.1'],[]},
 {251,['ns_1@127.0.0.1'],[]},
 {252,['ns_1@127.0.0.1'],[]},
 {253,['ns_1@127.0.0.1'],[]},
 {254,['ns_1@127.0.0.1'],[]},
 {255,['ns_1@127.0.0.1'],[]},
 {256,['ns_1@127.0.0.1'],[]},
 {257,['ns_1@127.0.0.1'],[]},
 {258,['ns_1@127.0.0.1'],[]},
 {259,['ns_1@127.0.0.1'],[]},
 {260,['ns_1@127.0.0.1'],[]},
 {261,['ns_1@127.0.0.1'],[]},
 {262,['ns_1@127.0.0.1'],[]},
 {263,['ns_1@127.0.0.1'],[]},
 {264,['ns_1@127.0.0.1'],[]},
 {265,['ns_1@127.0.0.1'],[]},
 {266,['ns_1@127.0.0.1'],[]},
 {267,['ns_1@127.0.0.1'],[]},
 {268,['ns_1@127.0.0.1'],[]},
 {269,['ns_1@127.0.0.1'],[]},
 {270,['ns_1@127.0.0.1'],[]},
 {271,['ns_1@127.0.0.1'],[]},
 {272,['ns_1@127.0.0.1'],[]},
 {273,['ns_1@127.0.0.1'],[]},
 {274,['ns_1@127.0.0.1'],[]},
 {275,['ns_1@127.0.0.1'],[]},
 {276,['ns_1@127.0.0.1'],[]},
 {277,['ns_1@127.0.0.1'],[]},
 {278,['ns_1@127.0.0.1'],[]},
 {279,['ns_1@127.0.0.1'],[]},
 {280,['ns_1@127.0.0.1'],[]},
 {281,['ns_1@127.0.0.1'],[]},
 {282,['ns_1@127.0.0.1'],[]},
 {283,['ns_1@127.0.0.1'],[]},
 {284,['ns_1@127.0.0.1'],[]},
 {285,['ns_1@127.0.0.1'],[]},
 {286,['ns_1@127.0.0.1'],[]},
 {287,['ns_1@127.0.0.1'],[]},
 {288,['ns_1@127.0.0.1'],[]},
 {289,['ns_1@127.0.0.1'],[]},
 {290,['ns_1@127.0.0.1'],[]},
 {291,['ns_1@127.0.0.1'],[]},
 {292,['ns_1@127.0.0.1'],[]},
 {293,['ns_1@127.0.0.1'],[]},
 {294,['ns_1@127.0.0.1'],[]},
 {295,['ns_1@127.0.0.1'],[]},
 {296,['ns_1@127.0.0.1'],[]},
 {297,['ns_1@127.0.0.1'],[]},
 {298,['ns_1@127.0.0.1'],[]},
 {299,['ns_1@127.0.0.1'],[]},
 {300,['ns_1@127.0.0.1'],[]},
 {301,['ns_1@127.0.0.1'],[]},
 {302,['ns_1@127.0.0.1'],[]},
 {303,['ns_1@127.0.0.1'],[]},
 {304,['ns_1@127.0.0.1'],[]},
 {305,['ns_1@127.0.0.1'],[]},
 {306,['ns_1@127.0.0.1'],[]},
 {307,['ns_1@127.0.0.1'],[]},
 {308,['ns_1@127.0.0.1'],[]},
 {309,['ns_1@127.0.0.1'],[]},
 {310,['ns_1@127.0.0.1'],[]},
 {311,['ns_1@127.0.0.1'],[]},
 {312,['ns_1@127.0.0.1'],[]},
 {313,['ns_1@127.0.0.1'],[]},
 {314,['ns_1@127.0.0.1'],[]},
 {315,['ns_1@127.0.0.1'],[]},
 {316,['ns_1@127.0.0.1'],[]},
 {317,['ns_1@127.0.0.1'],[]},
 {318,['ns_1@127.0.0.1'],[]},
 {319,['ns_1@127.0.0.1'],[]},
 {320,['ns_1@127.0.0.1'],[]},
 {321,['ns_1@127.0.0.1'],[]},
 {322,['ns_1@127.0.0.1'],[]},
 {323,['ns_1@127.0.0.1'],[]},
 {324,['ns_1@127.0.0.1'],[]},
 {325,['ns_1@127.0.0.1'],[]},
 {326,['ns_1@127.0.0.1'],[]},
 {327,['ns_1@127.0.0.1'],[]},
 {328,['ns_1@127.0.0.1'],[]},
 {329,['ns_1@127.0.0.1'],[]},
 {330,['ns_1@127.0.0.1'],[]},
 {331,['ns_1@127.0.0.1'],[]},
 {332,['ns_1@127.0.0.1'],[]},
 {333,['ns_1@127.0.0.1'],[]},
 {334,['ns_1@127.0.0.1'],[]},
 {335,['ns_1@127.0.0.1'],[]},
 {336,['ns_1@127.0.0.1'],[]},
 {337,['ns_1@127.0.0.1'],[]},
 {338,['ns_1@127.0.0.1'],[]},
 {339,['ns_1@127.0.0.1'],[]},
 {340,['ns_1@127.0.0.1'],[]},
 {341,['ns_1@127.0.0.1'],[]},
 {342,['ns_1@127.0.0.1'],[]},
 {343,['ns_1@127.0.0.1'],[]},
 {344,['ns_1@127.0.0.1'],[]},
 {345,['ns_1@127.0.0.1'],[]},
 {346,['ns_1@127.0.0.1'],[]},
 {347,['ns_1@127.0.0.1'],[]},
 {348,['ns_1@127.0.0.1'],[]},
 {349,['ns_1@127.0.0.1'],[]},
 {350,['ns_1@127.0.0.1'],[]},
 {351,['ns_1@127.0.0.1'],[]},
 {352,['ns_1@127.0.0.1'],[]},
 {353,['ns_1@127.0.0.1'],[]},
 {354,['ns_1@127.0.0.1'],[]},
 {355,['ns_1@127.0.0.1'],[]},
 {356,['ns_1@127.0.0.1'],[]},
 {357,['ns_1@127.0.0.1'],[]},
 {358,['ns_1@127.0.0.1'],[]},
 {359,['ns_1@127.0.0.1'],[]},
 {360,['ns_1@127.0.0.1'],[]},
 {361,['ns_1@127.0.0.1'],[]},
 {362,['ns_1@127.0.0.1'],[]},
 {363,['ns_1@127.0.0.1'],[]},
 {364,['ns_1@127.0.0.1'],[]},
 {365,['ns_1@127.0.0.1'],[]},
 {366,['ns_1@127.0.0.1'],[]},
 {367,['ns_1@127.0.0.1'],[]},
 {368,['ns_1@127.0.0.1'],[]},
 {369,['ns_1@127.0.0.1'],[]},
 {370,['ns_1@127.0.0.1'],[]},
 {371,['ns_1@127.0.0.1'],[]},
 {372,['ns_1@127.0.0.1'],[]},
 {373,['ns_1@127.0.0.1'],[]},
 {374,['ns_1@127.0.0.1'],[]},
 {375,['ns_1@127.0.0.1'],[]},
 {376,['ns_1@127.0.0.1'],[]},
 {377,['ns_1@127.0.0.1'],[]},
 {378,['ns_1@127.0.0.1'],[]},
 {379,['ns_1@127.0.0.1'],[]},
 {380,['ns_1@127.0.0.1'],[]},
 {381,['ns_1@127.0.0.1'],[]},
 {382,['ns_1@127.0.0.1'],[]},
 {383,['ns_1@127.0.0.1'],[]},
 {384,['ns_1@127.0.0.1'],[]},
 {385,['ns_1@127.0.0.1'],[]},
 {386,['ns_1@127.0.0.1'],[]},
 {387,['ns_1@127.0.0.1'],[]},
 {388,['ns_1@127.0.0.1'],[]},
 {389,['ns_1@127.0.0.1'],[]},
 {390,['ns_1@127.0.0.1'],[]},
 {391,['ns_1@127.0.0.1'],[]},
 {392,['ns_1@127.0.0.1'],[]},
 {393,['ns_1@127.0.0.1'],[]},
 {394,['ns_1@127.0.0.1'],[]},
 {395,['ns_1@127.0.0.1'],[]},
 {396,['ns_1@127.0.0.1'],[]},
 {397,['ns_1@127.0.0.1'],[]},
 {398,['ns_1@127.0.0.1'],[]},
 {399,['ns_1@127.0.0.1'],[]},
 {400,['ns_1@127.0.0.1'],[]},
 {401,['ns_1@127.0.0.1'],[]},
 {402,['ns_1@127.0.0.1'],[]},
 {403,['ns_1@127.0.0.1'],[]},
 {404,['ns_1@127.0.0.1'],[]},
 {405,['ns_1@127.0.0.1'],[]},
 {406,['ns_1@127.0.0.1'],[]},
 {407,['ns_1@127.0.0.1'],[]},
 {408,['ns_1@127.0.0.1'],[]},
 {409,['ns_1@127.0.0.1'],[]},
 {410,['ns_1@127.0.0.1'],[]},
 {411,['ns_1@127.0.0.1'],[]},
 {412,['ns_1@127.0.0.1'],[]},
 {413,['ns_1@127.0.0.1'],[]},
 {414,['ns_1@127.0.0.1'],[]},
 {415,['ns_1@127.0.0.1'],[]},
 {416,['ns_1@127.0.0.1'],[]},
 {417,['ns_1@127.0.0.1'],[]},
 {418,['ns_1@127.0.0.1'],[]},
 {419,['ns_1@127.0.0.1'],[]},
 {420,['ns_1@127.0.0.1'],[]},
 {421,['ns_1@127.0.0.1'],[]},
 {422,['ns_1@127.0.0.1'],[]},
 {423,['ns_1@127.0.0.1'],[]},
 {424,['ns_1@127.0.0.1'],[]},
 {425,['ns_1@127.0.0.1'],[]},
 {426,['ns_1@127.0.0.1'],[]},
 {427,['ns_1@127.0.0.1'],[]},
 {428,['ns_1@127.0.0.1'],[]},
 {429,['ns_1@127.0.0.1'],[]},
 {430,['ns_1@127.0.0.1'],[]},
 {431,['ns_1@127.0.0.1'],[]},
 {432,['ns_1@127.0.0.1'],[]},
 {433,['ns_1@127.0.0.1'],[]},
 {434,['ns_1@127.0.0.1'],[]},
 {435,['ns_1@127.0.0.1'],[]},
 {436,['ns_1@127.0.0.1'],[]},
 {437,['ns_1@127.0.0.1'],[]},
 {438,['ns_1@127.0.0.1'],[]},
 {439,['ns_1@127.0.0.1'],[]},
 {440,['ns_1@127.0.0.1'],[]},
 {441,['ns_1@127.0.0.1'],[]},
 {442,['ns_1@127.0.0.1'],[]},
 {443,['ns_1@127.0.0.1'],[]},
 {444,['ns_1@127.0.0.1'],[]},
 {445,['ns_1@127.0.0.1'],[]},
 {446,['ns_1@127.0.0.1'],[]},
 {447,['ns_1@127.0.0.1'],[]},
 {448,['ns_1@127.0.0.1'],[]},
 {449,['ns_1@127.0.0.1'],[]},
 {450,['ns_1@127.0.0.1'],[]},
 {451,['ns_1@127.0.0.1'],[]},
 {452,['ns_1@127.0.0.1'],[]},
 {453,['ns_1@127.0.0.1'],[]},
 {454,['ns_1@127.0.0.1'],[]},
 {455,['ns_1@127.0.0.1'],[]},
 {456,['ns_1@127.0.0.1'],[]},
 {457,['ns_1@127.0.0.1'],[]},
 {458,['ns_1@127.0.0.1'],[]},
 {459,['ns_1@127.0.0.1'],[]},
 {460,['ns_1@127.0.0.1'],[]},
 {461,['ns_1@127.0.0.1'],[]},
 {462,['ns_1@127.0.0.1'],[]},
 {463,['ns_1@127.0.0.1'],[]},
 {464,['ns_1@127.0.0.1'],[]},
 {465,['ns_1@127.0.0.1'],[]},
 {466,['ns_1@127.0.0.1'],[]},
 {467,['ns_1@127.0.0.1'],[]},
 {468,['ns_1@127.0.0.1'],[]},
 {469,['ns_1@127.0.0.1'],[]},
 {470,['ns_1@127.0.0.1'],[]},
 {471,['ns_1@127.0.0.1'],[]},
 {472,['ns_1@127.0.0.1'],[]},
 {473,['ns_1@127.0.0.1'],[]},
 {474,['ns_1@127.0.0.1'],[]},
 {475,['ns_1@127.0.0.1'],[]},
 {476,['ns_1@127.0.0.1'],[]},
 {477,['ns_1@127.0.0.1'],[]},
 {478,['ns_1@127.0.0.1'],[]},
 {479,['ns_1@127.0.0.1'],[]},
 {480,['ns_1@127.0.0.1'],[]},
 {481,['ns_1@127.0.0.1'],[]},
 {482,['ns_1@127.0.0.1'],[]},
 {483,['ns_1@127.0.0.1'],[]},
 {484,['ns_1@127.0.0.1'],[]},
 {485,['ns_1@127.0.0.1'],[]},
 {486,['ns_1@127.0.0.1'],[]},
 {487,['ns_1@127.0.0.1'],[]},
 {488,['ns_1@127.0.0.1'],[]},
 {489,['ns_1@127.0.0.1'],[]},
 {490,['ns_1@127.0.0.1'],[]},
 {491,['ns_1@127.0.0.1'],[]},
 {492,['ns_1@127.0.0.1'],[]},
 {493,['ns_1@127.0.0.1'],[]},
 {494,['ns_1@127.0.0.1'],[]},
 {495,['ns_1@127.0.0.1'],[]},
 {496,['ns_1@127.0.0.1'],[]},
 {497,['ns_1@127.0.0.1'],[]},
 {498,['ns_1@127.0.0.1'],[]},
 {499,['ns_1@127.0.0.1'],[]},
 {500,['ns_1@127.0.0.1'],[]},
 {501,['ns_1@127.0.0.1'],[]},
 {502,['ns_1@127.0.0.1'],[]},
 {503,['ns_1@127.0.0.1'],[]},
 {504,['ns_1@127.0.0.1'],[]},
 {505,['ns_1@127.0.0.1'],[]},
 {506,['ns_1@127.0.0.1'],[]},
 {507,['ns_1@127.0.0.1'],[]},
 {508,['ns_1@127.0.0.1'],[]},
 {509,['ns_1@127.0.0.1'],[]},
 {510,['ns_1@127.0.0.1'],[]},
 {511,['ns_1@127.0.0.1'],[]},
 {512,['ns_1@127.0.0.1'],[]},
 {513,['ns_1@127.0.0.1'],[]},
 {514,['ns_1@127.0.0.1'],[]},
 {515,['ns_1@127.0.0.1'],[]},
 {516,['ns_1@127.0.0.1'],[]},
 {517,['ns_1@127.0.0.1'],[]},
 {518,['ns_1@127.0.0.1'],[]},
 {519,['ns_1@127.0.0.1'],[]},
 {520,['ns_1@127.0.0.1'],[]},
 {521,['ns_1@127.0.0.1'],[]},
 {522,['ns_1@127.0.0.1'],[]},
 {523,['ns_1@127.0.0.1'],[]},
 {524,['ns_1@127.0.0.1'],[]},
 {525,['ns_1@127.0.0.1'],[]},
 {526,['ns_1@127.0.0.1'],[]},
 {527,['ns_1@127.0.0.1'],[]},
 {528,['ns_1@127.0.0.1'],[]},
 {529,['ns_1@127.0.0.1'],[]},
 {530,['ns_1@127.0.0.1'],[]},
 {531,['ns_1@127.0.0.1'],[]},
 {532,['ns_1@127.0.0.1'],[]},
 {533,['ns_1@127.0.0.1'],[]},
 {534,['ns_1@127.0.0.1'],[]},
 {535,['ns_1@127.0.0.1'],[]},
 {536,['ns_1@127.0.0.1'],[]},
 {537,['ns_1@127.0.0.1'],[]},
 {538,['ns_1@127.0.0.1'],[]},
 {539,['ns_1@127.0.0.1'],[]},
 {540,['ns_1@127.0.0.1'],[]},
 {541,['ns_1@127.0.0.1'],[]},
 {542,['ns_1@127.0.0.1'],[]},
 {543,['ns_1@127.0.0.1'],[]},
 {544,['ns_1@127.0.0.1'],[]},
 {545,['ns_1@127.0.0.1'],[]},
 {546,['ns_1@127.0.0.1'],[]},
 {547,['ns_1@127.0.0.1'],[]},
 {548,['ns_1@127.0.0.1'],[]},
 {549,['ns_1@127.0.0.1'],[]},
 {550,['ns_1@127.0.0.1'],[]},
 {551,['ns_1@127.0.0.1'],[]},
 {552,['ns_1@127.0.0.1'],[]},
 {553,['ns_1@127.0.0.1'],[]},
 {554,['ns_1@127.0.0.1'],[]},
 {555,['ns_1@127.0.0.1'],[]},
 {556,['ns_1@127.0.0.1'],[]},
 {557,['ns_1@127.0.0.1'],[]},
 {558,['ns_1@127.0.0.1'],[]},
 {559,['ns_1@127.0.0.1'],[]},
 {560,['ns_1@127.0.0.1'],[]},
 {561,['ns_1@127.0.0.1'],[]},
 {562,['ns_1@127.0.0.1'],[]},
 {563,['ns_1@127.0.0.1'],[]},
 {564,['ns_1@127.0.0.1'],[]},
 {565,['ns_1@127.0.0.1'],[]},
 {566,['ns_1@127.0.0.1'],[]},
 {567,['ns_1@127.0.0.1'],[]},
 {568,['ns_1@127.0.0.1'],[]},
 {569,['ns_1@127.0.0.1'],[]},
 {570,['ns_1@127.0.0.1'],[]},
 {571,['ns_1@127.0.0.1'],[]},
 {572,['ns_1@127.0.0.1'],[]},
 {573,['ns_1@127.0.0.1'],[]},
 {574,['ns_1@127.0.0.1'],[]},
 {575,['ns_1@127.0.0.1'],[]},
 {576,['ns_1@127.0.0.1'],[]},
 {577,['ns_1@127.0.0.1'],[]},
 {578,['ns_1@127.0.0.1'],[]},
 {579,['ns_1@127.0.0.1'],[]},
 {580,['ns_1@127.0.0.1'],[]},
 {581,['ns_1@127.0.0.1'],[]},
 {582,['ns_1@127.0.0.1'],[]},
 {583,['ns_1@127.0.0.1'],[]},
 {584,['ns_1@127.0.0.1'],[]},
 {585,['ns_1@127.0.0.1'],[]},
 {586,['ns_1@127.0.0.1'],[]},
 {587,['ns_1@127.0.0.1'],[]},
 {588,['ns_1@127.0.0.1'],[]},
 {589,['ns_1@127.0.0.1'],[]},
 {590,['ns_1@127.0.0.1'],[]},
 {591,['ns_1@127.0.0.1'],[]},
 {592,['ns_1@127.0.0.1'],[]},
 {593,['ns_1@127.0.0.1'],[]},
 {594,['ns_1@127.0.0.1'],[]},
 {595,['ns_1@127.0.0.1'],[]},
 {596,['ns_1@127.0.0.1'],[]},
 {597,['ns_1@127.0.0.1'],[]},
 {598,['ns_1@127.0.0.1'],[]},
 {599,['ns_1@127.0.0.1'],[]},
 {600,['ns_1@127.0.0.1'],[]},
 {601,['ns_1@127.0.0.1'],[]},
 {602,['ns_1@127.0.0.1'],[]},
 {603,['ns_1@127.0.0.1'],[]},
 {604,['ns_1@127.0.0.1'],[]},
 {605,['ns_1@127.0.0.1'],[]},
 {606,['ns_1@127.0.0.1'],[]},
 {607,['ns_1@127.0.0.1'],[]},
 {608,['ns_1@127.0.0.1'],[]},
 {609,['ns_1@127.0.0.1'],[]},
 {610,['ns_1@127.0.0.1'],[]},
 {611,['ns_1@127.0.0.1'],[]},
 {612,['ns_1@127.0.0.1'],[]},
 {613,['ns_1@127.0.0.1'],[]},
 {614,['ns_1@127.0.0.1'],[]},
 {615,['ns_1@127.0.0.1'],[]},
 {616,['ns_1@127.0.0.1'],[]},
 {617,['ns_1@127.0.0.1'],[]},
 {618,['ns_1@127.0.0.1'],[]},
 {619,['ns_1@127.0.0.1'],[]},
 {620,['ns_1@127.0.0.1'],[]},
 {621,['ns_1@127.0.0.1'],[]},
 {622,['ns_1@127.0.0.1'],[]},
 {623,['ns_1@127.0.0.1'],[]},
 {624,['ns_1@127.0.0.1'],[]},
 {625,['ns_1@127.0.0.1'],[]},
 {626,['ns_1@127.0.0.1'],[]},
 {627,['ns_1@127.0.0.1'],[]},
 {628,['ns_1@127.0.0.1'],[]},
 {629,['ns_1@127.0.0.1'],[]},
 {630,['ns_1@127.0.0.1'],[]},
 {631,['ns_1@127.0.0.1'],[]},
 {632,['ns_1@127.0.0.1'],[]},
 {633,['ns_1@127.0.0.1'],[]},
 {634,['ns_1@127.0.0.1'],[]},
 {635,['ns_1@127.0.0.1'],[]},
 {636,['ns_1@127.0.0.1'],[]},
 {637,['ns_1@127.0.0.1'],[]},
 {638,['ns_1@127.0.0.1'],[]},
 {639,['ns_1@127.0.0.1'],[]},
 {640,['ns_1@127.0.0.1'],[]},
 {641,['ns_1@127.0.0.1'],[]},
 {642,['ns_1@127.0.0.1'],[]},
 {643,['ns_1@127.0.0.1'],[]},
 {644,['ns_1@127.0.0.1'],[]},
 {645,['ns_1@127.0.0.1'],[]},
 {646,['ns_1@127.0.0.1'],[]},
 {647,['ns_1@127.0.0.1'],[]},
 {648,['ns_1@127.0.0.1'],[]},
 {649,['ns_1@127.0.0.1'],[]},
 {650,['ns_1@127.0.0.1'],[]},
 {651,['ns_1@127.0.0.1'],[]},
 {652,['ns_1@127.0.0.1'],[]},
 {653,['ns_1@127.0.0.1'],[]},
 {654,['ns_1@127.0.0.1'],[]},
 {655,['ns_1@127.0.0.1'],[]},
 {656,['ns_1@127.0.0.1'],[]},
 {657,['ns_1@127.0.0.1'],[]},
 {658,['ns_1@127.0.0.1'],[]},
 {659,['ns_1@127.0.0.1'],[]},
 {660,['ns_1@127.0.0.1'],[]},
 {661,['ns_1@127.0.0.1'],[]},
 {662,['ns_1@127.0.0.1'],[]},
 {663,['ns_1@127.0.0.1'],[]},
 {664,['ns_1@127.0.0.1'],[]},
 {665,['ns_1@127.0.0.1'],[]},
 {666,['ns_1@127.0.0.1'],[]},
 {667,['ns_1@127.0.0.1'],[]},
 {668,['ns_1@127.0.0.1'],[]},
 {669,['ns_1@127.0.0.1'],[]},
 {670,['ns_1@127.0.0.1'],[]},
 {671,['ns_1@127.0.0.1'],[]},
 {672,['ns_1@127.0.0.1'],[]},
 {673,['ns_1@127.0.0.1'],[]},
 {674,['ns_1@127.0.0.1'],[]},
 {675,['ns_1@127.0.0.1'],[]},
 {676,['ns_1@127.0.0.1'],[]},
 {677,['ns_1@127.0.0.1'],[]},
 {678,['ns_1@127.0.0.1'],[]},
 {679,['ns_1@127.0.0.1'],[]},
 {680,['ns_1@127.0.0.1'],[]},
 {681,['ns_1@127.0.0.1'],[]},
 {682,['ns_1@127.0.0.1'],[]},
 {683,['ns_1@127.0.0.1'],[]},
 {684,['ns_1@127.0.0.1'],[]},
 {685,['ns_1@127.0.0.1'],[]},
 {686,['ns_1@127.0.0.1'],[]},
 {687,['ns_1@127.0.0.1'],[]},
 {688,['ns_1@127.0.0.1'],[]},
 {689,['ns_1@127.0.0.1'],[]},
 {690,['ns_1@127.0.0.1'],[]},
 {691,['ns_1@127.0.0.1'],[]},
 {692,['ns_1@127.0.0.1'],[]},
 {693,['ns_1@127.0.0.1'],[]},
 {694,['ns_1@127.0.0.1'],[]},
 {695,['ns_1@127.0.0.1'],[]},
 {696,['ns_1@127.0.0.1'],[]},
 {697,['ns_1@127.0.0.1'],[]},
 {698,['ns_1@127.0.0.1'],[]},
 {699,['ns_1@127.0.0.1'],[]},
 {700,['ns_1@127.0.0.1'],[]},
 {701,['ns_1@127.0.0.1'],[]},
 {702,['ns_1@127.0.0.1'],[]},
 {703,['ns_1@127.0.0.1'],[]},
 {704,['ns_1@127.0.0.1'],[]},
 {705,['ns_1@127.0.0.1'],[]},
 {706,['ns_1@127.0.0.1'],[]},
 {707,['ns_1@127.0.0.1'],[]},
 {708,['ns_1@127.0.0.1'],[]},
 {709,['ns_1@127.0.0.1'],[]},
 {710,['ns_1@127.0.0.1'],[]},
 {711,['ns_1@127.0.0.1'],[]},
 {712,['ns_1@127.0.0.1'],[]},
 {713,['ns_1@127.0.0.1'],[]},
 {714,['ns_1@127.0.0.1'],[]},
 {715,['ns_1@127.0.0.1'],[]},
 {716,['ns_1@127.0.0.1'],[]},
 {717,['ns_1@127.0.0.1'],[]},
 {718,['ns_1@127.0.0.1'],[]},
 {719,['ns_1@127.0.0.1'],[]},
 {720,['ns_1@127.0.0.1'],[]},
 {721,['ns_1@127.0.0.1'],[]},
 {722,['ns_1@127.0.0.1'],[]},
 {723,['ns_1@127.0.0.1'],[]},
 {724,['ns_1@127.0.0.1'],[]},
 {725,['ns_1@127.0.0.1'],[]},
 {726,['ns_1@127.0.0.1'],[]},
 {727,['ns_1@127.0.0.1'],[]},
 {728,['ns_1@127.0.0.1'],[]},
 {729,['ns_1@127.0.0.1'],[]},
 {730,['ns_1@127.0.0.1'],[]},
 {731,['ns_1@127.0.0.1'],[]},
 {732,['ns_1@127.0.0.1'],[]},
 {733,['ns_1@127.0.0.1'],[]},
 {734,['ns_1@127.0.0.1'],[]},
 {735,['ns_1@127.0.0.1'],[]},
 {736,['ns_1@127.0.0.1'],[]},
 {737,['ns_1@127.0.0.1'],[]},
 {738,['ns_1@127.0.0.1'],[]},
 {739,['ns_1@127.0.0.1'],[]},
 {740,['ns_1@127.0.0.1'],[]},
 {741,['ns_1@127.0.0.1'],[]},
 {742,['ns_1@127.0.0.1'],[]},
 {743,['ns_1@127.0.0.1'],[]},
 {744,['ns_1@127.0.0.1'],[]},
 {745,['ns_1@127.0.0.1'],[]},
 {746,['ns_1@127.0.0.1'],[]},
 {747,['ns_1@127.0.0.1'],[]},
 {748,['ns_1@127.0.0.1'],[]},
 {749,['ns_1@127.0.0.1'],[]},
 {750,['ns_1@127.0.0.1'],[]},
 {751,['ns_1@127.0.0.1'],[]},
 {752,['ns_1@127.0.0.1'],[]},
 {753,['ns_1@127.0.0.1'],[]},
 {754,['ns_1@127.0.0.1'],[]},
 {755,['ns_1@127.0.0.1'],[]},
 {756,['ns_1@127.0.0.1'],[]},
 {757,['ns_1@127.0.0.1'],[]},
 {758,['ns_1@127.0.0.1'],[]},
 {759,['ns_1@127.0.0.1'],[]},
 {760,['ns_1@127.0.0.1'],[]},
 {761,['ns_1@127.0.0.1'],[]},
 {762,['ns_1@127.0.0.1'],[]},
 {763,['ns_1@127.0.0.1'],[]},
 {764,['ns_1@127.0.0.1'],[]},
 {765,['ns_1@127.0.0.1'],[]},
 {766,['ns_1@127.0.0.1'],[]},
 {767,['ns_1@127.0.0.1'],[]},
 {768,['ns_1@127.0.0.1'],[]},
 {769,['ns_1@127.0.0.1'],[]},
 {770,['ns_1@127.0.0.1'],[]},
 {771,['ns_1@127.0.0.1'],[]},
 {772,['ns_1@127.0.0.1'],[]},
 {773,['ns_1@127.0.0.1'],[]},
 {774,['ns_1@127.0.0.1'],[]},
 {775,['ns_1@127.0.0.1'],[]},
 {776,['ns_1@127.0.0.1'],[]},
 {777,['ns_1@127.0.0.1'],[]},
 {778,['ns_1@127.0.0.1'],[]},
 {779,['ns_1@127.0.0.1'],[]},
 {780,['ns_1@127.0.0.1'],[]},
 {781,['ns_1@127.0.0.1'],[]},
 {782,['ns_1@127.0.0.1'],[]},
 {783,['ns_1@127.0.0.1'],[]},
 {784,['ns_1@127.0.0.1'],[]},
 {785,['ns_1@127.0.0.1'],[]},
 {786,['ns_1@127.0.0.1'],[]},
 {787,['ns_1@127.0.0.1'],[]},
 {788,['ns_1@127.0.0.1'],[]},
 {789,['ns_1@127.0.0.1'],[]},
 {790,['ns_1@127.0.0.1'],[]},
 {791,['ns_1@127.0.0.1'],[]},
 {792,['ns_1@127.0.0.1'],[]},
 {793,['ns_1@127.0.0.1'],[]},
 {794,['ns_1@127.0.0.1'],[]},
 {795,['ns_1@127.0.0.1'],[]},
 {796,['ns_1@127.0.0.1'],[]},
 {797,['ns_1@127.0.0.1'],[]},
 {798,['ns_1@127.0.0.1'],[]},
 {799,['ns_1@127.0.0.1'],[]},
 {800,['ns_1@127.0.0.1'],[]},
 {801,['ns_1@127.0.0.1'],[]},
 {802,['ns_1@127.0.0.1'],[]},
 {803,['ns_1@127.0.0.1'],[]},
 {804,['ns_1@127.0.0.1'],[]},
 {805,['ns_1@127.0.0.1'],[]},
 {806,['ns_1@127.0.0.1'],[]},
 {807,['ns_1@127.0.0.1'],[]},
 {808,['ns_1@127.0.0.1'],[]},
 {809,['ns_1@127.0.0.1'],[]},
 {810,['ns_1@127.0.0.1'],[]},
 {811,['ns_1@127.0.0.1'],[]},
 {812,['ns_1@127.0.0.1'],[]},
 {813,['ns_1@127.0.0.1'],[]},
 {814,['ns_1@127.0.0.1'],[]},
 {815,['ns_1@127.0.0.1'],[]},
 {816,['ns_1@127.0.0.1'],[]},
 {817,['ns_1@127.0.0.1'],[]},
 {818,['ns_1@127.0.0.1'],[]},
 {819,['ns_1@127.0.0.1'],[]},
 {820,['ns_1@127.0.0.1'],[]},
 {821,['ns_1@127.0.0.1'],[]},
 {822,['ns_1@127.0.0.1'],[]},
 {823,['ns_1@127.0.0.1'],[]},
 {824,['ns_1@127.0.0.1'],[]},
 {825,['ns_1@127.0.0.1'],[]},
 {826,['ns_1@127.0.0.1'],[]},
 {827,['ns_1@127.0.0.1'],[]},
 {828,['ns_1@127.0.0.1'],[]},
 {829,['ns_1@127.0.0.1'],[]},
 {830,['ns_1@127.0.0.1'],[]},
 {831,['ns_1@127.0.0.1'],[]},
 {832,['ns_1@127.0.0.1'],[]},
 {833,['ns_1@127.0.0.1'],[]},
 {834,['ns_1@127.0.0.1'],[]},
 {835,['ns_1@127.0.0.1'],[]},
 {836,['ns_1@127.0.0.1'],[]},
 {837,['ns_1@127.0.0.1'],[]},
 {838,['ns_1@127.0.0.1'],[]},
 {839,['ns_1@127.0.0.1'],[]},
 {840,['ns_1@127.0.0.1'],[]},
 {841,['ns_1@127.0.0.1'],[]},
 {842,['ns_1@127.0.0.1'],[]},
 {843,['ns_1@127.0.0.1'],[]},
 {844,['ns_1@127.0.0.1'],[]},
 {845,['ns_1@127.0.0.1'],[]},
 {846,['ns_1@127.0.0.1'],[]},
 {847,['ns_1@127.0.0.1'],[]},
 {848,['ns_1@127.0.0.1'],[]},
 {849,['ns_1@127.0.0.1'],[]},
 {850,['ns_1@127.0.0.1'],[]},
 {851,['ns_1@127.0.0.1'],[]},
 {852,['ns_1@127.0.0.1'],[]},
 {853,['ns_1@127.0.0.1'],[]},
 {854,['ns_1@127.0.0.1'],[]},
 {855,['ns_1@127.0.0.1'],[]},
 {856,['ns_1@127.0.0.1'],[]},
 {857,['ns_1@127.0.0.1'],[]},
 {858,['ns_1@127.0.0.1'],[]},
 {859,['ns_1@127.0.0.1'],[]},
 {860,['ns_1@127.0.0.1'],[]},
 {861,['ns_1@127.0.0.1'],[]},
 {862,['ns_1@127.0.0.1'],[]},
 {863,['ns_1@127.0.0.1'],[]},
 {864,['ns_1@127.0.0.1'],[]},
 {865,['ns_1@127.0.0.1'],[]},
 {866,['ns_1@127.0.0.1'],[]},
 {867,['ns_1@127.0.0.1'],[]},
 {868,['ns_1@127.0.0.1'],[]},
 {869,['ns_1@127.0.0.1'],[]},
 {870,['ns_1@127.0.0.1'],[]},
 {871,['ns_1@127.0.0.1'],[]},
 {872,['ns_1@127.0.0.1'],[]},
 {873,['ns_1@127.0.0.1'],[]},
 {874,['ns_1@127.0.0.1'],[]},
 {875,['ns_1@127.0.0.1'],[]},
 {876,['ns_1@127.0.0.1'],[]},
 {877,['ns_1@127.0.0.1'],[]},
 {878,['ns_1@127.0.0.1'],[]},
 {879,['ns_1@127.0.0.1'],[]},
 {880,['ns_1@127.0.0.1'],[]},
 {881,['ns_1@127.0.0.1'],[]},
 {882,['ns_1@127.0.0.1'],[]},
 {883,['ns_1@127.0.0.1'],[]},
 {884,['ns_1@127.0.0.1'],[]},
 {885,['ns_1@127.0.0.1'],[]},
 {886,['ns_1@127.0.0.1'],[]},
 {887,['ns_1@127.0.0.1'],[]},
 {888,['ns_1@127.0.0.1'],[]},
 {889,['ns_1@127.0.0.1'],[]},
 {890,['ns_1@127.0.0.1'],[]},
 {891,['ns_1@127.0.0.1'],[]},
 {892,['ns_1@127.0.0.1'],[]},
 {893,['ns_1@127.0.0.1'],[]},
 {894,['ns_1@127.0.0.1'],[]},
 {895,['ns_1@127.0.0.1'],[]},
 {896,['ns_1@127.0.0.1'],[]},
 {897,['ns_1@127.0.0.1'],[]},
 {898,['ns_1@127.0.0.1'],[]},
 {899,['ns_1@127.0.0.1'],[]},
 {900,['ns_1@127.0.0.1'],[]},
 {901,['ns_1@127.0.0.1'],[]},
 {902,['ns_1@127.0.0.1'],[]},
 {903,['ns_1@127.0.0.1'],[]},
 {904,['ns_1@127.0.0.1'],[]},
 {905,['ns_1@127.0.0.1'],[]},
 {906,['ns_1@127.0.0.1'],[]},
 {907,['ns_1@127.0.0.1'],[]},
 {908,['ns_1@127.0.0.1'],[]},
 {909,['ns_1@127.0.0.1'],[]},
 {910,['ns_1@127.0.0.1'],[]},
 {911,['ns_1@127.0.0.1'],[]},
 {912,['ns_1@127.0.0.1'],[]},
 {913,['ns_1@127.0.0.1'],[]},
 {914,['ns_1@127.0.0.1'],[]},
 {915,['ns_1@127.0.0.1'],[]},
 {916,['ns_1@127.0.0.1'],[]},
 {917,['ns_1@127.0.0.1'],[]},
 {918,['ns_1@127.0.0.1'],[]},
 {919,['ns_1@127.0.0.1'],[]},
 {920,['ns_1@127.0.0.1'],[]},
 {921,['ns_1@127.0.0.1'],[]},
 {922,['ns_1@127.0.0.1'],[]},
 {923,['ns_1@127.0.0.1'],[]},
 {924,['ns_1@127.0.0.1'],[]},
 {925,['ns_1@127.0.0.1'],[]},
 {926,['ns_1@127.0.0.1'],[]},
 {927,['ns_1@127.0.0.1'],[]},
 {928,['ns_1@127.0.0.1'],[]},
 {929,['ns_1@127.0.0.1'],[]},
 {930,['ns_1@127.0.0.1'],[]},
 {931,['ns_1@127.0.0.1'],[]},
 {932,['ns_1@127.0.0.1'],[]},
 {933,['ns_1@127.0.0.1'],[]},
 {934,['ns_1@127.0.0.1'],[]},
 {935,['ns_1@127.0.0.1'],[]},
 {936,['ns_1@127.0.0.1'],[]},
 {937,['ns_1@127.0.0.1'],[]},
 {938,['ns_1@127.0.0.1'],[]},
 {939,['ns_1@127.0.0.1'],[]},
 {940,['ns_1@127.0.0.1'],[]},
 {941,['ns_1@127.0.0.1'],[]},
 {942,['ns_1@127.0.0.1'],[]},
 {943,['ns_1@127.0.0.1'],[]},
 {944,['ns_1@127.0.0.1'],[]},
 {945,['ns_1@127.0.0.1'],[]},
 {946,['ns_1@127.0.0.1'],[]},
 {947,['ns_1@127.0.0.1'],[]},
 {948,['ns_1@127.0.0.1'],[]},
 {949,['ns_1@127.0.0.1'],[]},
 {950,['ns_1@127.0.0.1'],[]},
 {951,['ns_1@127.0.0.1'],[]},
 {952,['ns_1@127.0.0.1'],[]},
 {953,['ns_1@127.0.0.1'],[]},
 {954,['ns_1@127.0.0.1'],[]},
 {955,['ns_1@127.0.0.1'],[]},
 {956,['ns_1@127.0.0.1'],[]},
 {957,['ns_1@127.0.0.1'],[]},
 {958,['ns_1@127.0.0.1'],[]},
 {959,['ns_1@127.0.0.1'],[]},
 {960,['ns_1@127.0.0.1'],[]},
 {961,['ns_1@127.0.0.1'],[]},
 {962,['ns_1@127.0.0.1'],[]},
 {963,['ns_1@127.0.0.1'],[]},
 {964,['ns_1@127.0.0.1'],[]},
 {965,['ns_1@127.0.0.1'],[]},
 {966,['ns_1@127.0.0.1'],[]},
 {967,['ns_1@127.0.0.1'],[]},
 {968,['ns_1@127.0.0.1'],[]},
 {969,['ns_1@127.0.0.1'],[]},
 {970,['ns_1@127.0.0.1'],[]},
 {971,['ns_1@127.0.0.1'],[]},
 {972,['ns_1@127.0.0.1'],[]},
 {973,['ns_1@127.0.0.1'],[]},
 {974,['ns_1@127.0.0.1'],[]},
 {975,['ns_1@127.0.0.1'],[]},
 {976,['ns_1@127.0.0.1'],[]},
 {977,['ns_1@127.0.0.1'],[]},
 {978,['ns_1@127.0.0.1'],[]},
 {979,['ns_1@127.0.0.1'],[]},
 {980,['ns_1@127.0.0.1'],[]},
 {981,['ns_1@127.0.0.1'],[]},
 {982,['ns_1@127.0.0.1'],[]},
 {983,['ns_1@127.0.0.1'],[]},
 {984,['ns_1@127.0.0.1'],[]},
 {985,['ns_1@127.0.0.1'],[]},
 {986,['ns_1@127.0.0.1'],[]},
 {987,['ns_1@127.0.0.1'],[]},
 {988,['ns_1@127.0.0.1'],[]},
 {989,['ns_1@127.0.0.1'],[]},
 {990,['ns_1@127.0.0.1'],[]},
 {991,['ns_1@127.0.0.1'],[]},
 {992,['ns_1@127.0.0.1'],[]},
 {993,['ns_1@127.0.0.1'],[]},
 {994,['ns_1@127.0.0.1'],[]},
 {995,['ns_1@127.0.0.1'],[]},
 {996,['ns_1@127.0.0.1'],[]},
 {997,['ns_1@127.0.0.1'],[]},
 {998,['ns_1@127.0.0.1'],[]},
 {999,['ns_1@127.0.0.1'],[]},
 {1000,['ns_1@127.0.0.1'],[]},
 {1001,['ns_1@127.0.0.1'],[]},
 {1002,['ns_1@127.0.0.1'],[]},
 {1003,['ns_1@127.0.0.1'],[]},
 {1004,['ns_1@127.0.0.1'],[]},
 {1005,['ns_1@127.0.0.1'],[]},
 {1006,['ns_1@127.0.0.1'],[]},
 {1007,['ns_1@127.0.0.1'],[]},
 {1008,['ns_1@127.0.0.1'],[]},
 {1009,['ns_1@127.0.0.1'],[]},
 {1010,['ns_1@127.0.0.1'],[]},
 {1011,['ns_1@127.0.0.1'],[]},
 {1012,['ns_1@127.0.0.1'],[]},
 {1013,['ns_1@127.0.0.1'],[]},
 {1014,['ns_1@127.0.0.1'],[]},
 {1015,['ns_1@127.0.0.1'],[]},
 {1016,['ns_1@127.0.0.1'],[]},
 {1017,['ns_1@127.0.0.1'],[]},
 {1018,['ns_1@127.0.0.1'],[]},
 {1019,['ns_1@127.0.0.1'],[]},
 {1020,['ns_1@127.0.0.1'],[]},
 {1021,['ns_1@127.0.0.1'],[]},
 {1022,['ns_1@127.0.0.1'],[]},
 {1023,['ns_1@127.0.0.1'],[]}]
[ns_server:debug,2020-06-16T21:33:49.786Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:config_sync:259]Going to pull config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:info,2020-06-16T21:33:49.799Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 0 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 2 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 3 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 4 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 5 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.373.0>:ns_heart:grab_latest_stats:264]Ignoring failure to grab "platzi" stats:
{error,no_samples}

[ns_server:info,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 6 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 7 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 8 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.800Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 9 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 10 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 11 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 12 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 13 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 14 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 15 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 16 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 17 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 18 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 19 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.801Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 20 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 21 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 22 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 23 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 24 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 25 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 26 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 27 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 28 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 29 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 30 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 31 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 32 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 33 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 34 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.802Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 35 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 36 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 37 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 38 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 39 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 40 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 41 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 42 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 43 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 44 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 45 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 46 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 47 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.803Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 48 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.804Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 49 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.804Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 50 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.804Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 51 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.805Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 52 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.805Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 53 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.805Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 54 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.805Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 55 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.805Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 56 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.805Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 57 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.805Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 58 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 59 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 60 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 61 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 62 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 63 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 64 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 65 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 66 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 67 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 68 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 69 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 70 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 71 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.806Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 72 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 73 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 74 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 75 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 76 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 77 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 78 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 79 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 80 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 81 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 82 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 83 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 84 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.807Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 85 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 86 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 87 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 88 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 89 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 90 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 91 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 92 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 93 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 94 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 95 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 96 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 97 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 98 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 99 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 100 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 101 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 102 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.808Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 103 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 104 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 105 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 106 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 107 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 108 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 109 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 110 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 111 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 112 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 113 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 114 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 115 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 116 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 117 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 118 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 119 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.809Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 120 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 121 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 122 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 123 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 124 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 125 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 126 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 127 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 128 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 129 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 130 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 131 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 132 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 133 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 134 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.810Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 135 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.811Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 136 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.811Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 137 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.811Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 138 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.811Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 139 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.811Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 140 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.811Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 141 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.812Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 142 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.812Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 143 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.812Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 144 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.812Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 145 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.812Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 146 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.813Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 147 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.813Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 148 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.813Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 149 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.813Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 150 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.813Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 151 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.813Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 152 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.813Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 153 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.813Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 154 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.813Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 155 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.814Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 156 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.814Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 157 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.814Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 158 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.814Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 159 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.814Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 160 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.815Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 161 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.815Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 162 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.815Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 163 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 164 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 165 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 166 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 167 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 168 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 169 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 170 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 171 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 172 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 173 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 174 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 175 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.816Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 176 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.817Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 177 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.817Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 178 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.817Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 179 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.817Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 180 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.817Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 181 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.817Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 182 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.817Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 183 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.818Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 184 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.818Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 185 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.819Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 186 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.820Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 187 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.827Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 188 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.827Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 189 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.827Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 190 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.827Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 191 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.827Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 192 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.827Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 193 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.828Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 194 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.828Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 195 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.828Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 196 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.829Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 197 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.829Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 198 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.830Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 199 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.830Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 200 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.830Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 201 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.830Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 202 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.830Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 203 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.830Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 204 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.831Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 205 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.831Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 206 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.831Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 207 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.831Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 208 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.831Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 209 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.832Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 210 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.832Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 211 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.832Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 212 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.832Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 213 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.832Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 214 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.833Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 215 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.833Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 216 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.833Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 217 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.833Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 218 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.834Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 219 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.834Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 220 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.834Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 221 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.834Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 222 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.836Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 223 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.836Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 224 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.836Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 225 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.836Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 226 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.836Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 227 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.836Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 228 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.837Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 229 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.837Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 230 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.837Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 231 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.837Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 232 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.838Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 233 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.838Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 234 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.838Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 235 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.838Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 236 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.838Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 237 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.838Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 238 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.838Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 239 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.838Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 240 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.844Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 241 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.844Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 242 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.845Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 243 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.845Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 244 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.845Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 245 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.845Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 246 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.845Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 247 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.846Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 248 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.846Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 249 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.846Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 250 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.846Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 251 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.846Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 252 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.846Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 253 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.846Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 254 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.847Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 255 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.847Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 256 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.847Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 257 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.847Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 258 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.847Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 259 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.847Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 260 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.847Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 261 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.847Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 262 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.848Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 263 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.848Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 264 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.848Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 265 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.848Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 266 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.848Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 267 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.848Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 268 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.848Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 269 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.848Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 270 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 271 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 272 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 273 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 274 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 275 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 276 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 277 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 278 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 279 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 280 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 281 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.849Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 282 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 283 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 284 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 285 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 286 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 287 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 288 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 289 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 290 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 291 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 292 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 293 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.850Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 294 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 295 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 296 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 297 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 298 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 299 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 300 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 301 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 302 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 303 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 304 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 305 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.851Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 306 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.853Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 307 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.853Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 308 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.853Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 309 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.853Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 310 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.853Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 311 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.855Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 312 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.855Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 313 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.855Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 314 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.855Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 315 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.855Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 316 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.855Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 317 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.855Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 318 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.855Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 319 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.856Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 320 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.856Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 321 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.856Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 322 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.856Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 323 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.856Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 324 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.856Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 325 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.856Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 326 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 327 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 328 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 329 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 330 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 331 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 332 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 333 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 334 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 335 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 336 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 337 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.860Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 338 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.861Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 339 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.861Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 340 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.861Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 341 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.861Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 342 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.861Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 343 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 344 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 345 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 346 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 347 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 348 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 349 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 350 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 351 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 352 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 353 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 354 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 355 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.862Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 356 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 357 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 358 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 359 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 360 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 361 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 362 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 363 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 364 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 365 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 366 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.863Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 367 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 368 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 369 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 370 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 371 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 372 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 373 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 374 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 375 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 376 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 377 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 378 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 379 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.864Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 380 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 381 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 382 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 383 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 384 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 385 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 386 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 387 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 388 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 389 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 390 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.865Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 391 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 392 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 393 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 394 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 395 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 396 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 397 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 398 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 399 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 400 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 401 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.866Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 402 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 403 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 404 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 405 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 406 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 407 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 408 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 409 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 410 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 411 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.867Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 412 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 413 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 414 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 415 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 416 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 417 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 418 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 419 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 420 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 421 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 422 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.868Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 423 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.869Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 424 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.869Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 425 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.869Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 426 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.869Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 427 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.869Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 428 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.869Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 429 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.869Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 430 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.869Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 431 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.869Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 432 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 433 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 434 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 435 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 436 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 437 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 438 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 439 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 440 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 441 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 442 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 443 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 444 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.870Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 445 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.871Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 446 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.871Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 447 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.871Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 448 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.871Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 449 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.871Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 450 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.872Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 451 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.873Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 452 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.873Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 453 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.873Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 454 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.873Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 455 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.873Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 456 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.873Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 457 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.874Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 458 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.874Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 459 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.874Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 460 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.874Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 461 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.874Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 462 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.874Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 463 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.874Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 464 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.875Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 465 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.875Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 466 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 467 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 468 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 469 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 470 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 471 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 472 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 473 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 474 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 475 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.876Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 476 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.877Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 477 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.877Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 478 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.877Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 479 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.877Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 480 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.877Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 481 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.878Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 482 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.881Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 483 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.881Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 484 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.881Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 485 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.881Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 486 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.882Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 487 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.882Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 488 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.882Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 489 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.882Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 490 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.882Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 491 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.882Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 492 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.882Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 493 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 494 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 495 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 496 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 497 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 498 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 499 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 500 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 501 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 502 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.883Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 503 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 504 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 505 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 506 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 507 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 508 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 509 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 510 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 511 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 512 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 513 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 514 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 515 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 516 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 517 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.884Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 518 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.885Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 519 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.885Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 520 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.885Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 521 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.885Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 522 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.885Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 523 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.885Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 524 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.885Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 525 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.885Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 526 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.885Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 527 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 528 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 529 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 530 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 531 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 532 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 533 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 534 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 535 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 536 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 537 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 538 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 539 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 540 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 541 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 542 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 543 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.886Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 544 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 545 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 546 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 547 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 548 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 549 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 550 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 551 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 552 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 553 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 554 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 555 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 556 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 557 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 558 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.887Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 559 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 560 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 561 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 562 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 563 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 564 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 565 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 566 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 567 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 568 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 569 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 570 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 571 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 572 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.888Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 573 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.889Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 574 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.889Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 575 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.889Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 576 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.895Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 577 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.896Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 578 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.897Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 579 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.897Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 580 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.897Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 581 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.897Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 582 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.898Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 583 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.898Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 584 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.898Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 585 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.898Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 586 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.898Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 587 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.899Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 588 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.899Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 589 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.899Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 590 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.899Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 591 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.899Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 592 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.900Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 593 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.900Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 594 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.900Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 595 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.900Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 596 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.900Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 597 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.901Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 598 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.901Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 599 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.901Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 600 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.901Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 601 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.901Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 602 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.904Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 603 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.905Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 604 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.905Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 605 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.905Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 606 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.905Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 607 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.906Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 608 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.906Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 609 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.906Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 610 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.906Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 611 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.906Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 612 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.906Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 613 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.906Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 614 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.906Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 615 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.911Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 616 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.911Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 617 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.911Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 618 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.911Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 619 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.911Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 620 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.911Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 621 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.911Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 622 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 623 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 624 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 625 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 626 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 627 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 628 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 629 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 630 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 631 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 632 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 633 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 634 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 635 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 636 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 637 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 638 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.912Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 639 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 640 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 641 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 642 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 643 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 644 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 645 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 646 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 647 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 648 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 649 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 650 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 651 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 652 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 653 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 654 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.913Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 655 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 656 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 657 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 658 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 659 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 660 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 661 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 662 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 663 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 664 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 665 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 666 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 667 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 668 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 669 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.914Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 670 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 671 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 672 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 673 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 674 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 675 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 676 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 677 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 678 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 679 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 680 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 681 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 682 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 683 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 684 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.915Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 685 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.918Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 686 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.921Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 687 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.921Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 688 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.921Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 689 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.921Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 690 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.921Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 691 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.923Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 692 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.923Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 693 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.924Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 694 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.924Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 695 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.924Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 696 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.926Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 697 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.926Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 698 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.926Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 699 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.927Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 700 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.927Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 701 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.927Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 702 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.928Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 703 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.938Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 704 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 705 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 706 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 707 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 708 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 709 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 710 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 711 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 712 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 713 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 714 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 715 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.939Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 716 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.940Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 717 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.940Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 718 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.940Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 719 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.940Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 720 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.940Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 721 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.940Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 722 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.940Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 723 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.940Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 724 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 725 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 726 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 727 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 728 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 729 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 730 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 731 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 732 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 733 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 734 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 735 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.941Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 736 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.942Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 737 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.942Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 738 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.947Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 739 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.963Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 740 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.963Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 741 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.963Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 742 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.964Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 743 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.964Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 744 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.964Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 745 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.964Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 746 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.964Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 747 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.964Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 748 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.966Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 749 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.966Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 750 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.966Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 751 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.966Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 752 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.966Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 753 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.967Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 754 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.975Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 755 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.975Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 756 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.976Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 757 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.976Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 758 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.976Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 759 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.976Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 760 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.976Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 761 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.976Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 762 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.976Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 763 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 764 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 765 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 766 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 767 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 768 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 769 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 770 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 771 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 772 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.977Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 773 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 774 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 775 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 776 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 777 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 778 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 779 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 780 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 781 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 782 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 783 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.978Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 784 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 785 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 786 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 787 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 788 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 789 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 790 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 791 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 792 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 793 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 794 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 795 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.979Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 796 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.980Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 797 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.980Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 798 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.980Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 799 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.980Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 800 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.980Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 801 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.986Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 802 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.986Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 803 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.986Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 804 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.986Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 805 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.986Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 806 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.987Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 807 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.987Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 808 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.987Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 809 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.987Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 810 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.988Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 811 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.988Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 812 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.989Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 813 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.991Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 814 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.993Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 815 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.993Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 816 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.993Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 817 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.993Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 818 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.993Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 819 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.994Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 820 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 821 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 822 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 823 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 824 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 825 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 826 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 827 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 828 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 829 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 830 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 831 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 832 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 833 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.997Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 834 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 835 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 836 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 837 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 838 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 839 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 840 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 841 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 842 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 843 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 844 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 845 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 846 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 847 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 848 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 849 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.998Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 850 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 851 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 852 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 853 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 854 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 855 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 856 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 857 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 858 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 859 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 860 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 861 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 862 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 863 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 864 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 865 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 866 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 867 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:49.999Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 868 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 869 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 870 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 871 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 872 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 873 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 874 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 875 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 876 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 877 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 878 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 879 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 880 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 881 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 882 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 883 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 884 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.000Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 885 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 886 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 887 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 888 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 889 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 890 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 891 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 892 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 893 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 894 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 895 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 896 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 897 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 898 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 899 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 900 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.001Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 901 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 902 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 903 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 904 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 905 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 906 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 907 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 908 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 909 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 910 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 911 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 912 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 913 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 914 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 915 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 916 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 917 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 918 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 919 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.002Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 920 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 921 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 922 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 923 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 924 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 925 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 926 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 927 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 928 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 929 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 930 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 931 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.003Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 932 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 933 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 934 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 935 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 936 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 937 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 938 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 939 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 940 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 941 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 942 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 943 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 944 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 945 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 946 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 947 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 948 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.011Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 949 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 950 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 951 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 952 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 953 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 954 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 955 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 956 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 957 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 958 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 959 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 960 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 961 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 962 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 963 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 964 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 965 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.012Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 966 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 967 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 968 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 969 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 970 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 971 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 972 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 973 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 974 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 975 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 976 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 977 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 978 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 979 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.013Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 980 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.019Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 981 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 982 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 983 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 984 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 985 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 986 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 987 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 988 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 989 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 990 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 991 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 992 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.020Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 993 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 994 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 995 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 996 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 997 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 998 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 999 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1000 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1001 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1002 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1003 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1004 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1005 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1006 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1007 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1008 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1009 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1010 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.021Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1011 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1012 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1013 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1014 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1015 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1016 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1017 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1018 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1019 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1020 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1021 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1022 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2020-06-16T21:33:50.022Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:sanify_chain:499]Setting vbucket 1023 in "platzi" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2020-06-16T21:33:50.030Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:maybe_config_sync:242]Found states mismatch in bucket "platzi":
[{0,['ns_1@127.0.0.1'],[]},
 {1,['ns_1@127.0.0.1'],[]},
 {2,['ns_1@127.0.0.1'],[]},
 {3,['ns_1@127.0.0.1'],[]},
 {4,['ns_1@127.0.0.1'],[]},
 {5,['ns_1@127.0.0.1'],[]},
 {6,['ns_1@127.0.0.1'],[]},
 {7,['ns_1@127.0.0.1'],[]},
 {8,['ns_1@127.0.0.1'],[]},
 {9,['ns_1@127.0.0.1'],[]},
 {10,['ns_1@127.0.0.1'],[]},
 {11,['ns_1@127.0.0.1'],[]},
 {12,['ns_1@127.0.0.1'],[]},
 {13,['ns_1@127.0.0.1'],[]},
 {14,['ns_1@127.0.0.1'],[]},
 {15,['ns_1@127.0.0.1'],[]},
 {16,['ns_1@127.0.0.1'],[]},
 {17,['ns_1@127.0.0.1'],[]},
 {18,['ns_1@127.0.0.1'],[]},
 {19,['ns_1@127.0.0.1'],[]},
 {20,['ns_1@127.0.0.1'],[]},
 {21,['ns_1@127.0.0.1'],[]},
 {22,['ns_1@127.0.0.1'],[]},
 {23,['ns_1@127.0.0.1'],[]},
 {24,['ns_1@127.0.0.1'],[]},
 {25,['ns_1@127.0.0.1'],[]},
 {26,['ns_1@127.0.0.1'],[]},
 {27,['ns_1@127.0.0.1'],[]},
 {28,['ns_1@127.0.0.1'],[]},
 {29,['ns_1@127.0.0.1'],[]},
 {30,['ns_1@127.0.0.1'],[]},
 {31,['ns_1@127.0.0.1'],[]},
 {32,['ns_1@127.0.0.1'],[]},
 {33,['ns_1@127.0.0.1'],[]},
 {34,['ns_1@127.0.0.1'],[]},
 {35,['ns_1@127.0.0.1'],[]},
 {36,['ns_1@127.0.0.1'],[]},
 {37,['ns_1@127.0.0.1'],[]},
 {38,['ns_1@127.0.0.1'],[]},
 {39,['ns_1@127.0.0.1'],[]},
 {40,['ns_1@127.0.0.1'],[]},
 {41,['ns_1@127.0.0.1'],[]},
 {42,['ns_1@127.0.0.1'],[]},
 {43,['ns_1@127.0.0.1'],[]},
 {44,['ns_1@127.0.0.1'],[]},
 {45,['ns_1@127.0.0.1'],[]},
 {46,['ns_1@127.0.0.1'],[]},
 {47,['ns_1@127.0.0.1'],[]},
 {48,['ns_1@127.0.0.1'],[]},
 {49,['ns_1@127.0.0.1'],[]},
 {50,['ns_1@127.0.0.1'],[]},
 {51,['ns_1@127.0.0.1'],[]},
 {52,['ns_1@127.0.0.1'],[]},
 {53,['ns_1@127.0.0.1'],[]},
 {54,['ns_1@127.0.0.1'],[]},
 {55,['ns_1@127.0.0.1'],[]},
 {56,['ns_1@127.0.0.1'],[]},
 {57,['ns_1@127.0.0.1'],[]},
 {58,['ns_1@127.0.0.1'],[]},
 {59,['ns_1@127.0.0.1'],[]},
 {60,['ns_1@127.0.0.1'],[]},
 {61,['ns_1@127.0.0.1'],[]},
 {62,['ns_1@127.0.0.1'],[]},
 {63,['ns_1@127.0.0.1'],[]},
 {64,['ns_1@127.0.0.1'],[]},
 {65,['ns_1@127.0.0.1'],[]},
 {66,['ns_1@127.0.0.1'],[]},
 {67,['ns_1@127.0.0.1'],[]},
 {68,['ns_1@127.0.0.1'],[]},
 {69,['ns_1@127.0.0.1'],[]},
 {70,['ns_1@127.0.0.1'],[]},
 {71,['ns_1@127.0.0.1'],[]},
 {72,['ns_1@127.0.0.1'],[]},
 {73,['ns_1@127.0.0.1'],[]},
 {74,['ns_1@127.0.0.1'],[]},
 {75,['ns_1@127.0.0.1'],[]},
 {76,['ns_1@127.0.0.1'],[]},
 {77,['ns_1@127.0.0.1'],[]},
 {78,['ns_1@127.0.0.1'],[]},
 {79,['ns_1@127.0.0.1'],[]},
 {80,['ns_1@127.0.0.1'],[]},
 {81,['ns_1@127.0.0.1'],[]},
 {82,['ns_1@127.0.0.1'],[]},
 {83,['ns_1@127.0.0.1'],[]},
 {84,['ns_1@127.0.0.1'],[]},
 {85,['ns_1@127.0.0.1'],[]},
 {86,['ns_1@127.0.0.1'],[]},
 {87,['ns_1@127.0.0.1'],[]},
 {88,['ns_1@127.0.0.1'],[]},
 {89,['ns_1@127.0.0.1'],[]},
 {90,['ns_1@127.0.0.1'],[]},
 {91,['ns_1@127.0.0.1'],[]},
 {92,['ns_1@127.0.0.1'],[]},
 {93,['ns_1@127.0.0.1'],[]},
 {94,['ns_1@127.0.0.1'],[]},
 {95,['ns_1@127.0.0.1'],[]},
 {96,['ns_1@127.0.0.1'],[]},
 {97,['ns_1@127.0.0.1'],[]},
 {98,['ns_1@127.0.0.1'],[]},
 {99,['ns_1@127.0.0.1'],[]},
 {100,['ns_1@127.0.0.1'],[]},
 {101,['ns_1@127.0.0.1'],[]},
 {102,['ns_1@127.0.0.1'],[]},
 {103,['ns_1@127.0.0.1'],[]},
 {104,['ns_1@127.0.0.1'],[]},
 {105,['ns_1@127.0.0.1'],[]},
 {106,['ns_1@127.0.0.1'],[]},
 {107,['ns_1@127.0.0.1'],[]},
 {108,['ns_1@127.0.0.1'],[]},
 {109,['ns_1@127.0.0.1'],[]},
 {110,['ns_1@127.0.0.1'],[]},
 {111,['ns_1@127.0.0.1'],[]},
 {112,['ns_1@127.0.0.1'],[]},
 {113,['ns_1@127.0.0.1'],[]},
 {114,['ns_1@127.0.0.1'],[]},
 {115,['ns_1@127.0.0.1'],[]},
 {116,['ns_1@127.0.0.1'],[]},
 {117,['ns_1@127.0.0.1'],[]},
 {118,['ns_1@127.0.0.1'],[]},
 {119,['ns_1@127.0.0.1'],[]},
 {120,['ns_1@127.0.0.1'],[]},
 {121,['ns_1@127.0.0.1'],[]},
 {122,['ns_1@127.0.0.1'],[]},
 {123,['ns_1@127.0.0.1'],[]},
 {124,['ns_1@127.0.0.1'],[]},
 {125,['ns_1@127.0.0.1'],[]},
 {126,['ns_1@127.0.0.1'],[]},
 {127,['ns_1@127.0.0.1'],[]},
 {128,['ns_1@127.0.0.1'],[]},
 {129,['ns_1@127.0.0.1'],[]},
 {130,['ns_1@127.0.0.1'],[]},
 {131,['ns_1@127.0.0.1'],[]},
 {132,['ns_1@127.0.0.1'],[]},
 {133,['ns_1@127.0.0.1'],[]},
 {134,['ns_1@127.0.0.1'],[]},
 {135,['ns_1@127.0.0.1'],[]},
 {136,['ns_1@127.0.0.1'],[]},
 {137,['ns_1@127.0.0.1'],[]},
 {138,['ns_1@127.0.0.1'],[]},
 {139,['ns_1@127.0.0.1'],[]},
 {140,['ns_1@127.0.0.1'],[]},
 {141,['ns_1@127.0.0.1'],[]},
 {142,['ns_1@127.0.0.1'],[]},
 {143,['ns_1@127.0.0.1'],[]},
 {144,['ns_1@127.0.0.1'],[]},
 {145,['ns_1@127.0.0.1'],[]},
 {146,['ns_1@127.0.0.1'],[]},
 {147,['ns_1@127.0.0.1'],[]},
 {148,['ns_1@127.0.0.1'],[]},
 {149,['ns_1@127.0.0.1'],[]},
 {150,['ns_1@127.0.0.1'],[]},
 {151,['ns_1@127.0.0.1'],[]},
 {152,['ns_1@127.0.0.1'],[]},
 {153,['ns_1@127.0.0.1'],[]},
 {154,['ns_1@127.0.0.1'],[]},
 {155,['ns_1@127.0.0.1'],[]},
 {156,['ns_1@127.0.0.1'],[]},
 {157,['ns_1@127.0.0.1'],[]},
 {158,['ns_1@127.0.0.1'],[]},
 {159,['ns_1@127.0.0.1'],[]},
 {160,['ns_1@127.0.0.1'],[]},
 {161,['ns_1@127.0.0.1'],[]},
 {162,['ns_1@127.0.0.1'],[]},
 {163,['ns_1@127.0.0.1'],[]},
 {164,['ns_1@127.0.0.1'],[]},
 {165,['ns_1@127.0.0.1'],[]},
 {166,['ns_1@127.0.0.1'],[]},
 {167,['ns_1@127.0.0.1'],[]},
 {168,['ns_1@127.0.0.1'],[]},
 {169,['ns_1@127.0.0.1'],[]},
 {170,['ns_1@127.0.0.1'],[]},
 {171,['ns_1@127.0.0.1'],[]},
 {172,['ns_1@127.0.0.1'],[]},
 {173,['ns_1@127.0.0.1'],[]},
 {174,['ns_1@127.0.0.1'],[]},
 {175,['ns_1@127.0.0.1'],[]},
 {176,['ns_1@127.0.0.1'],[]},
 {177,['ns_1@127.0.0.1'],[]},
 {178,['ns_1@127.0.0.1'],[]},
 {179,['ns_1@127.0.0.1'],[]},
 {180,['ns_1@127.0.0.1'],[]},
 {181,['ns_1@127.0.0.1'],[]},
 {182,['ns_1@127.0.0.1'],[]},
 {183,['ns_1@127.0.0.1'],[]},
 {184,['ns_1@127.0.0.1'],[]},
 {185,['ns_1@127.0.0.1'],[]},
 {186,['ns_1@127.0.0.1'],[]},
 {187,['ns_1@127.0.0.1'],[]},
 {188,['ns_1@127.0.0.1'],[]},
 {189,['ns_1@127.0.0.1'],[]},
 {190,['ns_1@127.0.0.1'],[]},
 {191,['ns_1@127.0.0.1'],[]},
 {192,['ns_1@127.0.0.1'],[]},
 {193,['ns_1@127.0.0.1'],[]},
 {194,['ns_1@127.0.0.1'],[]},
 {195,['ns_1@127.0.0.1'],[]},
 {196,['ns_1@127.0.0.1'],[]},
 {197,['ns_1@127.0.0.1'],[]},
 {198,['ns_1@127.0.0.1'],[]},
 {199,['ns_1@127.0.0.1'],[]},
 {200,['ns_1@127.0.0.1'],[]},
 {201,['ns_1@127.0.0.1'],[]},
 {202,['ns_1@127.0.0.1'],[]},
 {203,['ns_1@127.0.0.1'],[]},
 {204,['ns_1@127.0.0.1'],[]},
 {205,['ns_1@127.0.0.1'],[]},
 {206,['ns_1@127.0.0.1'],[]},
 {207,['ns_1@127.0.0.1'],[]},
 {208,['ns_1@127.0.0.1'],[]},
 {209,['ns_1@127.0.0.1'],[]},
 {210,['ns_1@127.0.0.1'],[]},
 {211,['ns_1@127.0.0.1'],[]},
 {212,['ns_1@127.0.0.1'],[]},
 {213,['ns_1@127.0.0.1'],[]},
 {214,['ns_1@127.0.0.1'],[]},
 {215,['ns_1@127.0.0.1'],[]},
 {216,['ns_1@127.0.0.1'],[]},
 {217,['ns_1@127.0.0.1'],[]},
 {218,['ns_1@127.0.0.1'],[]},
 {219,['ns_1@127.0.0.1'],[]},
 {220,['ns_1@127.0.0.1'],[]},
 {221,['ns_1@127.0.0.1'],[]},
 {222,['ns_1@127.0.0.1'],[]},
 {223,['ns_1@127.0.0.1'],[]},
 {224,['ns_1@127.0.0.1'],[]},
 {225,['ns_1@127.0.0.1'],[]},
 {226,['ns_1@127.0.0.1'],[]},
 {227,['ns_1@127.0.0.1'],[]},
 {228,['ns_1@127.0.0.1'],[]},
 {229,['ns_1@127.0.0.1'],[]},
 {230,['ns_1@127.0.0.1'],[]},
 {231,['ns_1@127.0.0.1'],[]},
 {232,['ns_1@127.0.0.1'],[]},
 {233,['ns_1@127.0.0.1'],[]},
 {234,['ns_1@127.0.0.1'],[]},
 {235,['ns_1@127.0.0.1'],[]},
 {236,['ns_1@127.0.0.1'],[]},
 {237,['ns_1@127.0.0.1'],[]},
 {238,['ns_1@127.0.0.1'],[]},
 {239,['ns_1@127.0.0.1'],[]},
 {240,['ns_1@127.0.0.1'],[]},
 {241,['ns_1@127.0.0.1'],[]},
 {242,['ns_1@127.0.0.1'],[]},
 {243,['ns_1@127.0.0.1'],[]},
 {244,['ns_1@127.0.0.1'],[]},
 {245,['ns_1@127.0.0.1'],[]},
 {246,['ns_1@127.0.0.1'],[]},
 {247,['ns_1@127.0.0.1'],[]},
 {248,['ns_1@127.0.0.1'],[]},
 {249,['ns_1@127.0.0.1'],[]},
 {250,['ns_1@127.0.0.1'],[]},
 {251,['ns_1@127.0.0.1'],[]},
 {252,['ns_1@127.0.0.1'],[]},
 {253,['ns_1@127.0.0.1'],[]},
 {254,['ns_1@127.0.0.1'],[]},
 {255,['ns_1@127.0.0.1'],[]},
 {256,['ns_1@127.0.0.1'],[]},
 {257,['ns_1@127.0.0.1'],[]},
 {258,['ns_1@127.0.0.1'],[]},
 {259,['ns_1@127.0.0.1'],[]},
 {260,['ns_1@127.0.0.1'],[]},
 {261,['ns_1@127.0.0.1'],[]},
 {262,['ns_1@127.0.0.1'],[]},
 {263,['ns_1@127.0.0.1'],[]},
 {264,['ns_1@127.0.0.1'],[]},
 {265,['ns_1@127.0.0.1'],[]},
 {266,['ns_1@127.0.0.1'],[]},
 {267,['ns_1@127.0.0.1'],[]},
 {268,['ns_1@127.0.0.1'],[]},
 {269,['ns_1@127.0.0.1'],[]},
 {270,['ns_1@127.0.0.1'],[]},
 {271,['ns_1@127.0.0.1'],[]},
 {272,['ns_1@127.0.0.1'],[]},
 {273,['ns_1@127.0.0.1'],[]},
 {274,['ns_1@127.0.0.1'],[]},
 {275,['ns_1@127.0.0.1'],[]},
 {276,['ns_1@127.0.0.1'],[]},
 {277,['ns_1@127.0.0.1'],[]},
 {278,['ns_1@127.0.0.1'],[]},
 {279,['ns_1@127.0.0.1'],[]},
 {280,['ns_1@127.0.0.1'],[]},
 {281,['ns_1@127.0.0.1'],[]},
 {282,['ns_1@127.0.0.1'],[]},
 {283,['ns_1@127.0.0.1'],[]},
 {284,['ns_1@127.0.0.1'],[]},
 {285,['ns_1@127.0.0.1'],[]},
 {286,['ns_1@127.0.0.1'],[]},
 {287,['ns_1@127.0.0.1'],[]},
 {288,['ns_1@127.0.0.1'],[]},
 {289,['ns_1@127.0.0.1'],[]},
 {290,['ns_1@127.0.0.1'],[]},
 {291,['ns_1@127.0.0.1'],[]},
 {292,['ns_1@127.0.0.1'],[]},
 {293,['ns_1@127.0.0.1'],[]},
 {294,['ns_1@127.0.0.1'],[]},
 {295,['ns_1@127.0.0.1'],[]},
 {296,['ns_1@127.0.0.1'],[]},
 {297,['ns_1@127.0.0.1'],[]},
 {298,['ns_1@127.0.0.1'],[]},
 {299,['ns_1@127.0.0.1'],[]},
 {300,['ns_1@127.0.0.1'],[]},
 {301,['ns_1@127.0.0.1'],[]},
 {302,['ns_1@127.0.0.1'],[]},
 {303,['ns_1@127.0.0.1'],[]},
 {304,['ns_1@127.0.0.1'],[]},
 {305,['ns_1@127.0.0.1'],[]},
 {306,['ns_1@127.0.0.1'],[]},
 {307,['ns_1@127.0.0.1'],[]},
 {308,['ns_1@127.0.0.1'],[]},
 {309,['ns_1@127.0.0.1'],[]},
 {310,['ns_1@127.0.0.1'],[]},
 {311,['ns_1@127.0.0.1'],[]},
 {312,['ns_1@127.0.0.1'],[]},
 {313,['ns_1@127.0.0.1'],[]},
 {314,['ns_1@127.0.0.1'],[]},
 {315,['ns_1@127.0.0.1'],[]},
 {316,['ns_1@127.0.0.1'],[]},
 {317,['ns_1@127.0.0.1'],[]},
 {318,['ns_1@127.0.0.1'],[]},
 {319,['ns_1@127.0.0.1'],[]},
 {320,['ns_1@127.0.0.1'],[]},
 {321,['ns_1@127.0.0.1'],[]},
 {322,['ns_1@127.0.0.1'],[]},
 {323,['ns_1@127.0.0.1'],[]},
 {324,['ns_1@127.0.0.1'],[]},
 {325,['ns_1@127.0.0.1'],[]},
 {326,['ns_1@127.0.0.1'],[]},
 {327,['ns_1@127.0.0.1'],[]},
 {328,['ns_1@127.0.0.1'],[]},
 {329,['ns_1@127.0.0.1'],[]},
 {330,['ns_1@127.0.0.1'],[]},
 {331,['ns_1@127.0.0.1'],[]},
 {332,['ns_1@127.0.0.1'],[]},
 {333,['ns_1@127.0.0.1'],[]},
 {334,['ns_1@127.0.0.1'],[]},
 {335,['ns_1@127.0.0.1'],[]},
 {336,['ns_1@127.0.0.1'],[]},
 {337,['ns_1@127.0.0.1'],[]},
 {338,['ns_1@127.0.0.1'],[]},
 {339,['ns_1@127.0.0.1'],[]},
 {340,['ns_1@127.0.0.1'],[]},
 {341,['ns_1@127.0.0.1'],[]},
 {342,['ns_1@127.0.0.1'],[]},
 {343,['ns_1@127.0.0.1'],[]},
 {344,['ns_1@127.0.0.1'],[]},
 {345,['ns_1@127.0.0.1'],[]},
 {346,['ns_1@127.0.0.1'],[]},
 {347,['ns_1@127.0.0.1'],[]},
 {348,['ns_1@127.0.0.1'],[]},
 {349,['ns_1@127.0.0.1'],[]},
 {350,['ns_1@127.0.0.1'],[]},
 {351,['ns_1@127.0.0.1'],[]},
 {352,['ns_1@127.0.0.1'],[]},
 {353,['ns_1@127.0.0.1'],[]},
 {354,['ns_1@127.0.0.1'],[]},
 {355,['ns_1@127.0.0.1'],[]},
 {356,['ns_1@127.0.0.1'],[]},
 {357,['ns_1@127.0.0.1'],[]},
 {358,['ns_1@127.0.0.1'],[]},
 {359,['ns_1@127.0.0.1'],[]},
 {360,['ns_1@127.0.0.1'],[]},
 {361,['ns_1@127.0.0.1'],[]},
 {362,['ns_1@127.0.0.1'],[]},
 {363,['ns_1@127.0.0.1'],[]},
 {364,['ns_1@127.0.0.1'],[]},
 {365,['ns_1@127.0.0.1'],[]},
 {366,['ns_1@127.0.0.1'],[]},
 {367,['ns_1@127.0.0.1'],[]},
 {368,['ns_1@127.0.0.1'],[]},
 {369,['ns_1@127.0.0.1'],[]},
 {370,['ns_1@127.0.0.1'],[]},
 {371,['ns_1@127.0.0.1'],[]},
 {372,['ns_1@127.0.0.1'],[]},
 {373,['ns_1@127.0.0.1'],[]},
 {374,['ns_1@127.0.0.1'],[]},
 {375,['ns_1@127.0.0.1'],[]},
 {376,['ns_1@127.0.0.1'],[]},
 {377,['ns_1@127.0.0.1'],[]},
 {378,['ns_1@127.0.0.1'],[]},
 {379,['ns_1@127.0.0.1'],[]},
 {380,['ns_1@127.0.0.1'],[]},
 {381,['ns_1@127.0.0.1'],[]},
 {382,['ns_1@127.0.0.1'],[]},
 {383,['ns_1@127.0.0.1'],[]},
 {384,['ns_1@127.0.0.1'],[]},
 {385,['ns_1@127.0.0.1'],[]},
 {386,['ns_1@127.0.0.1'],[]},
 {387,['ns_1@127.0.0.1'],[]},
 {388,['ns_1@127.0.0.1'],[]},
 {389,['ns_1@127.0.0.1'],[]},
 {390,['ns_1@127.0.0.1'],[]},
 {391,['ns_1@127.0.0.1'],[]},
 {392,['ns_1@127.0.0.1'],[]},
 {393,['ns_1@127.0.0.1'],[]},
 {394,['ns_1@127.0.0.1'],[]},
 {395,['ns_1@127.0.0.1'],[]},
 {396,['ns_1@127.0.0.1'],[]},
 {397,['ns_1@127.0.0.1'],[]},
 {398,['ns_1@127.0.0.1'],[]},
 {399,['ns_1@127.0.0.1'],[]},
 {400,['ns_1@127.0.0.1'],[]},
 {401,['ns_1@127.0.0.1'],[]},
 {402,['ns_1@127.0.0.1'],[]},
 {403,['ns_1@127.0.0.1'],[]},
 {404,['ns_1@127.0.0.1'],[]},
 {405,['ns_1@127.0.0.1'],[]},
 {406,['ns_1@127.0.0.1'],[]},
 {407,['ns_1@127.0.0.1'],[]},
 {408,['ns_1@127.0.0.1'],[]},
 {409,['ns_1@127.0.0.1'],[]},
 {410,['ns_1@127.0.0.1'],[]},
 {411,['ns_1@127.0.0.1'],[]},
 {412,['ns_1@127.0.0.1'],[]},
 {413,['ns_1@127.0.0.1'],[]},
 {414,['ns_1@127.0.0.1'],[]},
 {415,['ns_1@127.0.0.1'],[]},
 {416,['ns_1@127.0.0.1'],[]},
 {417,['ns_1@127.0.0.1'],[]},
 {418,['ns_1@127.0.0.1'],[]},
 {419,['ns_1@127.0.0.1'],[]},
 {420,['ns_1@127.0.0.1'],[]},
 {421,['ns_1@127.0.0.1'],[]},
 {422,['ns_1@127.0.0.1'],[]},
 {423,['ns_1@127.0.0.1'],[]},
 {424,['ns_1@127.0.0.1'],[]},
 {425,['ns_1@127.0.0.1'],[]},
 {426,['ns_1@127.0.0.1'],[]},
 {427,['ns_1@127.0.0.1'],[]},
 {428,['ns_1@127.0.0.1'],[]},
 {429,['ns_1@127.0.0.1'],[]},
 {430,['ns_1@127.0.0.1'],[]},
 {431,['ns_1@127.0.0.1'],[]},
 {432,['ns_1@127.0.0.1'],[]},
 {433,['ns_1@127.0.0.1'],[]},
 {434,['ns_1@127.0.0.1'],[]},
 {435,['ns_1@127.0.0.1'],[]},
 {436,['ns_1@127.0.0.1'],[]},
 {437,['ns_1@127.0.0.1'],[]},
 {438,['ns_1@127.0.0.1'],[]},
 {439,['ns_1@127.0.0.1'],[]},
 {440,['ns_1@127.0.0.1'],[]},
 {441,['ns_1@127.0.0.1'],[]},
 {442,['ns_1@127.0.0.1'],[]},
 {443,['ns_1@127.0.0.1'],[]},
 {444,['ns_1@127.0.0.1'],[]},
 {445,['ns_1@127.0.0.1'],[]},
 {446,['ns_1@127.0.0.1'],[]},
 {447,['ns_1@127.0.0.1'],[]},
 {448,['ns_1@127.0.0.1'],[]},
 {449,['ns_1@127.0.0.1'],[]},
 {450,['ns_1@127.0.0.1'],[]},
 {451,['ns_1@127.0.0.1'],[]},
 {452,['ns_1@127.0.0.1'],[]},
 {453,['ns_1@127.0.0.1'],[]},
 {454,['ns_1@127.0.0.1'],[]},
 {455,['ns_1@127.0.0.1'],[]},
 {456,['ns_1@127.0.0.1'],[]},
 {457,['ns_1@127.0.0.1'],[]},
 {458,['ns_1@127.0.0.1'],[]},
 {459,['ns_1@127.0.0.1'],[]},
 {460,['ns_1@127.0.0.1'],[]},
 {461,['ns_1@127.0.0.1'],[]},
 {462,['ns_1@127.0.0.1'],[]},
 {463,['ns_1@127.0.0.1'],[]},
 {464,['ns_1@127.0.0.1'],[]},
 {465,['ns_1@127.0.0.1'],[]},
 {466,['ns_1@127.0.0.1'],[]},
 {467,['ns_1@127.0.0.1'],[]},
 {468,['ns_1@127.0.0.1'],[]},
 {469,['ns_1@127.0.0.1'],[]},
 {470,['ns_1@127.0.0.1'],[]},
 {471,['ns_1@127.0.0.1'],[]},
 {472,['ns_1@127.0.0.1'],[]},
 {473,['ns_1@127.0.0.1'],[]},
 {474,['ns_1@127.0.0.1'],[]},
 {475,['ns_1@127.0.0.1'],[]},
 {476,['ns_1@127.0.0.1'],[]},
 {477,['ns_1@127.0.0.1'],[]},
 {478,['ns_1@127.0.0.1'],[]},
 {479,['ns_1@127.0.0.1'],[]},
 {480,['ns_1@127.0.0.1'],[]},
 {481,['ns_1@127.0.0.1'],[]},
 {482,['ns_1@127.0.0.1'],[]},
 {483,['ns_1@127.0.0.1'],[]},
 {484,['ns_1@127.0.0.1'],[]},
 {485,['ns_1@127.0.0.1'],[]},
 {486,['ns_1@127.0.0.1'],[]},
 {487,['ns_1@127.0.0.1'],[]},
 {488,['ns_1@127.0.0.1'],[]},
 {489,['ns_1@127.0.0.1'],[]},
 {490,['ns_1@127.0.0.1'],[]},
 {491,['ns_1@127.0.0.1'],[]},
 {492,['ns_1@127.0.0.1'],[]},
 {493,['ns_1@127.0.0.1'],[]},
 {494,['ns_1@127.0.0.1'],[]},
 {495,['ns_1@127.0.0.1'],[]},
 {496,['ns_1@127.0.0.1'],[]},
 {497,['ns_1@127.0.0.1'],[]},
 {498,['ns_1@127.0.0.1'],[]},
 {499,['ns_1@127.0.0.1'],[]},
 {500,['ns_1@127.0.0.1'],[]},
 {501,['ns_1@127.0.0.1'],[]},
 {502,['ns_1@127.0.0.1'],[]},
 {503,['ns_1@127.0.0.1'],[]},
 {504,['ns_1@127.0.0.1'],[]},
 {505,['ns_1@127.0.0.1'],[]},
 {506,['ns_1@127.0.0.1'],[]},
 {507,['ns_1@127.0.0.1'],[]},
 {508,['ns_1@127.0.0.1'],[]},
 {509,['ns_1@127.0.0.1'],[]},
 {510,['ns_1@127.0.0.1'],[]},
 {511,['ns_1@127.0.0.1'],[]},
 {512,['ns_1@127.0.0.1'],[]},
 {513,['ns_1@127.0.0.1'],[]},
 {514,['ns_1@127.0.0.1'],[]},
 {515,['ns_1@127.0.0.1'],[]},
 {516,['ns_1@127.0.0.1'],[]},
 {517,['ns_1@127.0.0.1'],[]},
 {518,['ns_1@127.0.0.1'],[]},
 {519,['ns_1@127.0.0.1'],[]},
 {520,['ns_1@127.0.0.1'],[]},
 {521,['ns_1@127.0.0.1'],[]},
 {522,['ns_1@127.0.0.1'],[]},
 {523,['ns_1@127.0.0.1'],[]},
 {524,['ns_1@127.0.0.1'],[]},
 {525,['ns_1@127.0.0.1'],[]},
 {526,['ns_1@127.0.0.1'],[]},
 {527,['ns_1@127.0.0.1'],[]},
 {528,['ns_1@127.0.0.1'],[]},
 {529,['ns_1@127.0.0.1'],[]},
 {530,['ns_1@127.0.0.1'],[]},
 {531,['ns_1@127.0.0.1'],[]},
 {532,['ns_1@127.0.0.1'],[]},
 {533,['ns_1@127.0.0.1'],[]},
 {534,['ns_1@127.0.0.1'],[]},
 {535,['ns_1@127.0.0.1'],[]},
 {536,['ns_1@127.0.0.1'],[]},
 {537,['ns_1@127.0.0.1'],[]},
 {538,['ns_1@127.0.0.1'],[]},
 {539,['ns_1@127.0.0.1'],[]},
 {540,['ns_1@127.0.0.1'],[]},
 {541,['ns_1@127.0.0.1'],[]},
 {542,['ns_1@127.0.0.1'],[]},
 {543,['ns_1@127.0.0.1'],[]},
 {544,['ns_1@127.0.0.1'],[]},
 {545,['ns_1@127.0.0.1'],[]},
 {546,['ns_1@127.0.0.1'],[]},
 {547,['ns_1@127.0.0.1'],[]},
 {548,['ns_1@127.0.0.1'],[]},
 {549,['ns_1@127.0.0.1'],[]},
 {550,['ns_1@127.0.0.1'],[]},
 {551,['ns_1@127.0.0.1'],[]},
 {552,['ns_1@127.0.0.1'],[]},
 {553,['ns_1@127.0.0.1'],[]},
 {554,['ns_1@127.0.0.1'],[]},
 {555,['ns_1@127.0.0.1'],[]},
 {556,['ns_1@127.0.0.1'],[]},
 {557,['ns_1@127.0.0.1'],[]},
 {558,['ns_1@127.0.0.1'],[]},
 {559,['ns_1@127.0.0.1'],[]},
 {560,['ns_1@127.0.0.1'],[]},
 {561,['ns_1@127.0.0.1'],[]},
 {562,['ns_1@127.0.0.1'],[]},
 {563,['ns_1@127.0.0.1'],[]},
 {564,['ns_1@127.0.0.1'],[]},
 {565,['ns_1@127.0.0.1'],[]},
 {566,['ns_1@127.0.0.1'],[]},
 {567,['ns_1@127.0.0.1'],[]},
 {568,['ns_1@127.0.0.1'],[]},
 {569,['ns_1@127.0.0.1'],[]},
 {570,['ns_1@127.0.0.1'],[]},
 {571,['ns_1@127.0.0.1'],[]},
 {572,['ns_1@127.0.0.1'],[]},
 {573,['ns_1@127.0.0.1'],[]},
 {574,['ns_1@127.0.0.1'],[]},
 {575,['ns_1@127.0.0.1'],[]},
 {576,['ns_1@127.0.0.1'],[]},
 {577,['ns_1@127.0.0.1'],[]},
 {578,['ns_1@127.0.0.1'],[]},
 {579,['ns_1@127.0.0.1'],[]},
 {580,['ns_1@127.0.0.1'],[]},
 {581,['ns_1@127.0.0.1'],[]},
 {582,['ns_1@127.0.0.1'],[]},
 {583,['ns_1@127.0.0.1'],[]},
 {584,['ns_1@127.0.0.1'],[]},
 {585,['ns_1@127.0.0.1'],[]},
 {586,['ns_1@127.0.0.1'],[]},
 {587,['ns_1@127.0.0.1'],[]},
 {588,['ns_1@127.0.0.1'],[]},
 {589,['ns_1@127.0.0.1'],[]},
 {590,['ns_1@127.0.0.1'],[]},
 {591,['ns_1@127.0.0.1'],[]},
 {592,['ns_1@127.0.0.1'],[]},
 {593,['ns_1@127.0.0.1'],[]},
 {594,['ns_1@127.0.0.1'],[]},
 {595,['ns_1@127.0.0.1'],[]},
 {596,['ns_1@127.0.0.1'],[]},
 {597,['ns_1@127.0.0.1'],[]},
 {598,['ns_1@127.0.0.1'],[]},
 {599,['ns_1@127.0.0.1'],[]},
 {600,['ns_1@127.0.0.1'],[]},
 {601,['ns_1@127.0.0.1'],[]},
 {602,['ns_1@127.0.0.1'],[]},
 {603,['ns_1@127.0.0.1'],[]},
 {604,['ns_1@127.0.0.1'],[]},
 {605,['ns_1@127.0.0.1'],[]},
 {606,['ns_1@127.0.0.1'],[]},
 {607,['ns_1@127.0.0.1'],[]},
 {608,['ns_1@127.0.0.1'],[]},
 {609,['ns_1@127.0.0.1'],[]},
 {610,['ns_1@127.0.0.1'],[]},
 {611,['ns_1@127.0.0.1'],[]},
 {612,['ns_1@127.0.0.1'],[]},
 {613,['ns_1@127.0.0.1'],[]},
 {614,['ns_1@127.0.0.1'],[]},
 {615,['ns_1@127.0.0.1'],[]},
 {616,['ns_1@127.0.0.1'],[]},
 {617,['ns_1@127.0.0.1'],[]},
 {618,['ns_1@127.0.0.1'],[]},
 {619,['ns_1@127.0.0.1'],[]},
 {620,['ns_1@127.0.0.1'],[]},
 {621,['ns_1@127.0.0.1'],[]},
 {622,['ns_1@127.0.0.1'],[]},
 {623,['ns_1@127.0.0.1'],[]},
 {624,['ns_1@127.0.0.1'],[]},
 {625,['ns_1@127.0.0.1'],[]},
 {626,['ns_1@127.0.0.1'],[]},
 {627,['ns_1@127.0.0.1'],[]},
 {628,['ns_1@127.0.0.1'],[]},
 {629,['ns_1@127.0.0.1'],[]},
 {630,['ns_1@127.0.0.1'],[]},
 {631,['ns_1@127.0.0.1'],[]},
 {632,['ns_1@127.0.0.1'],[]},
 {633,['ns_1@127.0.0.1'],[]},
 {634,['ns_1@127.0.0.1'],[]},
 {635,['ns_1@127.0.0.1'],[]},
 {636,['ns_1@127.0.0.1'],[]},
 {637,['ns_1@127.0.0.1'],[]},
 {638,['ns_1@127.0.0.1'],[]},
 {639,['ns_1@127.0.0.1'],[]},
 {640,['ns_1@127.0.0.1'],[]},
 {641,['ns_1@127.0.0.1'],[]},
 {642,['ns_1@127.0.0.1'],[]},
 {643,['ns_1@127.0.0.1'],[]},
 {644,['ns_1@127.0.0.1'],[]},
 {645,['ns_1@127.0.0.1'],[]},
 {646,['ns_1@127.0.0.1'],[]},
 {647,['ns_1@127.0.0.1'],[]},
 {648,['ns_1@127.0.0.1'],[]},
 {649,['ns_1@127.0.0.1'],[]},
 {650,['ns_1@127.0.0.1'],[]},
 {651,['ns_1@127.0.0.1'],[]},
 {652,['ns_1@127.0.0.1'],[]},
 {653,['ns_1@127.0.0.1'],[]},
 {654,['ns_1@127.0.0.1'],[]},
 {655,['ns_1@127.0.0.1'],[]},
 {656,['ns_1@127.0.0.1'],[]},
 {657,['ns_1@127.0.0.1'],[]},
 {658,['ns_1@127.0.0.1'],[]},
 {659,['ns_1@127.0.0.1'],[]},
 {660,['ns_1@127.0.0.1'],[]},
 {661,['ns_1@127.0.0.1'],[]},
 {662,['ns_1@127.0.0.1'],[]},
 {663,['ns_1@127.0.0.1'],[]},
 {664,['ns_1@127.0.0.1'],[]},
 {665,['ns_1@127.0.0.1'],[]},
 {666,['ns_1@127.0.0.1'],[]},
 {667,['ns_1@127.0.0.1'],[]},
 {668,['ns_1@127.0.0.1'],[]},
 {669,['ns_1@127.0.0.1'],[]},
 {670,['ns_1@127.0.0.1'],[]},
 {671,['ns_1@127.0.0.1'],[]},
 {672,['ns_1@127.0.0.1'],[]},
 {673,['ns_1@127.0.0.1'],[]},
 {674,['ns_1@127.0.0.1'],[]},
 {675,['ns_1@127.0.0.1'],[]},
 {676,['ns_1@127.0.0.1'],[]},
 {677,['ns_1@127.0.0.1'],[]},
 {678,['ns_1@127.0.0.1'],[]},
 {679,['ns_1@127.0.0.1'],[]},
 {680,['ns_1@127.0.0.1'],[]},
 {681,['ns_1@127.0.0.1'],[]},
 {682,['ns_1@127.0.0.1'],[]},
 {683,['ns_1@127.0.0.1'],[]},
 {684,['ns_1@127.0.0.1'],[]},
 {685,['ns_1@127.0.0.1'],[]},
 {686,['ns_1@127.0.0.1'],[]},
 {687,['ns_1@127.0.0.1'],[]},
 {688,['ns_1@127.0.0.1'],[]},
 {689,['ns_1@127.0.0.1'],[]},
 {690,['ns_1@127.0.0.1'],[]},
 {691,['ns_1@127.0.0.1'],[]},
 {692,['ns_1@127.0.0.1'],[]},
 {693,['ns_1@127.0.0.1'],[]},
 {694,['ns_1@127.0.0.1'],[]},
 {695,['ns_1@127.0.0.1'],[]},
 {696,['ns_1@127.0.0.1'],[]},
 {697,['ns_1@127.0.0.1'],[]},
 {698,['ns_1@127.0.0.1'],[]},
 {699,['ns_1@127.0.0.1'],[]},
 {700,['ns_1@127.0.0.1'],[]},
 {701,['ns_1@127.0.0.1'],[]},
 {702,['ns_1@127.0.0.1'],[]},
 {703,['ns_1@127.0.0.1'],[]},
 {704,['ns_1@127.0.0.1'],[]},
 {705,['ns_1@127.0.0.1'],[]},
 {706,['ns_1@127.0.0.1'],[]},
 {707,['ns_1@127.0.0.1'],[]},
 {708,['ns_1@127.0.0.1'],[]},
 {709,['ns_1@127.0.0.1'],[]},
 {710,['ns_1@127.0.0.1'],[]},
 {711,['ns_1@127.0.0.1'],[]},
 {712,['ns_1@127.0.0.1'],[]},
 {713,['ns_1@127.0.0.1'],[]},
 {714,['ns_1@127.0.0.1'],[]},
 {715,['ns_1@127.0.0.1'],[]},
 {716,['ns_1@127.0.0.1'],[]},
 {717,['ns_1@127.0.0.1'],[]},
 {718,['ns_1@127.0.0.1'],[]},
 {719,['ns_1@127.0.0.1'],[]},
 {720,['ns_1@127.0.0.1'],[]},
 {721,['ns_1@127.0.0.1'],[]},
 {722,['ns_1@127.0.0.1'],[]},
 {723,['ns_1@127.0.0.1'],[]},
 {724,['ns_1@127.0.0.1'],[]},
 {725,['ns_1@127.0.0.1'],[]},
 {726,['ns_1@127.0.0.1'],[]},
 {727,['ns_1@127.0.0.1'],[]},
 {728,['ns_1@127.0.0.1'],[]},
 {729,['ns_1@127.0.0.1'],[]},
 {730,['ns_1@127.0.0.1'],[]},
 {731,['ns_1@127.0.0.1'],[]},
 {732,['ns_1@127.0.0.1'],[]},
 {733,['ns_1@127.0.0.1'],[]},
 {734,['ns_1@127.0.0.1'],[]},
 {735,['ns_1@127.0.0.1'],[]},
 {736,['ns_1@127.0.0.1'],[]},
 {737,['ns_1@127.0.0.1'],[]},
 {738,['ns_1@127.0.0.1'],[]},
 {739,['ns_1@127.0.0.1'],[]},
 {740,['ns_1@127.0.0.1'],[]},
 {741,['ns_1@127.0.0.1'],[]},
 {742,['ns_1@127.0.0.1'],[]},
 {743,['ns_1@127.0.0.1'],[]},
 {744,['ns_1@127.0.0.1'],[]},
 {745,['ns_1@127.0.0.1'],[]},
 {746,['ns_1@127.0.0.1'],[]},
 {747,['ns_1@127.0.0.1'],[]},
 {748,['ns_1@127.0.0.1'],[]},
 {749,['ns_1@127.0.0.1'],[]},
 {750,['ns_1@127.0.0.1'],[]},
 {751,['ns_1@127.0.0.1'],[]},
 {752,['ns_1@127.0.0.1'],[]},
 {753,['ns_1@127.0.0.1'],[]},
 {754,['ns_1@127.0.0.1'],[]},
 {755,['ns_1@127.0.0.1'],[]},
 {756,['ns_1@127.0.0.1'],[]},
 {757,['ns_1@127.0.0.1'],[]},
 {758,['ns_1@127.0.0.1'],[]},
 {759,['ns_1@127.0.0.1'],[]},
 {760,['ns_1@127.0.0.1'],[]},
 {761,['ns_1@127.0.0.1'],[]},
 {762,['ns_1@127.0.0.1'],[]},
 {763,['ns_1@127.0.0.1'],[]},
 {764,['ns_1@127.0.0.1'],[]},
 {765,['ns_1@127.0.0.1'],[]},
 {766,['ns_1@127.0.0.1'],[]},
 {767,['ns_1@127.0.0.1'],[]},
 {768,['ns_1@127.0.0.1'],[]},
 {769,['ns_1@127.0.0.1'],[]},
 {770,['ns_1@127.0.0.1'],[]},
 {771,['ns_1@127.0.0.1'],[]},
 {772,['ns_1@127.0.0.1'],[]},
 {773,['ns_1@127.0.0.1'],[]},
 {774,['ns_1@127.0.0.1'],[]},
 {775,['ns_1@127.0.0.1'],[]},
 {776,['ns_1@127.0.0.1'],[]},
 {777,['ns_1@127.0.0.1'],[]},
 {778,['ns_1@127.0.0.1'],[]},
 {779,['ns_1@127.0.0.1'],[]},
 {780,['ns_1@127.0.0.1'],[]},
 {781,['ns_1@127.0.0.1'],[]},
 {782,['ns_1@127.0.0.1'],[]},
 {783,['ns_1@127.0.0.1'],[]},
 {784,['ns_1@127.0.0.1'],[]},
 {785,['ns_1@127.0.0.1'],[]},
 {786,['ns_1@127.0.0.1'],[]},
 {787,['ns_1@127.0.0.1'],[]},
 {788,['ns_1@127.0.0.1'],[]},
 {789,['ns_1@127.0.0.1'],[]},
 {790,['ns_1@127.0.0.1'],[]},
 {791,['ns_1@127.0.0.1'],[]},
 {792,['ns_1@127.0.0.1'],[]},
 {793,['ns_1@127.0.0.1'],[]},
 {794,['ns_1@127.0.0.1'],[]},
 {795,['ns_1@127.0.0.1'],[]},
 {796,['ns_1@127.0.0.1'],[]},
 {797,['ns_1@127.0.0.1'],[]},
 {798,['ns_1@127.0.0.1'],[]},
 {799,['ns_1@127.0.0.1'],[]},
 {800,['ns_1@127.0.0.1'],[]},
 {801,['ns_1@127.0.0.1'],[]},
 {802,['ns_1@127.0.0.1'],[]},
 {803,['ns_1@127.0.0.1'],[]},
 {804,['ns_1@127.0.0.1'],[]},
 {805,['ns_1@127.0.0.1'],[]},
 {806,['ns_1@127.0.0.1'],[]},
 {807,['ns_1@127.0.0.1'],[]},
 {808,['ns_1@127.0.0.1'],[]},
 {809,['ns_1@127.0.0.1'],[]},
 {810,['ns_1@127.0.0.1'],[]},
 {811,['ns_1@127.0.0.1'],[]},
 {812,['ns_1@127.0.0.1'],[]},
 {813,['ns_1@127.0.0.1'],[]},
 {814,['ns_1@127.0.0.1'],[]},
 {815,['ns_1@127.0.0.1'],[]},
 {816,['ns_1@127.0.0.1'],[]},
 {817,['ns_1@127.0.0.1'],[]},
 {818,['ns_1@127.0.0.1'],[]},
 {819,['ns_1@127.0.0.1'],[]},
 {820,['ns_1@127.0.0.1'],[]},
 {821,['ns_1@127.0.0.1'],[]},
 {822,['ns_1@127.0.0.1'],[]},
 {823,['ns_1@127.0.0.1'],[]},
 {824,['ns_1@127.0.0.1'],[]},
 {825,['ns_1@127.0.0.1'],[]},
 {826,['ns_1@127.0.0.1'],[]},
 {827,['ns_1@127.0.0.1'],[]},
 {828,['ns_1@127.0.0.1'],[]},
 {829,['ns_1@127.0.0.1'],[]},
 {830,['ns_1@127.0.0.1'],[]},
 {831,['ns_1@127.0.0.1'],[]},
 {832,['ns_1@127.0.0.1'],[]},
 {833,['ns_1@127.0.0.1'],[]},
 {834,['ns_1@127.0.0.1'],[]},
 {835,['ns_1@127.0.0.1'],[]},
 {836,['ns_1@127.0.0.1'],[]},
 {837,['ns_1@127.0.0.1'],[]},
 {838,['ns_1@127.0.0.1'],[]},
 {839,['ns_1@127.0.0.1'],[]},
 {840,['ns_1@127.0.0.1'],[]},
 {841,['ns_1@127.0.0.1'],[]},
 {842,['ns_1@127.0.0.1'],[]},
 {843,['ns_1@127.0.0.1'],[]},
 {844,['ns_1@127.0.0.1'],[]},
 {845,['ns_1@127.0.0.1'],[]},
 {846,['ns_1@127.0.0.1'],[]},
 {847,['ns_1@127.0.0.1'],[]},
 {848,['ns_1@127.0.0.1'],[]},
 {849,['ns_1@127.0.0.1'],[]},
 {850,['ns_1@127.0.0.1'],[]},
 {851,['ns_1@127.0.0.1'],[]},
 {852,['ns_1@127.0.0.1'],[]},
 {853,['ns_1@127.0.0.1'],[]},
 {854,['ns_1@127.0.0.1'],[]},
 {855,['ns_1@127.0.0.1'],[]},
 {856,['ns_1@127.0.0.1'],[]},
 {857,['ns_1@127.0.0.1'],[]},
 {858,['ns_1@127.0.0.1'],[]},
 {859,['ns_1@127.0.0.1'],[]},
 {860,['ns_1@127.0.0.1'],[]},
 {861,['ns_1@127.0.0.1'],[]},
 {862,['ns_1@127.0.0.1'],[]},
 {863,['ns_1@127.0.0.1'],[]},
 {864,['ns_1@127.0.0.1'],[]},
 {865,['ns_1@127.0.0.1'],[]},
 {866,['ns_1@127.0.0.1'],[]},
 {867,['ns_1@127.0.0.1'],[]},
 {868,['ns_1@127.0.0.1'],[]},
 {869,['ns_1@127.0.0.1'],[]},
 {870,['ns_1@127.0.0.1'],[]},
 {871,['ns_1@127.0.0.1'],[]},
 {872,['ns_1@127.0.0.1'],[]},
 {873,['ns_1@127.0.0.1'],[]},
 {874,['ns_1@127.0.0.1'],[]},
 {875,['ns_1@127.0.0.1'],[]},
 {876,['ns_1@127.0.0.1'],[]},
 {877,['ns_1@127.0.0.1'],[]},
 {878,['ns_1@127.0.0.1'],[]},
 {879,['ns_1@127.0.0.1'],[]},
 {880,['ns_1@127.0.0.1'],[]},
 {881,['ns_1@127.0.0.1'],[]},
 {882,['ns_1@127.0.0.1'],[]},
 {883,['ns_1@127.0.0.1'],[]},
 {884,['ns_1@127.0.0.1'],[]},
 {885,['ns_1@127.0.0.1'],[]},
 {886,['ns_1@127.0.0.1'],[]},
 {887,['ns_1@127.0.0.1'],[]},
 {888,['ns_1@127.0.0.1'],[]},
 {889,['ns_1@127.0.0.1'],[]},
 {890,['ns_1@127.0.0.1'],[]},
 {891,['ns_1@127.0.0.1'],[]},
 {892,['ns_1@127.0.0.1'],[]},
 {893,['ns_1@127.0.0.1'],[]},
 {894,['ns_1@127.0.0.1'],[]},
 {895,['ns_1@127.0.0.1'],[]},
 {896,['ns_1@127.0.0.1'],[]},
 {897,['ns_1@127.0.0.1'],[]},
 {898,['ns_1@127.0.0.1'],[]},
 {899,['ns_1@127.0.0.1'],[]},
 {900,['ns_1@127.0.0.1'],[]},
 {901,['ns_1@127.0.0.1'],[]},
 {902,['ns_1@127.0.0.1'],[]},
 {903,['ns_1@127.0.0.1'],[]},
 {904,['ns_1@127.0.0.1'],[]},
 {905,['ns_1@127.0.0.1'],[]},
 {906,['ns_1@127.0.0.1'],[]},
 {907,['ns_1@127.0.0.1'],[]},
 {908,['ns_1@127.0.0.1'],[]},
 {909,['ns_1@127.0.0.1'],[]},
 {910,['ns_1@127.0.0.1'],[]},
 {911,['ns_1@127.0.0.1'],[]},
 {912,['ns_1@127.0.0.1'],[]},
 {913,['ns_1@127.0.0.1'],[]},
 {914,['ns_1@127.0.0.1'],[]},
 {915,['ns_1@127.0.0.1'],[]},
 {916,['ns_1@127.0.0.1'],[]},
 {917,['ns_1@127.0.0.1'],[]},
 {918,['ns_1@127.0.0.1'],[]},
 {919,['ns_1@127.0.0.1'],[]},
 {920,['ns_1@127.0.0.1'],[]},
 {921,['ns_1@127.0.0.1'],[]},
 {922,['ns_1@127.0.0.1'],[]},
 {923,['ns_1@127.0.0.1'],[]},
 {924,['ns_1@127.0.0.1'],[]},
 {925,['ns_1@127.0.0.1'],[]},
 {926,['ns_1@127.0.0.1'],[]},
 {927,['ns_1@127.0.0.1'],[]},
 {928,['ns_1@127.0.0.1'],[]},
 {929,['ns_1@127.0.0.1'],[]},
 {930,['ns_1@127.0.0.1'],[]},
 {931,['ns_1@127.0.0.1'],[]},
 {932,['ns_1@127.0.0.1'],[]},
 {933,['ns_1@127.0.0.1'],[]},
 {934,['ns_1@127.0.0.1'],[]},
 {935,['ns_1@127.0.0.1'],[]},
 {936,['ns_1@127.0.0.1'],[]},
 {937,['ns_1@127.0.0.1'],[]},
 {938,['ns_1@127.0.0.1'],[]},
 {939,['ns_1@127.0.0.1'],[]},
 {940,['ns_1@127.0.0.1'],[]},
 {941,['ns_1@127.0.0.1'],[]},
 {942,['ns_1@127.0.0.1'],[]},
 {943,['ns_1@127.0.0.1'],[]},
 {944,['ns_1@127.0.0.1'],[]},
 {945,['ns_1@127.0.0.1'],[]},
 {946,['ns_1@127.0.0.1'],[]},
 {947,['ns_1@127.0.0.1'],[]},
 {948,['ns_1@127.0.0.1'],[]},
 {949,['ns_1@127.0.0.1'],[]},
 {950,['ns_1@127.0.0.1'],[]},
 {951,['ns_1@127.0.0.1'],[]},
 {952,['ns_1@127.0.0.1'],[]},
 {953,['ns_1@127.0.0.1'],[]},
 {954,['ns_1@127.0.0.1'],[]},
 {955,['ns_1@127.0.0.1'],[]},
 {956,['ns_1@127.0.0.1'],[]},
 {957,['ns_1@127.0.0.1'],[]},
 {958,['ns_1@127.0.0.1'],[]},
 {959,['ns_1@127.0.0.1'],[]},
 {960,['ns_1@127.0.0.1'],[]},
 {961,['ns_1@127.0.0.1'],[]},
 {962,['ns_1@127.0.0.1'],[]},
 {963,['ns_1@127.0.0.1'],[]},
 {964,['ns_1@127.0.0.1'],[]},
 {965,['ns_1@127.0.0.1'],[]},
 {966,['ns_1@127.0.0.1'],[]},
 {967,['ns_1@127.0.0.1'],[]},
 {968,['ns_1@127.0.0.1'],[]},
 {969,['ns_1@127.0.0.1'],[]},
 {970,['ns_1@127.0.0.1'],[]},
 {971,['ns_1@127.0.0.1'],[]},
 {972,['ns_1@127.0.0.1'],[]},
 {973,['ns_1@127.0.0.1'],[]},
 {974,['ns_1@127.0.0.1'],[]},
 {975,['ns_1@127.0.0.1'],[]},
 {976,['ns_1@127.0.0.1'],[]},
 {977,['ns_1@127.0.0.1'],[]},
 {978,['ns_1@127.0.0.1'],[]},
 {979,['ns_1@127.0.0.1'],[]},
 {980,['ns_1@127.0.0.1'],[]},
 {981,['ns_1@127.0.0.1'],[]},
 {982,['ns_1@127.0.0.1'],[]},
 {983,['ns_1@127.0.0.1'],[]},
 {984,['ns_1@127.0.0.1'],[]},
 {985,['ns_1@127.0.0.1'],[]},
 {986,['ns_1@127.0.0.1'],[]},
 {987,['ns_1@127.0.0.1'],[]},
 {988,['ns_1@127.0.0.1'],[]},
 {989,['ns_1@127.0.0.1'],[]},
 {990,['ns_1@127.0.0.1'],[]},
 {991,['ns_1@127.0.0.1'],[]},
 {992,['ns_1@127.0.0.1'],[]},
 {993,['ns_1@127.0.0.1'],[]},
 {994,['ns_1@127.0.0.1'],[]},
 {995,['ns_1@127.0.0.1'],[]},
 {996,['ns_1@127.0.0.1'],[]},
 {997,['ns_1@127.0.0.1'],[]},
 {998,['ns_1@127.0.0.1'],[]},
 {999,['ns_1@127.0.0.1'],[]},
 {1000,['ns_1@127.0.0.1'],[]},
 {1001,['ns_1@127.0.0.1'],[]},
 {1002,['ns_1@127.0.0.1'],[]},
 {1003,['ns_1@127.0.0.1'],[]},
 {1004,['ns_1@127.0.0.1'],[]},
 {1005,['ns_1@127.0.0.1'],[]},
 {1006,['ns_1@127.0.0.1'],[]},
 {1007,['ns_1@127.0.0.1'],[]},
 {1008,['ns_1@127.0.0.1'],[]},
 {1009,['ns_1@127.0.0.1'],[]},
 {1010,['ns_1@127.0.0.1'],[]},
 {1011,['ns_1@127.0.0.1'],[]},
 {1012,['ns_1@127.0.0.1'],[]},
 {1013,['ns_1@127.0.0.1'],[]},
 {1014,['ns_1@127.0.0.1'],[]},
 {1015,['ns_1@127.0.0.1'],[]},
 {1016,['ns_1@127.0.0.1'],[]},
 {1017,['ns_1@127.0.0.1'],[]},
 {1018,['ns_1@127.0.0.1'],[]},
 {1019,['ns_1@127.0.0.1'],[]},
 {1020,['ns_1@127.0.0.1'],[]},
 {1021,['ns_1@127.0.0.1'],[]},
 {1022,['ns_1@127.0.0.1'],[]},
 {1023,['ns_1@127.0.0.1'],[]}]
[ns_server:debug,2020-06-16T21:33:50.050Z,ns_1@127.0.0.1:<0.3011.0>:ns_janitor:config_sync:259]Going to push config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2020-06-16T21:33:50.056Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([buckets]..)
[ns_server:debug,2020-06-16T21:33:50.057Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2020-06-16T21:33:50.057Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:handle_call:128]Fully synchronized config in 16 us
[ns_server:info,2020-06-16T21:33:50.062Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1023 state to active
[ns_server:info,2020-06-16T21:33:50.064Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1022 state to active
[ns_server:info,2020-06-16T21:33:50.065Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1021 state to active
[ns_server:info,2020-06-16T21:33:50.065Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1020 state to active
[ns_server:info,2020-06-16T21:33:50.066Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1019 state to active
[ns_server:info,2020-06-16T21:33:50.066Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1018 state to active
[ns_server:info,2020-06-16T21:33:50.067Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1017 state to active
[ns_server:info,2020-06-16T21:33:50.070Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1016 state to active
[ns_server:info,2020-06-16T21:33:50.071Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1015 state to active
[ns_server:info,2020-06-16T21:33:50.071Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1014 state to active
[ns_server:info,2020-06-16T21:33:50.071Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1013 state to active
[ns_server:info,2020-06-16T21:33:50.072Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1012 state to active
[ns_server:info,2020-06-16T21:33:50.073Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1011 state to active
[ns_server:info,2020-06-16T21:33:50.073Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1010 state to active
[ns_server:info,2020-06-16T21:33:50.075Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1009 state to active
[ns_server:info,2020-06-16T21:33:50.078Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1008 state to active
[ns_server:info,2020-06-16T21:33:50.079Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1007 state to active
[ns_server:info,2020-06-16T21:33:50.082Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1006 state to active
[ns_server:info,2020-06-16T21:33:50.083Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1005 state to active
[ns_server:info,2020-06-16T21:33:50.084Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1004 state to active
[ns_server:info,2020-06-16T21:33:50.085Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1003 state to active
[ns_server:info,2020-06-16T21:33:50.085Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1002 state to active
[ns_server:info,2020-06-16T21:33:50.086Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1001 state to active
[ns_server:info,2020-06-16T21:33:50.087Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1000 state to active
[ns_server:info,2020-06-16T21:33:50.090Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 999 state to active
[ns_server:info,2020-06-16T21:33:50.103Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 998 state to active
[ns_server:info,2020-06-16T21:33:50.104Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 997 state to active
[ns_server:info,2020-06-16T21:33:50.105Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 996 state to active
[ns_server:info,2020-06-16T21:33:50.106Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 995 state to active
[ns_server:info,2020-06-16T21:33:50.107Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 994 state to active
[ns_server:info,2020-06-16T21:33:50.109Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 993 state to active
[ns_server:info,2020-06-16T21:33:50.114Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 992 state to active
[ns_server:info,2020-06-16T21:33:50.117Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 991 state to active
[ns_server:info,2020-06-16T21:33:50.119Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 990 state to active
[ns_server:info,2020-06-16T21:33:50.121Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 989 state to active
[ns_server:info,2020-06-16T21:33:50.123Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 988 state to active
[ns_server:info,2020-06-16T21:33:50.126Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 987 state to active
[ns_server:info,2020-06-16T21:33:50.136Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 986 state to active
[ns_server:info,2020-06-16T21:33:50.137Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 985 state to active
[ns_server:info,2020-06-16T21:33:50.140Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 984 state to active
[ns_server:info,2020-06-16T21:33:50.141Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 983 state to active
[ns_server:info,2020-06-16T21:33:50.143Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 982 state to active
[ns_server:info,2020-06-16T21:33:50.146Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 981 state to active
[ns_server:info,2020-06-16T21:33:50.147Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 980 state to active
[ns_server:info,2020-06-16T21:33:50.147Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 979 state to active
[ns_server:info,2020-06-16T21:33:50.148Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 978 state to active
[ns_server:info,2020-06-16T21:33:50.148Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 977 state to active
[ns_server:info,2020-06-16T21:33:50.255Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 976 state to active
[ns_server:info,2020-06-16T21:33:50.258Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 975 state to active
[ns_server:info,2020-06-16T21:33:50.261Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 974 state to active
[ns_server:info,2020-06-16T21:33:50.264Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 973 state to active
[ns_server:info,2020-06-16T21:33:50.265Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 972 state to active
[ns_server:info,2020-06-16T21:33:50.266Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 971 state to active
[ns_server:info,2020-06-16T21:33:50.266Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 970 state to active
[ns_server:info,2020-06-16T21:33:50.267Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 969 state to active
[ns_server:info,2020-06-16T21:33:50.269Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 968 state to active
[ns_server:info,2020-06-16T21:33:50.281Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 967 state to active
[ns_server:info,2020-06-16T21:33:50.284Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 966 state to active
[ns_server:info,2020-06-16T21:33:50.287Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 965 state to active
[ns_server:info,2020-06-16T21:33:50.289Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 964 state to active
[ns_server:info,2020-06-16T21:33:50.291Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 963 state to active
[ns_server:info,2020-06-16T21:33:50.293Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 962 state to active
[ns_server:info,2020-06-16T21:33:50.297Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 961 state to active
[ns_server:info,2020-06-16T21:33:50.304Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 960 state to active
[ns_server:info,2020-06-16T21:33:50.319Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 959 state to active
[ns_server:debug,2020-06-16T21:33:50.322Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:info,2020-06-16T21:33:50.322Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 958 state to active
[ns_server:info,2020-06-16T21:33:50.324Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 957 state to active
[ns_server:info,2020-06-16T21:33:50.325Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 956 state to active
[ns_server:info,2020-06-16T21:33:50.332Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 955 state to active
[ns_server:info,2020-06-16T21:33:50.334Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 954 state to active
[ns_server:info,2020-06-16T21:33:50.335Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 953 state to active
[ns_server:info,2020-06-16T21:33:50.336Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 952 state to active
[ns_server:info,2020-06-16T21:33:50.338Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 951 state to active
[ns_server:info,2020-06-16T21:33:50.339Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 950 state to active
[ns_server:info,2020-06-16T21:33:50.342Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 949 state to active
[ns_server:info,2020-06-16T21:33:50.347Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 948 state to active
[ns_server:info,2020-06-16T21:33:50.351Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 947 state to active
[ns_server:info,2020-06-16T21:33:50.353Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 946 state to active
[ns_server:info,2020-06-16T21:33:50.356Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 945 state to active
[ns_server:info,2020-06-16T21:33:50.364Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 944 state to active
[ns_server:info,2020-06-16T21:33:50.369Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 943 state to active
[ns_server:info,2020-06-16T21:33:50.370Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 942 state to active
[ns_server:info,2020-06-16T21:33:50.372Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 941 state to active
[ns_server:info,2020-06-16T21:33:50.375Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 940 state to active
[ns_server:info,2020-06-16T21:33:50.377Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 939 state to active
[ns_server:info,2020-06-16T21:33:50.380Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 938 state to active
[ns_server:info,2020-06-16T21:33:50.387Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 937 state to active
[ns_server:info,2020-06-16T21:33:50.411Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 936 state to active
[ns_server:info,2020-06-16T21:33:50.448Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 935 state to active
[ns_server:info,2020-06-16T21:33:50.454Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 934 state to active
[ns_server:info,2020-06-16T21:33:50.456Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 933 state to active
[ns_server:info,2020-06-16T21:33:50.457Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 932 state to active
[ns_server:info,2020-06-16T21:33:50.460Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 931 state to active
[ns_server:info,2020-06-16T21:33:50.472Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 930 state to active
[ns_server:info,2020-06-16T21:33:50.487Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 929 state to active
[ns_server:info,2020-06-16T21:33:50.490Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 928 state to active
[ns_server:info,2020-06-16T21:33:50.495Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 927 state to active
[ns_server:info,2020-06-16T21:33:50.504Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 926 state to active
[ns_server:info,2020-06-16T21:33:50.505Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 925 state to active
[ns_server:info,2020-06-16T21:33:50.505Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 924 state to active
[ns_server:info,2020-06-16T21:33:50.506Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 923 state to active
[ns_server:info,2020-06-16T21:33:50.507Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 922 state to active
[ns_server:info,2020-06-16T21:33:50.508Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 921 state to active
[ns_server:info,2020-06-16T21:33:50.509Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 920 state to active
[ns_server:info,2020-06-16T21:33:50.510Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 919 state to active
[ns_server:info,2020-06-16T21:33:50.514Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 918 state to active
[ns_server:info,2020-06-16T21:33:50.516Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 917 state to active
[ns_server:info,2020-06-16T21:33:50.518Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 916 state to active
[ns_server:info,2020-06-16T21:33:50.522Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 915 state to active
[ns_server:info,2020-06-16T21:33:50.522Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 914 state to active
[ns_server:info,2020-06-16T21:33:50.531Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 913 state to active
[ns_server:info,2020-06-16T21:33:50.538Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 912 state to active
[ns_server:info,2020-06-16T21:33:50.539Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 911 state to active
[ns_server:info,2020-06-16T21:33:50.543Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 910 state to active
[ns_server:info,2020-06-16T21:33:50.548Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 909 state to active
[ns_server:info,2020-06-16T21:33:50.558Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 908 state to active
[ns_server:info,2020-06-16T21:33:50.585Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 907 state to active
[ns_server:info,2020-06-16T21:33:50.596Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 906 state to active
[ns_server:info,2020-06-16T21:33:50.598Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 905 state to active
[ns_server:info,2020-06-16T21:33:50.601Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 904 state to active
[ns_server:info,2020-06-16T21:33:50.607Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 903 state to active
[ns_server:info,2020-06-16T21:33:50.617Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 902 state to active
[ns_server:info,2020-06-16T21:33:50.620Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 901 state to active
[ns_server:info,2020-06-16T21:33:50.621Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 900 state to active
[ns_server:info,2020-06-16T21:33:50.623Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 899 state to active
[ns_server:info,2020-06-16T21:33:50.624Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 898 state to active
[ns_server:info,2020-06-16T21:33:50.625Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 897 state to active
[ns_server:info,2020-06-16T21:33:50.626Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 896 state to active
[ns_server:info,2020-06-16T21:33:50.637Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 895 state to active
[ns_server:info,2020-06-16T21:33:50.638Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 894 state to active
[ns_server:info,2020-06-16T21:33:50.638Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 893 state to active
[ns_server:info,2020-06-16T21:33:50.639Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 892 state to active
[ns_server:info,2020-06-16T21:33:50.646Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 891 state to active
[ns_server:info,2020-06-16T21:33:50.648Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 890 state to active
[ns_server:info,2020-06-16T21:33:50.649Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 889 state to active
[ns_server:info,2020-06-16T21:33:50.650Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 888 state to active
[ns_server:info,2020-06-16T21:33:50.651Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 887 state to active
[ns_server:info,2020-06-16T21:33:50.652Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 886 state to active
[ns_server:info,2020-06-16T21:33:50.653Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 885 state to active
[ns_server:info,2020-06-16T21:33:50.654Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 884 state to active
[ns_server:info,2020-06-16T21:33:50.655Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 883 state to active
[ns_server:info,2020-06-16T21:33:50.665Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 882 state to active
[ns_server:info,2020-06-16T21:33:50.667Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 881 state to active
[ns_server:info,2020-06-16T21:33:50.668Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 880 state to active
[ns_server:info,2020-06-16T21:33:50.669Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 879 state to active
[ns_server:info,2020-06-16T21:33:50.691Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 878 state to active
[ns_server:info,2020-06-16T21:33:50.698Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 877 state to active
[ns_server:info,2020-06-16T21:33:50.700Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 876 state to active
[ns_server:info,2020-06-16T21:33:50.700Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 875 state to active
[ns_server:info,2020-06-16T21:33:50.701Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 874 state to active
[ns_server:info,2020-06-16T21:33:50.701Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 873 state to active
[ns_server:info,2020-06-16T21:33:50.703Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 872 state to active
[ns_server:info,2020-06-16T21:33:50.706Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 871 state to active
[ns_server:info,2020-06-16T21:33:50.707Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 870 state to active
[ns_server:info,2020-06-16T21:33:50.708Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 869 state to active
[ns_server:info,2020-06-16T21:33:50.710Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 868 state to active
[ns_server:info,2020-06-16T21:33:50.715Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 867 state to active
[ns_server:info,2020-06-16T21:33:50.717Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 866 state to active
[ns_server:info,2020-06-16T21:33:50.720Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 865 state to active
[ns_server:info,2020-06-16T21:33:50.721Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 864 state to active
[ns_server:info,2020-06-16T21:33:50.722Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 863 state to active
[ns_server:info,2020-06-16T21:33:50.722Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 862 state to active
[ns_server:info,2020-06-16T21:33:50.723Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 861 state to active
[ns_server:info,2020-06-16T21:33:50.731Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 860 state to active
[ns_server:info,2020-06-16T21:33:50.733Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 859 state to active
[ns_server:info,2020-06-16T21:33:50.736Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 858 state to active
[ns_server:info,2020-06-16T21:33:50.753Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 857 state to active
[ns_server:info,2020-06-16T21:33:50.755Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 856 state to active
[ns_server:info,2020-06-16T21:33:50.757Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 855 state to active
[ns_server:info,2020-06-16T21:33:50.763Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 854 state to active
[ns_server:info,2020-06-16T21:33:50.767Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 853 state to active
[ns_server:info,2020-06-16T21:33:50.771Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 852 state to active
[ns_server:info,2020-06-16T21:33:50.786Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 851 state to active
[ns_server:info,2020-06-16T21:33:50.787Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 850 state to active
[ns_server:info,2020-06-16T21:33:50.788Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 849 state to active
[ns_server:info,2020-06-16T21:33:50.792Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 848 state to active
[ns_server:info,2020-06-16T21:33:50.797Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 847 state to active
[ns_server:info,2020-06-16T21:33:50.800Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 846 state to active
[ns_server:info,2020-06-16T21:33:50.809Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 845 state to active
[ns_server:info,2020-06-16T21:33:50.813Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 844 state to active
[ns_server:info,2020-06-16T21:33:50.816Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 843 state to active
[ns_server:info,2020-06-16T21:33:50.817Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 842 state to active
[ns_server:info,2020-06-16T21:33:50.818Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 841 state to active
[ns_server:info,2020-06-16T21:33:50.819Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 840 state to active
[ns_server:info,2020-06-16T21:33:50.827Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 839 state to active
[ns_server:info,2020-06-16T21:33:50.832Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 838 state to active
[ns_server:info,2020-06-16T21:33:50.834Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 837 state to active
[ns_server:info,2020-06-16T21:33:50.839Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 836 state to active
[ns_server:info,2020-06-16T21:33:50.839Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 835 state to active
[ns_server:info,2020-06-16T21:33:50.840Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 834 state to active
[ns_server:info,2020-06-16T21:33:50.841Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 833 state to active
[ns_server:info,2020-06-16T21:33:50.846Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 832 state to active
[ns_server:info,2020-06-16T21:33:50.852Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 831 state to active
[ns_server:info,2020-06-16T21:33:50.854Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 830 state to active
[ns_server:info,2020-06-16T21:33:50.855Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 829 state to active
[ns_server:info,2020-06-16T21:33:50.857Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 828 state to active
[ns_server:info,2020-06-16T21:33:50.863Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 827 state to active
[ns_server:info,2020-06-16T21:33:50.867Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 826 state to active
[ns_server:info,2020-06-16T21:33:50.875Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 825 state to active
[ns_server:info,2020-06-16T21:33:50.876Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 824 state to active
[ns_server:info,2020-06-16T21:33:50.888Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 823 state to active
[ns_server:info,2020-06-16T21:33:50.913Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 822 state to active
[ns_server:info,2020-06-16T21:33:50.916Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 821 state to active
[ns_server:info,2020-06-16T21:33:50.919Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 820 state to active
[ns_server:info,2020-06-16T21:33:50.921Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 819 state to active
[ns_server:info,2020-06-16T21:33:50.922Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 818 state to active
[ns_server:info,2020-06-16T21:33:50.926Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 817 state to active
[ns_server:info,2020-06-16T21:33:50.937Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 816 state to active
[ns_server:info,2020-06-16T21:33:50.939Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 815 state to active
[ns_server:info,2020-06-16T21:33:50.940Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 814 state to active
[ns_server:info,2020-06-16T21:33:50.941Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 813 state to active
[ns_server:info,2020-06-16T21:33:50.943Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 812 state to active
[ns_server:info,2020-06-16T21:33:50.947Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 811 state to active
[ns_server:info,2020-06-16T21:33:50.954Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 810 state to active
[ns_server:info,2020-06-16T21:33:50.955Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 809 state to active
[ns_server:info,2020-06-16T21:33:50.955Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 808 state to active
[ns_server:info,2020-06-16T21:33:50.957Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 807 state to active
[ns_server:info,2020-06-16T21:33:50.958Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 806 state to active
[ns_server:info,2020-06-16T21:33:50.959Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 805 state to active
[ns_server:info,2020-06-16T21:33:50.963Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 804 state to active
[ns_server:info,2020-06-16T21:33:50.965Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 803 state to active
[ns_server:info,2020-06-16T21:33:50.969Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 802 state to active
[ns_server:info,2020-06-16T21:33:50.971Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 801 state to active
[ns_server:info,2020-06-16T21:33:50.973Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 800 state to active
[ns_server:info,2020-06-16T21:33:50.975Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 799 state to active
[ns_server:info,2020-06-16T21:33:50.977Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 798 state to active
[ns_server:info,2020-06-16T21:33:50.983Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 797 state to active
[ns_server:info,2020-06-16T21:33:50.985Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 796 state to active
[ns_server:info,2020-06-16T21:33:50.985Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 795 state to active
[ns_server:info,2020-06-16T21:33:50.986Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 794 state to active
[ns_server:info,2020-06-16T21:33:50.987Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 793 state to active
[ns_server:info,2020-06-16T21:33:50.988Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 792 state to active
[ns_server:info,2020-06-16T21:33:50.989Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 791 state to active
[ns_server:info,2020-06-16T21:33:50.992Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 790 state to active
[ns_server:info,2020-06-16T21:33:50.994Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 789 state to active
[ns_server:info,2020-06-16T21:33:51.000Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 788 state to active
[ns_server:info,2020-06-16T21:33:51.002Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 787 state to active
[ns_server:info,2020-06-16T21:33:51.006Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 786 state to active
[ns_server:info,2020-06-16T21:33:51.008Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 785 state to active
[ns_server:info,2020-06-16T21:33:51.009Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 784 state to active
[ns_server:info,2020-06-16T21:33:51.010Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 783 state to active
[ns_server:info,2020-06-16T21:33:51.012Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 782 state to active
[ns_server:info,2020-06-16T21:33:51.013Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 781 state to active
[ns_server:info,2020-06-16T21:33:51.016Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 780 state to active
[ns_server:info,2020-06-16T21:33:51.017Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 779 state to active
[ns_server:info,2020-06-16T21:33:51.018Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 778 state to active
[ns_server:info,2020-06-16T21:33:51.018Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 777 state to active
[ns_server:info,2020-06-16T21:33:51.019Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 776 state to active
[ns_server:info,2020-06-16T21:33:51.022Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 775 state to active
[ns_server:info,2020-06-16T21:33:51.022Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 774 state to active
[ns_server:info,2020-06-16T21:33:51.023Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 773 state to active
[ns_server:info,2020-06-16T21:33:51.026Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 772 state to active
[ns_server:info,2020-06-16T21:33:51.029Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 771 state to active
[ns_server:info,2020-06-16T21:33:51.030Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 770 state to active
[ns_server:info,2020-06-16T21:33:51.031Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 769 state to active
[ns_server:info,2020-06-16T21:33:51.032Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 768 state to active
[ns_server:info,2020-06-16T21:33:51.034Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 767 state to active
[ns_server:info,2020-06-16T21:33:51.036Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 766 state to active
[ns_server:info,2020-06-16T21:33:51.037Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 765 state to active
[ns_server:info,2020-06-16T21:33:51.039Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 764 state to active
[ns_server:info,2020-06-16T21:33:51.041Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 763 state to active
[ns_server:info,2020-06-16T21:33:51.043Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 762 state to active
[ns_server:info,2020-06-16T21:33:51.045Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 761 state to active
[ns_server:info,2020-06-16T21:33:51.058Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 760 state to active
[ns_server:info,2020-06-16T21:33:51.058Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 759 state to active
[ns_server:info,2020-06-16T21:33:51.060Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 758 state to active
[ns_server:info,2020-06-16T21:33:51.065Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 757 state to active
[ns_server:info,2020-06-16T21:33:51.070Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 756 state to active
[ns_server:info,2020-06-16T21:33:51.071Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 755 state to active
[ns_server:info,2020-06-16T21:33:51.072Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 754 state to active
[ns_server:info,2020-06-16T21:33:51.073Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 753 state to active
[ns_server:info,2020-06-16T21:33:51.073Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 752 state to active
[ns_server:info,2020-06-16T21:33:51.074Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 751 state to active
[ns_server:info,2020-06-16T21:33:51.075Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 750 state to active
[ns_server:info,2020-06-16T21:33:51.080Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 749 state to active
[ns_server:info,2020-06-16T21:33:51.082Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 748 state to active
[ns_server:info,2020-06-16T21:33:51.083Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 747 state to active
[ns_server:info,2020-06-16T21:33:51.084Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 746 state to active
[ns_server:info,2020-06-16T21:33:51.085Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 745 state to active
[ns_server:info,2020-06-16T21:33:51.086Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 744 state to active
[ns_server:info,2020-06-16T21:33:51.086Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 743 state to active
[ns_server:info,2020-06-16T21:33:51.087Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 742 state to active
[ns_server:info,2020-06-16T21:33:51.088Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 741 state to active
[ns_server:info,2020-06-16T21:33:51.089Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 740 state to active
[ns_server:info,2020-06-16T21:33:51.090Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 739 state to active
[ns_server:info,2020-06-16T21:33:51.090Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 738 state to active
[ns_server:info,2020-06-16T21:33:51.091Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 737 state to active
[ns_server:info,2020-06-16T21:33:51.092Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 736 state to active
[ns_server:info,2020-06-16T21:33:51.092Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 735 state to active
[ns_server:info,2020-06-16T21:33:51.093Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 734 state to active
[ns_server:info,2020-06-16T21:33:51.094Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 733 state to active
[ns_server:info,2020-06-16T21:33:51.095Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 732 state to active
[ns_server:info,2020-06-16T21:33:51.096Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 731 state to active
[ns_server:info,2020-06-16T21:33:51.099Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 730 state to active
[ns_server:info,2020-06-16T21:33:51.100Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 729 state to active
[ns_server:info,2020-06-16T21:33:51.101Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 728 state to active
[ns_server:info,2020-06-16T21:33:51.102Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 727 state to active
[ns_server:info,2020-06-16T21:33:51.102Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 726 state to active
[ns_server:info,2020-06-16T21:33:51.103Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 725 state to active
[ns_server:info,2020-06-16T21:33:51.104Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 724 state to active
[ns_server:info,2020-06-16T21:33:51.107Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 723 state to active
[ns_server:info,2020-06-16T21:33:51.108Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 722 state to active
[ns_server:info,2020-06-16T21:33:51.109Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 721 state to active
[ns_server:info,2020-06-16T21:33:51.110Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 720 state to active
[ns_server:info,2020-06-16T21:33:51.111Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 719 state to active
[ns_server:info,2020-06-16T21:33:51.113Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 718 state to active
[ns_server:info,2020-06-16T21:33:51.115Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 717 state to active
[ns_server:info,2020-06-16T21:33:51.117Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 716 state to active
[ns_server:info,2020-06-16T21:33:51.118Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 715 state to active
[ns_server:info,2020-06-16T21:33:51.120Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 714 state to active
[ns_server:info,2020-06-16T21:33:51.122Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 713 state to active
[ns_server:info,2020-06-16T21:33:51.124Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 712 state to active
[ns_server:info,2020-06-16T21:33:51.125Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 711 state to active
[ns_server:info,2020-06-16T21:33:51.127Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 710 state to active
[ns_server:info,2020-06-16T21:33:51.128Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 709 state to active
[ns_server:info,2020-06-16T21:33:51.135Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 708 state to active
[ns_server:info,2020-06-16T21:33:51.137Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 707 state to active
[ns_server:info,2020-06-16T21:33:51.141Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 706 state to active
[ns_server:info,2020-06-16T21:33:51.143Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 705 state to active
[ns_server:info,2020-06-16T21:33:51.144Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 704 state to active
[ns_server:info,2020-06-16T21:33:51.145Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 703 state to active
[ns_server:info,2020-06-16T21:33:51.146Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 702 state to active
[ns_server:info,2020-06-16T21:33:51.147Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 701 state to active
[ns_server:info,2020-06-16T21:33:51.148Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 700 state to active
[ns_server:info,2020-06-16T21:33:51.150Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 699 state to active
[ns_server:info,2020-06-16T21:33:51.151Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 698 state to active
[ns_server:info,2020-06-16T21:33:51.152Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 697 state to active
[ns_server:info,2020-06-16T21:33:51.153Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 696 state to active
[ns_server:info,2020-06-16T21:33:51.154Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 695 state to active
[ns_server:info,2020-06-16T21:33:51.155Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 694 state to active
[ns_server:info,2020-06-16T21:33:51.156Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 693 state to active
[ns_server:info,2020-06-16T21:33:51.157Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 692 state to active
[ns_server:info,2020-06-16T21:33:51.158Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 691 state to active
[ns_server:info,2020-06-16T21:33:51.159Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 690 state to active
[ns_server:info,2020-06-16T21:33:51.160Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 689 state to active
[ns_server:info,2020-06-16T21:33:51.162Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 688 state to active
[ns_server:info,2020-06-16T21:33:51.163Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 687 state to active
[ns_server:info,2020-06-16T21:33:51.169Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 686 state to active
[ns_server:info,2020-06-16T21:33:51.170Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 685 state to active
[ns_server:info,2020-06-16T21:33:51.171Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 684 state to active
[ns_server:info,2020-06-16T21:33:51.171Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 683 state to active
[ns_server:info,2020-06-16T21:33:51.173Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 682 state to active
[ns_server:info,2020-06-16T21:33:51.174Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 681 state to active
[ns_server:info,2020-06-16T21:33:51.176Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 680 state to active
[ns_server:info,2020-06-16T21:33:51.177Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 679 state to active
[ns_server:info,2020-06-16T21:33:51.178Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 678 state to active
[ns_server:info,2020-06-16T21:33:51.180Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 677 state to active
[ns_server:info,2020-06-16T21:33:51.185Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 676 state to active
[ns_server:info,2020-06-16T21:33:51.185Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 675 state to active
[ns_server:info,2020-06-16T21:33:51.186Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 674 state to active
[ns_server:info,2020-06-16T21:33:51.187Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 673 state to active
[ns_server:info,2020-06-16T21:33:51.188Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 672 state to active
[ns_server:info,2020-06-16T21:33:51.190Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 671 state to active
[ns_server:info,2020-06-16T21:33:51.191Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 670 state to active
[ns_server:info,2020-06-16T21:33:51.194Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 669 state to active
[ns_server:info,2020-06-16T21:33:51.195Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 668 state to active
[ns_server:info,2020-06-16T21:33:51.195Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 667 state to active
[ns_server:info,2020-06-16T21:33:51.196Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 666 state to active
[ns_server:info,2020-06-16T21:33:51.200Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 665 state to active
[ns_server:info,2020-06-16T21:33:51.201Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 664 state to active
[ns_server:info,2020-06-16T21:33:51.202Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 663 state to active
[ns_server:info,2020-06-16T21:33:51.204Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 662 state to active
[ns_server:info,2020-06-16T21:33:51.205Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 661 state to active
[ns_server:info,2020-06-16T21:33:51.206Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 660 state to active
[ns_server:info,2020-06-16T21:33:51.207Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 659 state to active
[ns_server:info,2020-06-16T21:33:51.211Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 658 state to active
[ns_server:info,2020-06-16T21:33:51.213Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 657 state to active
[ns_server:info,2020-06-16T21:33:51.214Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 656 state to active
[ns_server:info,2020-06-16T21:33:51.216Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 655 state to active
[ns_server:info,2020-06-16T21:33:51.218Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 654 state to active
[ns_server:info,2020-06-16T21:33:51.219Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 653 state to active
[ns_server:info,2020-06-16T21:33:51.223Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 652 state to active
[ns_server:info,2020-06-16T21:33:51.226Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 651 state to active
[ns_server:info,2020-06-16T21:33:51.234Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 650 state to active
[ns_server:info,2020-06-16T21:33:51.237Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 649 state to active
[ns_server:info,2020-06-16T21:33:51.242Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 648 state to active
[ns_server:info,2020-06-16T21:33:51.245Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 647 state to active
[ns_server:info,2020-06-16T21:33:51.245Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 646 state to active
[ns_server:info,2020-06-16T21:33:51.247Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 645 state to active
[ns_server:info,2020-06-16T21:33:51.249Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 644 state to active
[ns_server:info,2020-06-16T21:33:51.250Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 643 state to active
[ns_server:info,2020-06-16T21:33:51.254Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 642 state to active
[ns_server:info,2020-06-16T21:33:51.255Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 641 state to active
[ns_server:info,2020-06-16T21:33:51.255Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 640 state to active
[ns_server:info,2020-06-16T21:33:51.256Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 639 state to active
[ns_server:info,2020-06-16T21:33:51.262Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 638 state to active
[ns_server:info,2020-06-16T21:33:51.264Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 637 state to active
[ns_server:info,2020-06-16T21:33:51.266Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 636 state to active
[ns_server:info,2020-06-16T21:33:51.269Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 635 state to active
[ns_server:info,2020-06-16T21:33:51.271Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 634 state to active
[ns_server:info,2020-06-16T21:33:51.272Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 633 state to active
[ns_server:info,2020-06-16T21:33:51.273Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 632 state to active
[ns_server:info,2020-06-16T21:33:51.274Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 631 state to active
[ns_server:info,2020-06-16T21:33:51.275Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 630 state to active
[ns_server:info,2020-06-16T21:33:51.276Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 629 state to active
[ns_server:info,2020-06-16T21:33:51.277Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 628 state to active
[ns_server:info,2020-06-16T21:33:51.279Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 627 state to active
[ns_server:info,2020-06-16T21:33:51.279Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 626 state to active
[ns_server:info,2020-06-16T21:33:51.281Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 625 state to active
[ns_server:info,2020-06-16T21:33:51.283Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 624 state to active
[ns_server:info,2020-06-16T21:33:51.285Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 623 state to active
[ns_server:info,2020-06-16T21:33:51.287Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 622 state to active
[ns_server:info,2020-06-16T21:33:51.288Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 621 state to active
[ns_server:info,2020-06-16T21:33:51.293Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 620 state to active
[ns_server:info,2020-06-16T21:33:51.294Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 619 state to active
[ns_server:info,2020-06-16T21:33:51.295Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 618 state to active
[ns_server:info,2020-06-16T21:33:51.296Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 617 state to active
[ns_server:info,2020-06-16T21:33:51.297Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 616 state to active
[ns_server:info,2020-06-16T21:33:51.298Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 615 state to active
[ns_server:info,2020-06-16T21:33:51.299Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 614 state to active
[ns_server:info,2020-06-16T21:33:51.300Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 613 state to active
[ns_server:info,2020-06-16T21:33:51.305Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 612 state to active
[ns_server:info,2020-06-16T21:33:51.309Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 611 state to active
[ns_server:info,2020-06-16T21:33:51.310Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 610 state to active
[ns_server:info,2020-06-16T21:33:51.311Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 609 state to active
[ns_server:info,2020-06-16T21:33:51.312Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 608 state to active
[ns_server:info,2020-06-16T21:33:51.312Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 607 state to active
[ns_server:info,2020-06-16T21:33:51.314Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 606 state to active
[ns_server:info,2020-06-16T21:33:51.320Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 605 state to active
[ns_server:info,2020-06-16T21:33:51.321Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 604 state to active
[ns_server:info,2020-06-16T21:33:51.324Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 603 state to active
[ns_server:info,2020-06-16T21:33:51.325Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 602 state to active
[ns_server:info,2020-06-16T21:33:51.326Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 601 state to active
[ns_server:info,2020-06-16T21:33:51.327Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 600 state to active
[ns_server:info,2020-06-16T21:33:51.328Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 599 state to active
[ns_server:info,2020-06-16T21:33:51.330Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 598 state to active
[ns_server:info,2020-06-16T21:33:51.332Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 597 state to active
[ns_server:debug,2020-06-16T21:33:51.332Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:info,2020-06-16T21:33:51.334Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 596 state to active
[ns_server:info,2020-06-16T21:33:51.337Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 595 state to active
[ns_server:info,2020-06-16T21:33:51.338Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 594 state to active
[ns_server:info,2020-06-16T21:33:51.339Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 593 state to active
[ns_server:info,2020-06-16T21:33:51.339Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 592 state to active
[ns_server:info,2020-06-16T21:33:51.340Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 591 state to active
[ns_server:info,2020-06-16T21:33:51.344Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 590 state to active
[ns_server:info,2020-06-16T21:33:51.345Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 589 state to active
[ns_server:info,2020-06-16T21:33:51.353Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 588 state to active
[ns_server:info,2020-06-16T21:33:51.354Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 587 state to active
[ns_server:info,2020-06-16T21:33:51.355Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 586 state to active
[ns_server:info,2020-06-16T21:33:51.355Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 585 state to active
[ns_server:info,2020-06-16T21:33:51.356Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 584 state to active
[ns_server:info,2020-06-16T21:33:51.358Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 583 state to active
[ns_server:info,2020-06-16T21:33:51.358Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 582 state to active
[ns_server:info,2020-06-16T21:33:51.359Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 581 state to active
[ns_server:info,2020-06-16T21:33:51.360Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 580 state to active
[ns_server:info,2020-06-16T21:33:51.361Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 579 state to active
[ns_server:info,2020-06-16T21:33:51.362Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 578 state to active
[ns_server:info,2020-06-16T21:33:51.363Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 577 state to active
[ns_server:info,2020-06-16T21:33:51.364Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 576 state to active
[ns_server:info,2020-06-16T21:33:51.364Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 575 state to active
[ns_server:info,2020-06-16T21:33:51.365Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 574 state to active
[ns_server:info,2020-06-16T21:33:51.366Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 573 state to active
[ns_server:info,2020-06-16T21:33:51.367Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 572 state to active
[ns_server:info,2020-06-16T21:33:51.368Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 571 state to active
[ns_server:info,2020-06-16T21:33:51.369Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 570 state to active
[ns_server:info,2020-06-16T21:33:51.371Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 569 state to active
[ns_server:info,2020-06-16T21:33:51.372Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 568 state to active
[ns_server:info,2020-06-16T21:33:51.372Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 567 state to active
[ns_server:info,2020-06-16T21:33:51.373Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 566 state to active
[ns_server:info,2020-06-16T21:33:51.382Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 565 state to active
[ns_server:info,2020-06-16T21:33:51.383Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 564 state to active
[ns_server:info,2020-06-16T21:33:51.384Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 563 state to active
[ns_server:info,2020-06-16T21:33:51.385Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 562 state to active
[ns_server:info,2020-06-16T21:33:51.386Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 561 state to active
[ns_server:info,2020-06-16T21:33:51.387Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 560 state to active
[ns_server:info,2020-06-16T21:33:51.388Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 559 state to active
[ns_server:info,2020-06-16T21:33:51.390Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 558 state to active
[ns_server:info,2020-06-16T21:33:51.391Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 557 state to active
[ns_server:info,2020-06-16T21:33:51.392Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 556 state to active
[ns_server:info,2020-06-16T21:33:51.393Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 555 state to active
[ns_server:info,2020-06-16T21:33:51.393Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 554 state to active
[ns_server:info,2020-06-16T21:33:51.394Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 553 state to active
[ns_server:info,2020-06-16T21:33:51.395Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 552 state to active
[ns_server:info,2020-06-16T21:33:51.403Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 551 state to active
[ns_server:info,2020-06-16T21:33:51.404Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 550 state to active
[ns_server:info,2020-06-16T21:33:51.405Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 549 state to active
[ns_server:info,2020-06-16T21:33:51.406Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 548 state to active
[ns_server:info,2020-06-16T21:33:51.414Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 547 state to active
[ns_server:info,2020-06-16T21:33:51.415Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 546 state to active
[ns_server:info,2020-06-16T21:33:51.419Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 545 state to active
[ns_server:info,2020-06-16T21:33:51.419Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 544 state to active
[ns_server:info,2020-06-16T21:33:51.420Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 543 state to active
[ns_server:info,2020-06-16T21:33:51.422Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 542 state to active
[ns_server:info,2020-06-16T21:33:51.424Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 541 state to active
[ns_server:info,2020-06-16T21:33:51.426Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 540 state to active
[ns_server:info,2020-06-16T21:33:51.428Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 539 state to active
[ns_server:info,2020-06-16T21:33:51.433Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 538 state to active
[ns_server:info,2020-06-16T21:33:51.435Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 537 state to active
[ns_server:info,2020-06-16T21:33:51.444Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 536 state to active
[ns_server:info,2020-06-16T21:33:51.445Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 535 state to active
[ns_server:info,2020-06-16T21:33:51.449Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 534 state to active
[ns_server:info,2020-06-16T21:33:51.450Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 533 state to active
[ns_server:info,2020-06-16T21:33:51.451Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 532 state to active
[ns_server:info,2020-06-16T21:33:51.452Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 531 state to active
[ns_server:info,2020-06-16T21:33:51.459Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 530 state to active
[ns_server:info,2020-06-16T21:33:51.463Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 529 state to active
[ns_server:info,2020-06-16T21:33:51.464Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 528 state to active
[ns_server:info,2020-06-16T21:33:51.465Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 527 state to active
[ns_server:info,2020-06-16T21:33:51.469Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 526 state to active
[ns_server:info,2020-06-16T21:33:51.470Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 525 state to active
[ns_server:info,2020-06-16T21:33:51.475Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 524 state to active
[ns_server:info,2020-06-16T21:33:51.477Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 523 state to active
[ns_server:info,2020-06-16T21:33:51.478Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 522 state to active
[ns_server:info,2020-06-16T21:33:51.480Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 521 state to active
[ns_server:info,2020-06-16T21:33:51.481Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 520 state to active
[ns_server:info,2020-06-16T21:33:51.488Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 519 state to active
[ns_server:info,2020-06-16T21:33:51.490Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 518 state to active
[ns_server:info,2020-06-16T21:33:51.491Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 517 state to active
[ns_server:info,2020-06-16T21:33:51.492Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 516 state to active
[ns_server:info,2020-06-16T21:33:51.494Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 515 state to active
[ns_server:info,2020-06-16T21:33:51.502Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 514 state to active
[ns_server:info,2020-06-16T21:33:51.503Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 513 state to active
[ns_server:info,2020-06-16T21:33:51.504Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 512 state to active
[ns_server:info,2020-06-16T21:33:51.505Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 511 state to active
[ns_server:info,2020-06-16T21:33:51.507Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 510 state to active
[ns_server:info,2020-06-16T21:33:51.508Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 509 state to active
[ns_server:info,2020-06-16T21:33:51.510Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 508 state to active
[ns_server:info,2020-06-16T21:33:51.511Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 507 state to active
[ns_server:info,2020-06-16T21:33:51.512Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 506 state to active
[ns_server:info,2020-06-16T21:33:51.521Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 505 state to active
[ns_server:info,2020-06-16T21:33:51.523Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 504 state to active
[ns_server:info,2020-06-16T21:33:51.524Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 503 state to active
[ns_server:info,2020-06-16T21:33:51.528Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 502 state to active
[ns_server:info,2020-06-16T21:33:51.529Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 501 state to active
[ns_server:info,2020-06-16T21:33:51.532Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 500 state to active
[ns_server:info,2020-06-16T21:33:51.535Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 499 state to active
[ns_server:info,2020-06-16T21:33:51.536Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 498 state to active
[ns_server:info,2020-06-16T21:33:51.545Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 497 state to active
[ns_server:info,2020-06-16T21:33:51.546Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 496 state to active
[ns_server:info,2020-06-16T21:33:51.548Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 495 state to active
[ns_server:info,2020-06-16T21:33:51.551Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 494 state to active
[ns_server:info,2020-06-16T21:33:51.552Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 493 state to active
[ns_server:info,2020-06-16T21:33:51.558Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 492 state to active
[ns_server:info,2020-06-16T21:33:51.564Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 491 state to active
[ns_server:info,2020-06-16T21:33:51.567Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 490 state to active
[ns_server:info,2020-06-16T21:33:51.568Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 489 state to active
[ns_server:info,2020-06-16T21:33:51.570Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 488 state to active
[ns_server:info,2020-06-16T21:33:51.573Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 487 state to active
[ns_server:info,2020-06-16T21:33:51.576Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 486 state to active
[ns_server:info,2020-06-16T21:33:51.584Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 485 state to active
[ns_server:info,2020-06-16T21:33:51.587Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 484 state to active
[ns_server:info,2020-06-16T21:33:51.588Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 483 state to active
[ns_server:info,2020-06-16T21:33:51.590Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 482 state to active
[ns_server:info,2020-06-16T21:33:51.591Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 481 state to active
[ns_server:info,2020-06-16T21:33:51.592Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 480 state to active
[ns_server:info,2020-06-16T21:33:51.597Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 479 state to active
[ns_server:info,2020-06-16T21:33:51.601Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 478 state to active
[ns_server:info,2020-06-16T21:33:51.609Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 477 state to active
[ns_server:info,2020-06-16T21:33:51.614Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 476 state to active
[ns_server:info,2020-06-16T21:33:51.616Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 475 state to active
[ns_server:info,2020-06-16T21:33:51.619Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 474 state to active
[ns_server:info,2020-06-16T21:33:51.620Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 473 state to active
[ns_server:info,2020-06-16T21:33:51.622Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 472 state to active
[ns_server:info,2020-06-16T21:33:51.624Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 471 state to active
[ns_server:info,2020-06-16T21:33:51.628Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 470 state to active
[ns_server:info,2020-06-16T21:33:51.629Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 469 state to active
[ns_server:info,2020-06-16T21:33:51.631Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 468 state to active
[ns_server:info,2020-06-16T21:33:51.635Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 467 state to active
[ns_server:info,2020-06-16T21:33:51.636Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 466 state to active
[ns_server:info,2020-06-16T21:33:51.640Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 465 state to active
[ns_server:info,2020-06-16T21:33:51.641Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 464 state to active
[ns_server:info,2020-06-16T21:33:51.644Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 463 state to active
[ns_server:info,2020-06-16T21:33:51.654Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 462 state to active
[ns_server:info,2020-06-16T21:33:51.657Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 461 state to active
[ns_server:info,2020-06-16T21:33:51.661Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 460 state to active
[ns_server:info,2020-06-16T21:33:51.662Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 459 state to active
[ns_server:info,2020-06-16T21:33:51.663Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 458 state to active
[ns_server:info,2020-06-16T21:33:51.665Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 457 state to active
[ns_server:info,2020-06-16T21:33:51.666Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 456 state to active
[ns_server:info,2020-06-16T21:33:51.668Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 455 state to active
[ns_server:info,2020-06-16T21:33:51.670Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 454 state to active
[ns_server:info,2020-06-16T21:33:51.673Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 453 state to active
[ns_server:info,2020-06-16T21:33:51.677Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 452 state to active
[ns_server:info,2020-06-16T21:33:51.681Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 451 state to active
[ns_server:info,2020-06-16T21:33:51.683Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 450 state to active
[ns_server:info,2020-06-16T21:33:51.684Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 449 state to active
[ns_server:info,2020-06-16T21:33:51.687Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 448 state to active
[ns_server:info,2020-06-16T21:33:51.689Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 447 state to active
[ns_server:info,2020-06-16T21:33:51.691Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 446 state to active
[ns_server:info,2020-06-16T21:33:51.695Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 445 state to active
[ns_server:info,2020-06-16T21:33:51.699Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 444 state to active
[ns_server:info,2020-06-16T21:33:51.701Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 443 state to active
[ns_server:info,2020-06-16T21:33:51.705Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 442 state to active
[ns_server:info,2020-06-16T21:33:51.709Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 441 state to active
[ns_server:info,2020-06-16T21:33:51.710Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 440 state to active
[ns_server:info,2020-06-16T21:33:51.712Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 439 state to active
[ns_server:info,2020-06-16T21:33:51.713Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 438 state to active
[ns_server:info,2020-06-16T21:33:51.714Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 437 state to active
[ns_server:info,2020-06-16T21:33:51.715Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 436 state to active
[ns_server:info,2020-06-16T21:33:51.717Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 435 state to active
[ns_server:info,2020-06-16T21:33:51.717Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 434 state to active
[ns_server:info,2020-06-16T21:33:51.719Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 433 state to active
[ns_server:info,2020-06-16T21:33:51.720Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 432 state to active
[ns_server:info,2020-06-16T21:33:51.721Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 431 state to active
[ns_server:info,2020-06-16T21:33:51.722Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 430 state to active
[ns_server:info,2020-06-16T21:33:51.723Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 429 state to active
[ns_server:info,2020-06-16T21:33:51.723Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 428 state to active
[ns_server:info,2020-06-16T21:33:51.725Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 427 state to active
[ns_server:info,2020-06-16T21:33:51.726Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 426 state to active
[ns_server:info,2020-06-16T21:33:51.728Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 425 state to active
[ns_server:info,2020-06-16T21:33:51.730Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 424 state to active
[ns_server:info,2020-06-16T21:33:51.731Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 423 state to active
[ns_server:info,2020-06-16T21:33:51.739Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 422 state to active
[ns_server:info,2020-06-16T21:33:51.740Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 421 state to active
[ns_server:info,2020-06-16T21:33:51.746Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 420 state to active
[ns_server:info,2020-06-16T21:33:51.749Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 419 state to active
[ns_server:info,2020-06-16T21:33:51.751Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 418 state to active
[ns_server:info,2020-06-16T21:33:51.752Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 417 state to active
[ns_server:info,2020-06-16T21:33:51.753Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 416 state to active
[ns_server:info,2020-06-16T21:33:51.754Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 415 state to active
[ns_server:info,2020-06-16T21:33:51.756Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 414 state to active
[ns_server:info,2020-06-16T21:33:51.758Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 413 state to active
[ns_server:info,2020-06-16T21:33:51.759Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 412 state to active
[ns_server:info,2020-06-16T21:33:51.760Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 411 state to active
[ns_server:info,2020-06-16T21:33:51.761Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 410 state to active
[ns_server:info,2020-06-16T21:33:51.762Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 409 state to active
[ns_server:info,2020-06-16T21:33:51.763Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 408 state to active
[ns_server:info,2020-06-16T21:33:51.765Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 407 state to active
[ns_server:info,2020-06-16T21:33:51.766Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 406 state to active
[ns_server:info,2020-06-16T21:33:51.767Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 405 state to active
[ns_server:info,2020-06-16T21:33:51.768Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 404 state to active
[ns_server:info,2020-06-16T21:33:51.769Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 403 state to active
[ns_server:info,2020-06-16T21:33:51.770Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 402 state to active
[ns_server:info,2020-06-16T21:33:51.772Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 401 state to active
[ns_server:info,2020-06-16T21:33:51.776Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 400 state to active
[ns_server:info,2020-06-16T21:33:51.777Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 399 state to active
[ns_server:info,2020-06-16T21:33:51.778Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 398 state to active
[ns_server:info,2020-06-16T21:33:51.780Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 397 state to active
[ns_server:info,2020-06-16T21:33:51.793Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 396 state to active
[ns_server:info,2020-06-16T21:33:51.795Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 395 state to active
[ns_server:info,2020-06-16T21:33:51.797Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 394 state to active
[ns_server:info,2020-06-16T21:33:51.798Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 393 state to active
[ns_server:info,2020-06-16T21:33:51.810Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 392 state to active
[ns_server:info,2020-06-16T21:33:51.815Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 391 state to active
[ns_server:info,2020-06-16T21:33:51.818Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 390 state to active
[ns_server:info,2020-06-16T21:33:51.819Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 389 state to active
[ns_server:info,2020-06-16T21:33:51.820Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 388 state to active
[ns_server:info,2020-06-16T21:33:51.825Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 387 state to active
[ns_server:info,2020-06-16T21:33:51.827Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 386 state to active
[ns_server:info,2020-06-16T21:33:51.828Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 385 state to active
[ns_server:info,2020-06-16T21:33:51.829Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 384 state to active
[ns_server:info,2020-06-16T21:33:51.830Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 383 state to active
[ns_server:info,2020-06-16T21:33:51.833Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 382 state to active
[ns_server:info,2020-06-16T21:33:51.834Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 381 state to active
[ns_server:info,2020-06-16T21:33:51.835Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 380 state to active
[ns_server:info,2020-06-16T21:33:51.837Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 379 state to active
[ns_server:info,2020-06-16T21:33:51.841Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 378 state to active
[ns_server:info,2020-06-16T21:33:51.844Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 377 state to active
[ns_server:info,2020-06-16T21:33:51.852Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 376 state to active
[ns_server:info,2020-06-16T21:33:51.854Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 375 state to active
[ns_server:info,2020-06-16T21:33:51.855Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 374 state to active
[ns_server:info,2020-06-16T21:33:51.856Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 373 state to active
[ns_server:info,2020-06-16T21:33:51.861Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 372 state to active
[ns_server:info,2020-06-16T21:33:51.863Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 371 state to active
[ns_server:info,2020-06-16T21:33:51.867Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 370 state to active
[ns_server:info,2020-06-16T21:33:51.868Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 369 state to active
[ns_server:info,2020-06-16T21:33:51.869Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 368 state to active
[ns_server:info,2020-06-16T21:33:51.871Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 367 state to active
[ns_server:info,2020-06-16T21:33:51.874Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 366 state to active
[ns_server:info,2020-06-16T21:33:51.876Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 365 state to active
[ns_server:info,2020-06-16T21:33:51.879Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 364 state to active
[ns_server:info,2020-06-16T21:33:51.887Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 363 state to active
[ns_server:info,2020-06-16T21:33:51.890Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 362 state to active
[ns_server:info,2020-06-16T21:33:51.892Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 361 state to active
[ns_server:info,2020-06-16T21:33:51.895Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 360 state to active
[ns_server:info,2020-06-16T21:33:51.897Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 359 state to active
[ns_server:info,2020-06-16T21:33:51.900Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 358 state to active
[ns_server:info,2020-06-16T21:33:51.901Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 357 state to active
[ns_server:info,2020-06-16T21:33:51.902Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 356 state to active
[ns_server:info,2020-06-16T21:33:51.905Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 355 state to active
[ns_server:info,2020-06-16T21:33:51.907Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 354 state to active
[ns_server:info,2020-06-16T21:33:51.909Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 353 state to active
[ns_server:info,2020-06-16T21:33:51.910Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 352 state to active
[ns_server:info,2020-06-16T21:33:51.911Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 351 state to active
[ns_server:info,2020-06-16T21:33:51.912Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 350 state to active
[ns_server:info,2020-06-16T21:33:51.914Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 349 state to active
[ns_server:info,2020-06-16T21:33:51.915Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 348 state to active
[ns_server:info,2020-06-16T21:33:51.916Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 347 state to active
[ns_server:info,2020-06-16T21:33:51.918Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 346 state to active
[ns_server:info,2020-06-16T21:33:51.920Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 345 state to active
[ns_server:info,2020-06-16T21:33:51.921Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 344 state to active
[ns_server:info,2020-06-16T21:33:51.923Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 343 state to active
[ns_server:info,2020-06-16T21:33:51.925Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 342 state to active
[ns_server:info,2020-06-16T21:33:51.933Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 341 state to active
[ns_server:info,2020-06-16T21:33:51.934Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 340 state to active
[ns_server:info,2020-06-16T21:33:51.936Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 339 state to active
[ns_server:info,2020-06-16T21:33:51.938Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 338 state to active
[ns_server:info,2020-06-16T21:33:51.939Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 337 state to active
[ns_server:info,2020-06-16T21:33:51.944Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 336 state to active
[ns_server:info,2020-06-16T21:33:51.946Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 335 state to active
[ns_server:info,2020-06-16T21:33:51.948Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 334 state to active
[ns_server:info,2020-06-16T21:33:52.030Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 333 state to active
[ns_server:info,2020-06-16T21:33:52.039Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 332 state to active
[ns_server:info,2020-06-16T21:33:52.068Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 331 state to active
[ns_server:info,2020-06-16T21:33:52.096Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 330 state to active
[ns_server:info,2020-06-16T21:33:52.101Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 329 state to active
[ns_server:info,2020-06-16T21:33:52.102Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 328 state to active
[ns_server:info,2020-06-16T21:33:52.105Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 327 state to active
[ns_server:info,2020-06-16T21:33:52.106Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 326 state to active
[ns_server:info,2020-06-16T21:33:52.120Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 325 state to active
[ns_server:info,2020-06-16T21:33:52.124Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 324 state to active
[ns_server:info,2020-06-16T21:33:52.134Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 323 state to active
[ns_server:info,2020-06-16T21:33:52.137Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 322 state to active
[ns_server:info,2020-06-16T21:33:52.138Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 321 state to active
[ns_server:info,2020-06-16T21:33:52.140Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 320 state to active
[ns_server:info,2020-06-16T21:33:52.141Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 319 state to active
[ns_server:info,2020-06-16T21:33:52.173Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 318 state to active
[ns_server:info,2020-06-16T21:33:52.174Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 317 state to active
[ns_server:info,2020-06-16T21:33:52.206Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 316 state to active
[ns_server:info,2020-06-16T21:33:52.213Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 315 state to active
[ns_server:info,2020-06-16T21:33:52.281Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 314 state to active
[ns_server:info,2020-06-16T21:33:52.283Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 313 state to active
[ns_server:info,2020-06-16T21:33:52.290Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 312 state to active
[ns_server:info,2020-06-16T21:33:52.298Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 311 state to active
[ns_server:info,2020-06-16T21:33:52.317Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 310 state to active
[ns_server:info,2020-06-16T21:33:52.334Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 309 state to active
[ns_server:info,2020-06-16T21:33:52.335Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 308 state to active
[ns_server:info,2020-06-16T21:33:52.339Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 307 state to active
[ns_server:debug,2020-06-16T21:33:52.339Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:info,2020-06-16T21:33:52.343Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 306 state to active
[ns_server:info,2020-06-16T21:33:52.385Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 305 state to active
[ns_server:info,2020-06-16T21:33:52.387Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 304 state to active
[ns_server:info,2020-06-16T21:33:52.417Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 303 state to active
[ns_server:info,2020-06-16T21:33:52.423Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 302 state to active
[ns_server:info,2020-06-16T21:33:52.434Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 301 state to active
[ns_server:info,2020-06-16T21:33:52.439Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 300 state to active
[ns_server:info,2020-06-16T21:33:52.468Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 299 state to active
[ns_server:info,2020-06-16T21:33:52.469Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 298 state to active
[ns_server:info,2020-06-16T21:33:52.471Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 297 state to active
[ns_server:info,2020-06-16T21:33:52.472Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 296 state to active
[ns_server:info,2020-06-16T21:33:52.483Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 295 state to active
[ns_server:info,2020-06-16T21:33:52.487Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 294 state to active
[ns_server:info,2020-06-16T21:33:52.489Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 293 state to active
[ns_server:info,2020-06-16T21:33:52.496Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 292 state to active
[ns_server:info,2020-06-16T21:33:52.497Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 291 state to active
[ns_server:info,2020-06-16T21:33:52.505Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 290 state to active
[ns_server:info,2020-06-16T21:33:52.507Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 289 state to active
[ns_server:info,2020-06-16T21:33:52.512Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 288 state to active
[ns_server:info,2020-06-16T21:33:52.514Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 287 state to active
[ns_server:info,2020-06-16T21:33:52.515Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 286 state to active
[ns_server:info,2020-06-16T21:33:52.516Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 285 state to active
[ns_server:info,2020-06-16T21:33:52.518Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 284 state to active
[ns_server:info,2020-06-16T21:33:52.520Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 283 state to active
[ns_server:info,2020-06-16T21:33:52.521Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 282 state to active
[ns_server:info,2020-06-16T21:33:52.522Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 281 state to active
[ns_server:info,2020-06-16T21:33:52.532Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 280 state to active
[ns_server:info,2020-06-16T21:33:52.534Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 279 state to active
[ns_server:info,2020-06-16T21:33:52.538Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 278 state to active
[ns_server:info,2020-06-16T21:33:52.539Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 277 state to active
[ns_server:info,2020-06-16T21:33:52.541Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 276 state to active
[ns_server:info,2020-06-16T21:33:52.549Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 275 state to active
[ns_server:info,2020-06-16T21:33:52.557Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 274 state to active
[ns_server:info,2020-06-16T21:33:52.565Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 273 state to active
[ns_server:info,2020-06-16T21:33:52.574Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 272 state to active
[ns_server:info,2020-06-16T21:33:52.580Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 271 state to active
[ns_server:info,2020-06-16T21:33:52.584Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 270 state to active
[ns_server:info,2020-06-16T21:33:52.587Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 269 state to active
[ns_server:info,2020-06-16T21:33:52.590Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 268 state to active
[ns_server:info,2020-06-16T21:33:52.602Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 267 state to active
[ns_server:info,2020-06-16T21:33:52.604Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 266 state to active
[ns_server:info,2020-06-16T21:33:52.616Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 265 state to active
[ns_server:info,2020-06-16T21:33:52.617Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 264 state to active
[ns_server:info,2020-06-16T21:33:52.619Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 263 state to active
[ns_server:info,2020-06-16T21:33:52.621Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 262 state to active
[ns_server:info,2020-06-16T21:33:52.634Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 261 state to active
[ns_server:info,2020-06-16T21:33:52.649Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 260 state to active
[ns_server:info,2020-06-16T21:33:52.651Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 259 state to active
[ns_server:info,2020-06-16T21:33:52.652Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 258 state to active
[ns_server:info,2020-06-16T21:33:52.654Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 257 state to active
[ns_server:info,2020-06-16T21:33:52.657Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 256 state to active
[ns_server:info,2020-06-16T21:33:52.665Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 255 state to active
[ns_server:info,2020-06-16T21:33:52.667Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 254 state to active
[ns_server:info,2020-06-16T21:33:52.669Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 253 state to active
[ns_server:info,2020-06-16T21:33:52.714Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 252 state to active
[ns_server:info,2020-06-16T21:33:52.717Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 251 state to active
[ns_server:info,2020-06-16T21:33:52.718Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 250 state to active
[ns_server:info,2020-06-16T21:33:52.744Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 249 state to active
[ns_server:info,2020-06-16T21:33:52.747Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 248 state to active
[ns_server:info,2020-06-16T21:33:52.754Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 247 state to active
[ns_server:info,2020-06-16T21:33:52.755Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 246 state to active
[ns_server:info,2020-06-16T21:33:52.768Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 245 state to active
[ns_server:info,2020-06-16T21:33:52.769Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 244 state to active
[ns_server:info,2020-06-16T21:33:52.772Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 243 state to active
[ns_server:info,2020-06-16T21:33:52.773Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 242 state to active
[ns_server:info,2020-06-16T21:33:52.778Z,ns_1@127.0.0.1:<0.3045.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 241 state to active
[ns_server:info,2020-06-16T21:33:52.779Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 240 state to active
[ns_server:info,2020-06-16T21:33:52.780Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 239 state to active
[ns_server:info,2020-06-16T21:33:52.781Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 238 state to active
[ns_server:info,2020-06-16T21:33:52.782Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 237 state to active
[ns_server:info,2020-06-16T21:33:52.786Z,ns_1@127.0.0.1:<0.3047.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 236 state to active
[ns_server:info,2020-06-16T21:33:52.787Z,ns_1@127.0.0.1:<0.3047.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 235 state to active
[ns_server:info,2020-06-16T21:33:52.788Z,ns_1@127.0.0.1:<0.3047.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 234 state to active
[ns_server:info,2020-06-16T21:33:52.797Z,ns_1@127.0.0.1:<0.3047.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 233 state to active
[ns_server:info,2020-06-16T21:33:52.800Z,ns_1@127.0.0.1:<0.3047.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 232 state to active
[ns_server:info,2020-06-16T21:33:52.805Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 231 state to active
[ns_server:info,2020-06-16T21:33:52.806Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 230 state to active
[ns_server:info,2020-06-16T21:33:52.811Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 229 state to active
[ns_server:info,2020-06-16T21:33:52.813Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 228 state to active
[ns_server:info,2020-06-16T21:33:52.814Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 227 state to active
[ns_server:info,2020-06-16T21:33:52.818Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 226 state to active
[ns_server:info,2020-06-16T21:33:52.827Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 225 state to active
[ns_server:info,2020-06-16T21:33:52.828Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 224 state to active
[ns_server:info,2020-06-16T21:33:52.829Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 223 state to active
[ns_server:info,2020-06-16T21:33:52.830Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 222 state to active
[ns_server:info,2020-06-16T21:33:52.832Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 221 state to active
[ns_server:info,2020-06-16T21:33:52.833Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 220 state to active
[ns_server:info,2020-06-16T21:33:52.834Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 219 state to active
[ns_server:info,2020-06-16T21:33:52.835Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 218 state to active
[ns_server:info,2020-06-16T21:33:52.836Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 217 state to active
[ns_server:info,2020-06-16T21:33:52.837Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 216 state to active
[ns_server:info,2020-06-16T21:33:52.843Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 215 state to active
[ns_server:info,2020-06-16T21:33:52.849Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 214 state to active
[ns_server:info,2020-06-16T21:33:52.850Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 213 state to active
[ns_server:info,2020-06-16T21:33:52.851Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 212 state to active
[ns_server:info,2020-06-16T21:33:52.852Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 211 state to active
[ns_server:info,2020-06-16T21:33:52.853Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 210 state to active
[ns_server:info,2020-06-16T21:33:52.858Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 209 state to active
[ns_server:info,2020-06-16T21:33:52.859Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 208 state to active
[ns_server:info,2020-06-16T21:33:52.861Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 207 state to active
[ns_server:info,2020-06-16T21:33:52.867Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 206 state to active
[ns_server:info,2020-06-16T21:33:52.868Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 205 state to active
[ns_server:info,2020-06-16T21:33:52.869Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 204 state to active
[ns_server:info,2020-06-16T21:33:52.870Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 203 state to active
[ns_server:info,2020-06-16T21:33:52.877Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 202 state to active
[ns_server:info,2020-06-16T21:33:52.878Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 201 state to active
[ns_server:info,2020-06-16T21:33:52.881Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 200 state to active
[ns_server:info,2020-06-16T21:33:52.882Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 199 state to active
[ns_server:info,2020-06-16T21:33:52.883Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 198 state to active
[ns_server:info,2020-06-16T21:33:52.885Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 197 state to active
[ns_server:info,2020-06-16T21:33:52.887Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 196 state to active
[ns_server:info,2020-06-16T21:33:52.894Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 195 state to active
[ns_server:info,2020-06-16T21:33:52.899Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 194 state to active
[ns_server:info,2020-06-16T21:33:52.902Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 193 state to active
[ns_server:info,2020-06-16T21:33:52.905Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 192 state to active
[ns_server:info,2020-06-16T21:33:52.910Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 191 state to active
[ns_server:info,2020-06-16T21:33:52.911Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 190 state to active
[ns_server:info,2020-06-16T21:33:52.912Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 189 state to active
[ns_server:info,2020-06-16T21:33:52.913Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 188 state to active
[ns_server:info,2020-06-16T21:33:52.914Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 187 state to active
[ns_server:info,2020-06-16T21:33:52.915Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 186 state to active
[ns_server:info,2020-06-16T21:33:52.916Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 185 state to active
[ns_server:info,2020-06-16T21:33:52.917Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 184 state to active
[ns_server:info,2020-06-16T21:33:52.920Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 183 state to active
[ns_server:info,2020-06-16T21:33:52.921Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 182 state to active
[ns_server:info,2020-06-16T21:33:52.927Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 181 state to active
[ns_server:info,2020-06-16T21:33:52.935Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 180 state to active
[ns_server:info,2020-06-16T21:33:52.942Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 179 state to active
[ns_server:info,2020-06-16T21:33:52.944Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 178 state to active
[ns_server:info,2020-06-16T21:33:52.946Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 177 state to active
[ns_server:info,2020-06-16T21:33:52.947Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 176 state to active
[ns_server:info,2020-06-16T21:33:52.948Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 175 state to active
[ns_server:info,2020-06-16T21:33:52.948Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 174 state to active
[ns_server:info,2020-06-16T21:33:52.949Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 173 state to active
[ns_server:info,2020-06-16T21:33:52.950Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 172 state to active
[ns_server:info,2020-06-16T21:33:52.951Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 171 state to active
[ns_server:info,2020-06-16T21:33:52.951Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 170 state to active
[ns_server:info,2020-06-16T21:33:52.952Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 169 state to active
[ns_server:info,2020-06-16T21:33:52.953Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 168 state to active
[ns_server:info,2020-06-16T21:33:52.954Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 167 state to active
[ns_server:info,2020-06-16T21:33:52.955Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 166 state to active
[ns_server:info,2020-06-16T21:33:52.959Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 165 state to active
[ns_server:info,2020-06-16T21:33:52.961Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 164 state to active
[ns_server:info,2020-06-16T21:33:52.962Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 163 state to active
[ns_server:info,2020-06-16T21:33:52.962Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 162 state to active
[ns_server:info,2020-06-16T21:33:52.964Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 161 state to active
[ns_server:info,2020-06-16T21:33:52.964Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 160 state to active
[ns_server:info,2020-06-16T21:33:52.965Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 159 state to active
[ns_server:info,2020-06-16T21:33:52.967Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 158 state to active
[ns_server:info,2020-06-16T21:33:52.968Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 157 state to active
[ns_server:info,2020-06-16T21:33:52.969Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 156 state to active
[ns_server:info,2020-06-16T21:33:52.970Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 155 state to active
[ns_server:info,2020-06-16T21:33:52.971Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 154 state to active
[ns_server:info,2020-06-16T21:33:52.972Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 153 state to active
[ns_server:info,2020-06-16T21:33:52.973Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 152 state to active
[ns_server:info,2020-06-16T21:33:52.975Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 151 state to active
[ns_server:info,2020-06-16T21:33:52.976Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 150 state to active
[ns_server:info,2020-06-16T21:33:52.976Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 149 state to active
[ns_server:info,2020-06-16T21:33:52.978Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 148 state to active
[ns_server:info,2020-06-16T21:33:52.980Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 147 state to active
[ns_server:info,2020-06-16T21:33:52.981Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 146 state to active
[ns_server:info,2020-06-16T21:33:52.983Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 145 state to active
[ns_server:info,2020-06-16T21:33:52.985Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 144 state to active
[ns_server:info,2020-06-16T21:33:52.987Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 143 state to active
[ns_server:info,2020-06-16T21:33:53.005Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 142 state to active
[ns_server:info,2020-06-16T21:33:53.097Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 141 state to active
[ns_server:info,2020-06-16T21:33:53.162Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 140 state to active
[ns_server:info,2020-06-16T21:33:53.185Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 139 state to active
[ns_server:info,2020-06-16T21:33:53.201Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 138 state to active
[ns_server:info,2020-06-16T21:33:53.221Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 137 state to active
[ns_server:info,2020-06-16T21:33:53.231Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 136 state to active
[ns_server:info,2020-06-16T21:33:53.234Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 135 state to active
[ns_server:info,2020-06-16T21:33:53.239Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 134 state to active
[ns_server:info,2020-06-16T21:33:53.242Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 133 state to active
[ns_server:info,2020-06-16T21:33:53.245Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 132 state to active
[ns_server:info,2020-06-16T21:33:53.249Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 131 state to active
[ns_server:info,2020-06-16T21:33:53.265Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 130 state to active
[ns_server:info,2020-06-16T21:33:53.267Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 129 state to active
[ns_server:info,2020-06-16T21:33:53.269Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 128 state to active
[ns_server:info,2020-06-16T21:33:53.274Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 127 state to active
[ns_server:info,2020-06-16T21:33:53.281Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 126 state to active
[ns_server:info,2020-06-16T21:33:53.282Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 125 state to active
[ns_server:info,2020-06-16T21:33:53.283Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 124 state to active
[ns_server:info,2020-06-16T21:33:53.284Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 123 state to active
[ns_server:info,2020-06-16T21:33:53.286Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 122 state to active
[ns_server:info,2020-06-16T21:33:53.288Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 121 state to active
[ns_server:info,2020-06-16T21:33:53.289Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 120 state to active
[ns_server:info,2020-06-16T21:33:53.290Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 119 state to active
[ns_server:info,2020-06-16T21:33:53.292Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 118 state to active
[ns_server:info,2020-06-16T21:33:53.293Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 117 state to active
[ns_server:info,2020-06-16T21:33:53.294Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 116 state to active
[ns_server:info,2020-06-16T21:33:53.295Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 115 state to active
[ns_server:info,2020-06-16T21:33:53.297Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 114 state to active
[ns_server:info,2020-06-16T21:33:53.302Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 113 state to active
[ns_server:info,2020-06-16T21:33:53.305Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 112 state to active
[ns_server:info,2020-06-16T21:33:53.307Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 111 state to active
[ns_server:info,2020-06-16T21:33:53.308Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 110 state to active
[ns_server:info,2020-06-16T21:33:53.309Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 109 state to active
[ns_server:info,2020-06-16T21:33:53.311Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 108 state to active
[ns_server:info,2020-06-16T21:33:53.312Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 107 state to active
[ns_server:info,2020-06-16T21:33:53.314Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 106 state to active
[ns_server:info,2020-06-16T21:33:53.317Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 105 state to active
[ns_server:info,2020-06-16T21:33:53.320Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 104 state to active
[ns_server:info,2020-06-16T21:33:53.322Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 103 state to active
[ns_server:info,2020-06-16T21:33:53.324Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 102 state to active
[ns_server:info,2020-06-16T21:33:53.326Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 101 state to active
[ns_server:info,2020-06-16T21:33:53.327Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 100 state to active
[ns_server:info,2020-06-16T21:33:53.329Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 99 state to active
[ns_server:info,2020-06-16T21:33:53.330Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 98 state to active
[ns_server:info,2020-06-16T21:33:53.331Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 97 state to active
[ns_server:info,2020-06-16T21:33:53.331Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 96 state to active
[ns_server:info,2020-06-16T21:33:53.332Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 95 state to active
[ns_server:info,2020-06-16T21:33:53.333Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 94 state to active
[ns_server:info,2020-06-16T21:33:53.338Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 93 state to active
[ns_server:info,2020-06-16T21:33:53.340Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 92 state to active
[ns_server:info,2020-06-16T21:33:53.342Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 91 state to active
[ns_server:info,2020-06-16T21:33:53.343Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 90 state to active
[ns_server:info,2020-06-16T21:33:53.350Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 89 state to active
[ns_server:info,2020-06-16T21:33:53.351Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 88 state to active
[ns_server:info,2020-06-16T21:33:53.353Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 87 state to active
[ns_server:info,2020-06-16T21:33:53.357Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 86 state to active
[ns_server:info,2020-06-16T21:33:53.358Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 85 state to active
[ns_server:info,2020-06-16T21:33:53.370Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 84 state to active
[ns_server:info,2020-06-16T21:33:53.371Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 83 state to active
[ns_server:info,2020-06-16T21:33:53.373Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 82 state to active
[ns_server:info,2020-06-16T21:33:53.378Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 81 state to active
[ns_server:info,2020-06-16T21:33:53.379Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 80 state to active
[ns_server:info,2020-06-16T21:33:53.380Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 79 state to active
[ns_server:info,2020-06-16T21:33:53.381Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 78 state to active
[ns_server:debug,2020-06-16T21:33:53.381Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:info,2020-06-16T21:33:53.383Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 77 state to active
[ns_server:info,2020-06-16T21:33:53.385Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 76 state to active
[ns_server:info,2020-06-16T21:33:53.387Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 75 state to active
[ns_server:info,2020-06-16T21:33:53.388Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 74 state to active
[ns_server:info,2020-06-16T21:33:53.390Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 73 state to active
[ns_server:info,2020-06-16T21:33:53.393Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 72 state to active
[ns_server:info,2020-06-16T21:33:53.394Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 71 state to active
[ns_server:info,2020-06-16T21:33:53.396Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 70 state to active
[ns_server:info,2020-06-16T21:33:53.398Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 69 state to active
[ns_server:info,2020-06-16T21:33:53.410Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 68 state to active
[ns_server:info,2020-06-16T21:33:53.411Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 67 state to active
[ns_server:info,2020-06-16T21:33:53.414Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 66 state to active
[ns_server:info,2020-06-16T21:33:53.418Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 65 state to active
[ns_server:info,2020-06-16T21:33:53.421Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 64 state to active
[ns_server:info,2020-06-16T21:33:53.422Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 63 state to active
[ns_server:info,2020-06-16T21:33:53.428Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 62 state to active
[ns_server:info,2020-06-16T21:33:53.430Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 61 state to active
[ns_server:info,2020-06-16T21:33:53.432Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 60 state to active
[ns_server:info,2020-06-16T21:33:53.434Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 59 state to active
[ns_server:info,2020-06-16T21:33:53.435Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 58 state to active
[ns_server:info,2020-06-16T21:33:53.438Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 57 state to active
[ns_server:info,2020-06-16T21:33:53.441Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 56 state to active
[ns_server:info,2020-06-16T21:33:53.444Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 55 state to active
[ns_server:info,2020-06-16T21:33:53.445Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 54 state to active
[ns_server:info,2020-06-16T21:33:53.446Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 53 state to active
[ns_server:info,2020-06-16T21:33:53.449Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 52 state to active
[ns_server:info,2020-06-16T21:33:53.451Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 51 state to active
[ns_server:info,2020-06-16T21:33:53.452Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 50 state to active
[ns_server:info,2020-06-16T21:33:53.454Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 49 state to active
[ns_server:info,2020-06-16T21:33:53.458Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 48 state to active
[ns_server:info,2020-06-16T21:33:53.459Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 47 state to active
[ns_server:info,2020-06-16T21:33:53.460Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 46 state to active
[ns_server:info,2020-06-16T21:33:53.461Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 45 state to active
[ns_server:info,2020-06-16T21:33:53.462Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 44 state to active
[ns_server:info,2020-06-16T21:33:53.466Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 43 state to active
[ns_server:info,2020-06-16T21:33:53.467Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 42 state to active
[ns_server:info,2020-06-16T21:33:53.468Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 41 state to active
[ns_server:info,2020-06-16T21:33:53.469Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 40 state to active
[ns_server:info,2020-06-16T21:33:53.472Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 39 state to active
[ns_server:info,2020-06-16T21:33:53.474Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 38 state to active
[ns_server:info,2020-06-16T21:33:53.475Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 37 state to active
[ns_server:info,2020-06-16T21:33:53.476Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 36 state to active
[ns_server:info,2020-06-16T21:33:53.479Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 35 state to active
[ns_server:info,2020-06-16T21:33:53.482Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 34 state to active
[ns_server:info,2020-06-16T21:33:53.484Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 33 state to active
[ns_server:info,2020-06-16T21:33:53.486Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 32 state to active
[ns_server:info,2020-06-16T21:33:53.490Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 31 state to active
[ns_server:info,2020-06-16T21:33:53.498Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 30 state to active
[ns_server:info,2020-06-16T21:33:53.501Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 29 state to active
[ns_server:info,2020-06-16T21:33:53.502Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 28 state to active
[ns_server:info,2020-06-16T21:33:53.504Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 27 state to active
[ns_server:info,2020-06-16T21:33:53.505Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 26 state to active
[ns_server:info,2020-06-16T21:33:53.508Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 25 state to active
[ns_server:info,2020-06-16T21:33:53.514Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 24 state to active
[ns_server:info,2020-06-16T21:33:53.519Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 23 state to active
[ns_server:info,2020-06-16T21:33:53.520Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 22 state to active
[ns_server:info,2020-06-16T21:33:53.522Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 21 state to active
[ns_server:info,2020-06-16T21:33:53.524Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 20 state to active
[ns_server:info,2020-06-16T21:33:53.525Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 19 state to active
[ns_server:info,2020-06-16T21:33:53.531Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 18 state to active
[ns_server:info,2020-06-16T21:33:53.536Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 17 state to active
[ns_server:info,2020-06-16T21:33:53.540Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 16 state to active
[ns_server:info,2020-06-16T21:33:53.550Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 15 state to active
[ns_server:info,2020-06-16T21:33:53.553Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 14 state to active
[ns_server:info,2020-06-16T21:33:53.555Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 13 state to active
[ns_server:info,2020-06-16T21:33:53.580Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 12 state to active
[ns_server:info,2020-06-16T21:33:53.582Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 11 state to active
[ns_server:info,2020-06-16T21:33:53.596Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 10 state to active
[ns_server:info,2020-06-16T21:33:53.603Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 9 state to active
[ns_server:info,2020-06-16T21:33:53.619Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 8 state to active
[ns_server:info,2020-06-16T21:33:53.627Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 7 state to active
[ns_server:info,2020-06-16T21:33:53.638Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 6 state to active
[ns_server:info,2020-06-16T21:33:53.653Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 5 state to active
[ns_server:info,2020-06-16T21:33:53.654Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 4 state to active
[ns_server:info,2020-06-16T21:33:53.660Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 3 state to active
[ns_server:info,2020-06-16T21:33:53.661Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 2 state to active
[ns_server:info,2020-06-16T21:33:53.663Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 1 state to active
[ns_server:info,2020-06-16T21:33:53.665Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:do_handle_call:554]Changed bucket "platzi" vbucket 0 state to active
[ns_server:info,2020-06-16T21:33:53.670Z,ns_1@127.0.0.1:ns_memcached-platzi<0.3028.0>:ns_memcached:handle_call:309]Enabling traffic to bucket "platzi"
[ns_server:info,2020-06-16T21:33:53.671Z,ns_1@127.0.0.1:ns_memcached-platzi<0.3028.0>:ns_memcached:handle_call:313]Bucket "platzi" marked as warmed in 4 seconds
[ns_server:debug,2020-06-16T21:33:53.693Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@127.0.0.1',buckets_with_data} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562433}}]},
 {"platzi",<<"afd2b81586cc3bbf24b79179d05c51d0">>}]
[ns_server:debug,2020-06-16T21:33:53.694Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{48,63759562433}}]}]
[ns_server:debug,2020-06-16T21:33:53.724Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {node,'ns_1@127.0.0.1',buckets_with_data}]..)
[ns_server:info,2020-06-16T21:33:54.015Z,ns_1@127.0.0.1:ns_doctor<0.1995.0>:ns_doctor:update_status:322]The following buckets became ready on node 'ns_1@127.0.0.1': ["platzi"]
[ns_server:debug,2020-06-16T21:33:54.393Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:55.406Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:56.408Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:57.411Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:58.415Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:59.418Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:329]Checking if service service_cbas is started...
[ns_server:debug,2020-06-16T21:33:59.420Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:service_stats_collector:check_status:333]Service service_cbas is started
[stats:warn,2020-06-16T21:34:00.923Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:debug,2020-06-16T21:34:07.819Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:debug,2020-06-16T21:34:07.825Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:34:07.829Z,ns_1@127.0.0.1:<0.3860.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:34:07.856Z,ns_1@127.0.0.1:<0.3862.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 295735, disk size is 3259754
[ns_server:debug,2020-06-16T21:34:07.857Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:34:07.857Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2020-06-16T21:34:07.933Z,ns_1@127.0.0.1:<0.3859.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:34:07.938Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:34:07.938Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:34:10.824Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{49,63759562450}}]}]
[ns_server:debug,2020-06-16T21:34:10.824Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/info/versionToken">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562450}}]}|
 <<"{\"Version\":4}">>]
[ns_server:debug,2020-06-16T21:34:10.828Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,<<"/indexing/info/versionToken">>}]..)
[stats:warn,2020-06-16T21:34:11.053Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 4 ticks
[ns_server:debug,2020-06-16T21:34:17.157Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{50,63759562457}}]}]
[ns_server:debug,2020-06-16T21:34:17.157Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/cbas/cluster/state/metadata/replica">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562457}}]}|
 <<"{\"revision\":1,\"nodes\":[]}">>]
[ns_server:debug,2020-06-16T21:34:17.159Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/cbas/cluster/state/metadata/replica">>}]..)
[stats:warn,2020-06-16T21:34:18.202Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:debug,2020-06-16T21:34:18.855Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{51,63759562458}}]}]
[ns_server:debug,2020-06-16T21:34:18.856Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/ddl/commandToken/create/4683226085357154966/0">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{1,63759562458}}]}|
 <<"{\"DefnId\":4683226085357154966,\"BucketUUID\":\"afd2b81586cc3bbf24b79179d05c51d0\",\"Definitions\":{\"a771316973056c9759aee152c7c79f8b\":[{\"defnId\":4683226085357154966,\"name\":\"#primary\",\"using\":\"GSI\",\"bucket\":\"platzi\",\"isPrimary\":true,\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"NumReplica2\":{\"HasValue\":true,\"Base\":0,\"Incr\":0,\"Decr\":0},\"residentRatio\":100,\"instanceId\":5944500883951093624,\"part"...>>]
[ns_server:debug,2020-06-16T21:34:18.868Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/4683226085357154966/0">>}]..)
[ns_server:debug,2020-06-16T21:34:19.346Z,ns_1@127.0.0.1:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2020-06-16T21:34:19.757Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/ddl/commandToken/create/4683226085357154966/0">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{2,63759562459}}]}|
 '_deleted']
[ns_server:debug,2020-06-16T21:34:19.757Z,ns_1@127.0.0.1:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"a771316973056c9759aee152c7c79f8b">>} ->
[{'_vclock',[{<<"a771316973056c9759aee152c7c79f8b">>,{52,63759562459}}]}]
[ns_server:debug,2020-06-16T21:34:19.768Z,ns_1@127.0.0.1:ns_config_rep<0.345.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([{local_changes_count,
                                   <<"a771316973056c9759aee152c7c79f8b">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/4683226085357154966/0">>}]..)
[stats:warn,2020-06-16T21:34:27.459Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 3 ticks
[ns_server:debug,2020-06-16T21:34:37.858Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:34:37.859Z,ns_1@127.0.0.1:<0.6128.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:34:37.862Z,ns_1@127.0.0.1:<0.6130.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 384789, disk size is 4241408
[ns_server:debug,2020-06-16T21:34:37.862Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:34:37.862Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:34:37.940Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:34:37.947Z,ns_1@127.0.0.1:<0.6131.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:34:37.948Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:34:37.948Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:35:02.492Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[stats:warn,2020-06-16T21:35:04.192Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:debug,2020-06-16T21:35:07.863Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:35:07.864Z,ns_1@127.0.0.1:<0.7551.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:35:07.865Z,ns_1@127.0.0.1:<0.7553.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 384789, disk size is 4241408
[ns_server:debug,2020-06-16T21:35:07.866Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:35:07.866Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:35:07.949Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:35:07.960Z,ns_1@127.0.0.1:<0.7554.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:35:07.962Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:35:07.962Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[stats:warn,2020-06-16T21:35:10.859Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[stats:warn,2020-06-16T21:35:29.919Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[stats:warn,2020-06-16T21:35:36.686Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:debug,2020-06-16T21:35:37.867Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:35:37.868Z,ns_1@127.0.0.1:<0.8768.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:35:37.869Z,ns_1@127.0.0.1:<0.8770.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 384789, disk size is 4241408
[ns_server:debug,2020-06-16T21:35:37.870Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:35:37.870Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:35:37.963Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:35:37.968Z,ns_1@127.0.0.1:<0.8771.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:35:37.969Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:35:37.969Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:35:51.102Z,ns_1@127.0.0.1:<0.3046.0>:ns_memcached:verify_report_long_call:372]call {get_vbucket_details_stats,all,["state","topology"]} took too long: 665629 us
[ns_server:debug,2020-06-16T21:35:52.156Z,ns_1@127.0.0.1:<0.3044.0>:ns_memcached:verify_report_long_call:372]call {stats,<<"dcpagg :">>} took too long: 1158662 us
[stats:warn,2020-06-16T21:35:52.181Z,ns_1@127.0.0.1:<0.3040.0>:base_stats_collector:latest_tick:64](Collector: stats_collector) Dropped 1 ticks
[stats:warn,2020-06-16T21:35:53.069Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 4 ticks
[stats:warn,2020-06-16T21:36:01.742Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 3 ticks
[ns_server:debug,2020-06-16T21:36:07.871Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:36:07.871Z,ns_1@127.0.0.1:<0.11245.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:36:07.874Z,ns_1@127.0.0.1:<0.11247.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 384789, disk size is 4241408
[ns_server:debug,2020-06-16T21:36:07.875Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:36:07.876Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:36:07.970Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:36:08.028Z,ns_1@127.0.0.1:<0.11248.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:36:08.055Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:36:08.055Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 29s
[stats:warn,2020-06-16T21:36:09.297Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[stats:warn,2020-06-16T21:36:15.910Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:debug,2020-06-16T21:36:17.471Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[stats:warn,2020-06-16T21:36:24.173Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 2 ticks
[stats:warn,2020-06-16T21:36:31.277Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:debug,2020-06-16T21:36:37.057Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:36:37.064Z,ns_1@127.0.0.1:<0.12822.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:36:37.065Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:36:37.065Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:36:37.877Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:36:37.878Z,ns_1@127.0.0.1:<0.12865.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:36:37.883Z,ns_1@127.0.0.1:<0.12867.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 384789, disk size is 4241408
[ns_server:debug,2020-06-16T21:36:37.884Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:36:37.884Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[stats:warn,2020-06-16T21:36:38.405Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[stats:warn,2020-06-16T21:36:45.897Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 2 ticks
[stats:warn,2020-06-16T21:36:54.830Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 3 ticks
[stats:warn,2020-06-16T21:37:01.848Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:debug,2020-06-16T21:37:07.066Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:37:07.073Z,ns_1@127.0.0.1:<0.14031.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:37:07.074Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:37:07.074Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:37:07.892Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:37:07.894Z,ns_1@127.0.0.1:<0.14078.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:37:07.901Z,ns_1@127.0.0.1:<0.14080.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 384789, disk size is 4241408
[ns_server:debug,2020-06-16T21:37:07.902Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:37:07.902Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[stats:warn,2020-06-16T21:37:12.187Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 4 ticks
[stats:warn,2020-06-16T21:37:20.170Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 2 ticks
[stats:warn,2020-06-16T21:37:27.503Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:debug,2020-06-16T21:37:32.494Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[stats:warn,2020-06-16T21:37:34.911Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 2 ticks
[ns_server:debug,2020-06-16T21:37:37.075Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:37:37.082Z,ns_1@127.0.0.1:<0.15243.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:37:37.083Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:37:37.083Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:37:37.903Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:37:37.904Z,ns_1@127.0.0.1:<0.15293.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:37:37.907Z,ns_1@127.0.0.1:<0.15295.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 384789, disk size is 4241408
[ns_server:debug,2020-06-16T21:37:37.907Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:37:37.907Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[stats:warn,2020-06-16T21:37:41.719Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[stats:warn,2020-06-16T21:37:54.753Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 1 ticks
[ns_server:debug,2020-06-16T21:38:07.084Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:38:07.089Z,ns_1@127.0.0.1:<0.16465.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:38:07.091Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:38:07.091Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:38:07.908Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:38:07.909Z,ns_1@127.0.0.1:<0.16514.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:38:07.911Z,ns_1@127.0.0.1:<0.16516.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 384789, disk size is 4241408
[ns_server:debug,2020-06-16T21:38:07.912Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:38:07.912Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[stats:warn,2020-06-16T21:38:22.491Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 3 ticks
[stats:warn,2020-06-16T21:38:35.846Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 2 ticks
[ns_server:debug,2020-06-16T21:38:37.092Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_views) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:38:37.099Z,ns_1@127.0.0.1:<0.17684.0>:compaction_daemon:spawn_scheduled_views_compactor:494]Start compaction of indexes for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:38:37.099Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:38:37.100Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:38:37.912Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_scheduler_message:1309]Starting compaction (compact_kv) for the following buckets: 
[<<"platzi">>]
[ns_server:info,2020-06-16T21:38:37.914Z,ns_1@127.0.0.1:<0.17726.0>:compaction_daemon:spawn_scheduled_kv_compactor:468]Start compaction of vbuckets for bucket platzi with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2020-06-16T21:38:37.916Z,ns_1@127.0.0.1:<0.17728.0>:compaction_daemon:bucket_needs_compaction:969]`platzi` data size is 384789, disk size is 4241408
[ns_server:debug,2020-06-16T21:38:37.917Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_daemon:process_compactors_exit:1350]Finished compaction iteration.
[ns_server:debug,2020-06-16T21:38:37.917Z,ns_1@127.0.0.1:compaction_daemon<0.543.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2020-06-16T21:38:47.487Z,ns_1@127.0.0.1:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[stats:warn,2020-06-16T21:38:50.333Z,ns_1@127.0.0.1:service_stats_collector-cbas<0.2126.0>:base_stats_collector:latest_tick:64](Collector: service_stats_collector) Dropped 2 ticks
